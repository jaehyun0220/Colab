{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Media_Project #2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e8780e5870a3474084b805bfccc90359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8881b001f1454c9493ef89bd00982d13",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0957b752362e440ebe1f6d1cc2d384d4",
              "IPY_MODEL_674d5f1cb0ee443c85a8b2a6d111a7cf"
            ]
          }
        },
        "8881b001f1454c9493ef89bd00982d13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0957b752362e440ebe1f6d1cc2d384d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a62ba01aa854362842e73b1a54f0a52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_026ebdf63e6844459f31f8cc671b5bd3"
          }
        },
        "674d5f1cb0ee443c85a8b2a6d111a7cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_08472a45e42b49ef8bc8d3156bb4c485",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 28/28 [00:14&lt;00:00,  2.57it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c505af9bc184c37affce9482a484ecb"
          }
        },
        "4a62ba01aa854362842e73b1a54f0a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "026ebdf63e6844459f31f8cc671b5bd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08472a45e42b49ef8bc8d3156bb4c485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c505af9bc184c37affce9482a484ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd676e870a48466389004f2f8342632a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f908e03921234e529efa121320b88f72",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6de3dc76d666471ea1b8d816ceec3506",
              "IPY_MODEL_03cb2dd3ef7b43d9831b9014ae8c7fb9"
            ]
          }
        },
        "f908e03921234e529efa121320b88f72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6de3dc76d666471ea1b8d816ceec3506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58076072a951474e830e581a30c9ba09",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 492,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 492,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c3d5238c25b04e10b59b68c7a5bc2a26"
          }
        },
        "03cb2dd3ef7b43d9831b9014ae8c7fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a17d65e220048869231be3d046e8f9c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 492/492 [00:00&lt;00:00, 3879.65it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5072f4a0de1f40ffa4ff8132b0036935"
          }
        },
        "58076072a951474e830e581a30c9ba09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c3d5238c25b04e10b59b68c7a5bc2a26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a17d65e220048869231be3d046e8f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5072f4a0de1f40ffa4ff8132b0036935": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6f83c7be57f4cef8894c634382f0f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c8c1542381a94a9aa4665a9ce508b4fb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3f148d85f820483f8312afae56575d7a",
              "IPY_MODEL_f2d1c979061f41ecb72b4f6e6a821111"
            ]
          }
        },
        "c8c1542381a94a9aa4665a9ce508b4fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f148d85f820483f8312afae56575d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e2f85d3eb2754ef5bf48d0d17559ccee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73ca5bbab4f74be380f98f06233b9045"
          }
        },
        "f2d1c979061f41ecb72b4f6e6a821111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a5fa2f828a23467587117843adea161d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0% 0/3 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0cbf9562a20b465b9b23a96e315b5d0b"
          }
        },
        "e2f85d3eb2754ef5bf48d0d17559ccee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73ca5bbab4f74be380f98f06233b9045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a5fa2f828a23467587117843adea161d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0cbf9562a20b465b9b23a96e315b5d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06c6354c51584136ad4e394900178fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ebfc5f510ca04571877be6a588d1d7db",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d433bceb730347e9a1bab3b1a22ccbf6",
              "IPY_MODEL_94bebbe6c0f44697939ae3c5091fc6e8"
            ]
          }
        },
        "ebfc5f510ca04571877be6a588d1d7db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d433bceb730347e9a1bab3b1a22ccbf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8e31dac415874791941a857eb8db0c56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 12,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41dae93ae9f245429b2118fab7953fde"
          }
        },
        "94bebbe6c0f44697939ae3c5091fc6e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fdb9d32617f64d529ab210e0757e82bf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0% 0/12 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_898fa2bf60644d20a9392f82b0121482"
          }
        },
        "8e31dac415874791941a857eb8db0c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41dae93ae9f245429b2118fab7953fde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fdb9d32617f64d529ab210e0757e82bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "898fa2bf60644d20a9392f82b0121482": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaehyun0220/Colab/blob/master/Media_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmdavC_fLDrU",
        "colab_type": "text"
      },
      "source": [
        "#10조. 네이버 댓글 분석을 통한 상위, 하위 클립 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnLbyZflC1R1",
        "colab_type": "code",
        "outputId": "73846f33-a0dd-483a-ea3c-c3e33caf9ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Auth 인증 및 Google Drive 활용 Data load\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5oQGfjLC-Jb",
        "colab_type": "code",
        "outputId": "e66a97e4-5b47-4f82-9650-696b36dd6983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!ls ../gdrive/My\\ Drive/output"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_ep10.csv\t file_ep16.csv\tfile_ep22.csv  file_ep4.csv\n",
            "file_ep11.csv\t file_ep17.csv\tfile_ep23.csv  file_ep5.csv\n",
            "file_ep12.csv\t file_ep18.csv\tfile_ep24.csv  file_ep6.csv\n",
            "file_ep13.csv\t file_ep19.csv\tfile_ep25.csv  file_ep7.csv\n",
            "file_ep14_1.csv  file_ep1.csv\tfile_ep26.csv  file_ep8.csv\n",
            "file_ep14.csv\t file_ep20.csv\tfile_ep2.csv   file_ep9.csv\n",
            "file_ep15.csv\t file_ep21.csv\tfile_ep3.csv   TheLastEmpress.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bdQCRGv7AX6",
        "colab_type": "code",
        "outputId": "e78a71f1-c905-4c69-f50e-abddcf195e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!pip install regex"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\r\u001b[K     |▌                               | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 6.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 9.6MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 9.8MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 12.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 552kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 573kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 9.6MB/s \n",
            "\u001b[?25hInstalling collected packages: regex\n",
            "Successfully installed regex-2019.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEGh8u0qDI6h",
        "colab_type": "code",
        "outputId": "83b90301-e295-4abe-f9f4-dacf105793a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "# 기본 라이브러리 로드\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import regex as re\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "#데이터 전처리 관련 라이브러리 로드\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "#모델 알고리즘 로드\n",
        "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "\n",
        "# Deep Learning Model 로드\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "#차원축소 알고리즘 로드\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#HyperParameter Tuning을 위한 라이브러리 로드\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#모델 평가를 위한 라이브러리 로드\n",
        "from sklearn import metrics, model_selection\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "\n",
        "#데이터 분리를 위한 라이브러리 로드\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#수학 & 통계 관련 라이브러리 로드\n",
        "import scipy.stats as st\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl  # 기본 설정 만지는 용도\n",
        "import matplotlib.pyplot as plt  # 그래프 그리는 용도\n",
        "import matplotlib.font_manager as fm  # 폰트 관련 용도\n",
        "\n",
        "\n",
        "#Configure Visualization Defaults\n",
        "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
        "%matplotlib inline\n",
        "mpl.style.use('ggplot')\n",
        "sns.set_style('white')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpUzHCYqMYp1",
        "colab_type": "code",
        "outputId": "3cb65ff7-3fa1-4083-82b7-ad1d91cd13dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "filelist = os.listdir('../gdrive/My Drive/output')\n",
        "filelist"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['file_ep1.csv',\n",
              " 'file_ep16.csv',\n",
              " 'file_ep17.csv',\n",
              " 'file_ep10.csv',\n",
              " 'file_ep14.csv',\n",
              " 'file_ep12.csv',\n",
              " 'file_ep11.csv',\n",
              " 'file_ep14_1.csv',\n",
              " 'file_ep15.csv',\n",
              " 'file_ep13.csv',\n",
              " 'file_ep18.csv',\n",
              " 'file_ep19.csv',\n",
              " 'file_ep21.csv',\n",
              " 'file_ep20.csv',\n",
              " 'file_ep2.csv',\n",
              " 'file_ep22.csv',\n",
              " 'file_ep23.csv',\n",
              " 'file_ep25.csv',\n",
              " 'file_ep24.csv',\n",
              " 'file_ep26.csv',\n",
              " 'file_ep3.csv',\n",
              " 'file_ep4.csv',\n",
              " 'file_ep5.csv',\n",
              " 'file_ep6.csv',\n",
              " 'file_ep9.csv',\n",
              " 'file_ep7.csv',\n",
              " 'TheLastEmpress.csv',\n",
              " 'file_ep8.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Ink4z2DVoG",
        "colab_type": "code",
        "outputId": "b7e58ae8-200d-40a8-e953-869578d848a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "e8780e5870a3474084b805bfccc90359",
            "8881b001f1454c9493ef89bd00982d13",
            "0957b752362e440ebe1f6d1cc2d384d4",
            "674d5f1cb0ee443c85a8b2a6d111a7cf",
            "4a62ba01aa854362842e73b1a54f0a52",
            "026ebdf63e6844459f31f8cc671b5bd3",
            "08472a45e42b49ef8bc8d3156bb4c485",
            "0c505af9bc184c37affce9482a484ecb"
          ]
        }
      },
      "source": [
        "# 총 26회차 491개 하이라이트 클립 존재 (전체 재생수 = 107,221,654 / 클립 당 평균 재생수 = 218,374), \n",
        "# 이 중에서 예고편, 미공개, 인터뷰 등 클립 제외하고 총 422회 클립 대상\n",
        "df_title = pd.read_csv('../gdrive/My Drive/output/TheLastEmpress.csv', encoding = 'euc-kr')\n",
        "df_title.rename(columns=lambda x: re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》 ]', '', x), inplace=True)\n",
        "\n",
        "# 조회수 분포를 고려하여 각 회차별로 조회수 상위 4개, 하위 4개 클립을 샘플링 - 총 208개 클립\n",
        "# 좋아요수, 댓글 수, 댓글 내용, 댓글 작성자 정보 (웹크롤링 통한 추출)\n",
        "\n",
        "df_ep_tot = pd.DataFrame()\n",
        "for i in tqdm_notebook(filelist):\n",
        "  if (i[:4] == 'file'):\n",
        "    df_ep_temp = pd.read_csv('../gdrive/My Drive/output/'+i)\n",
        "    df_ep_temp['play'] = df_ep_temp['play'].apply(lambda x: int(re.sub(',','', x[4:])))\n",
        "    df_ep_temp['rank'] = df_ep_temp['play'].rank(method='dense', ascending=False)\n",
        "    df_ep_tot = df_ep_tot.append(df_ep_temp)\n",
        "\n",
        "df_ep_tot.drop(columns='Unnamed: 0', inplace=True)\n",
        "df_ep_tot['target'] = np.where(df_ep_tot['rank']<=4,1,0)\n",
        "df_ep_tot"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8780e5870a3474084b805bfccc90359",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=28), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>서지안</td>\n",
              "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>경</td>\n",
              "      <td>ㅏ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1841</th>\n",
              "      <td>Major</td>\n",
              "      <td>와 ㅅㅂ...피지컬봐....</td>\n",
              "      <td>550</td>\n",
              "      <td>1</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1842</th>\n",
              "      <td>스폰지밥</td>\n",
              "      <td>이쁘십니다 할때 나만설렜냐,,,,,,,</td>\n",
              "      <td>691</td>\n",
              "      <td>4</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1843</th>\n",
              "      <td>박한별</td>\n",
              "      <td>반했네</td>\n",
              "      <td>372</td>\n",
              "      <td>4</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1844</th>\n",
              "      <td>김민정</td>\n",
              "      <td>죄송하지만 이 분 이용합시다 ! 어쨌든 좋은게 좋은거죠 ..</td>\n",
              "      <td>593</td>\n",
              "      <td>4</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1845</th>\n",
              "      <td>블링블링</td>\n",
              "      <td>우빈이한테 전부 들이대는군</td>\n",
              "      <td>338</td>\n",
              "      <td>3</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40935 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          nick                                           contents  ...  rank  target\n",
              "0     rosi****  ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...  ...   1.0       1\n",
              "1       핑크에메랄드  왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...  ...   1.0       1\n",
              "2          서지안                                           ㅋㅋㅋㅋㅋㅋㅋㅋ  ...   1.0       1\n",
              "3            경                                                  ㅏ  ...   1.0       1\n",
              "4           홍홍            이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발  ...   1.0       1\n",
              "...        ...                                                ...  ...   ...     ...\n",
              "1841     Major                                    와 ㅅㅂ...피지컬봐....  ...   8.0       0\n",
              "1842      스폰지밥                              이쁘십니다 할때 나만설렜냐,,,,,,,  ...   8.0       0\n",
              "1843       박한별                                                반했네  ...   8.0       0\n",
              "1844       김민정                  죄송하지만 이 분 이용합시다 ! 어쨌든 좋은게 좋은거죠 ..  ...   8.0       0\n",
              "1845      블링블링                                     우빈이한테 전부 들이대는군  ...   8.0       0\n",
              "\n",
              "[40935 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-6DDOI-DyW_",
        "colab_type": "code",
        "outputId": "20958070-1984-4a29-a444-fa66dd3f3c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "display(df_title.head())\n",
        "display(df_title.info())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14</td>\n",
              "      <td>[숨멎 엔딩] 신성록, 설렘 폭발하는 반전 섹시미!</td>\n",
              "      <td>892539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>장나라, 악녀 이엘리야 때려잡는 카리스마 “얻다대고 반말”</td>\n",
              "      <td>852569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19</td>\n",
              "      <td>신은경 완벽 빙의한 ‘오아린 더빙 연기’</td>\n",
              "      <td>811897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>“사랑해요 폐하” 장나라, 신성록 계획 박살 내며 ‘흑화 스위치 ON’</td>\n",
              "      <td>757992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>뛰는 이엘리야 위 나는 오아린, 혼신의 눈물 연기(Feat. 윤소이 불꽃 따귀)</td>\n",
              "      <td>725808</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   episode                                         title    play\n",
              "0       14                  [숨멎 엔딩] 신성록, 설렘 폭발하는 반전 섹시미!  892539\n",
              "1        8              장나라, 악녀 이엘리야 때려잡는 카리스마 “얻다대고 반말”  852569\n",
              "2       19                        신은경 완벽 빙의한 ‘오아린 더빙 연기’  811897\n",
              "3       10       “사랑해요 폐하” 장나라, 신성록 계획 박살 내며 ‘흑화 스위치 ON’  757992\n",
              "4       10  뛰는 이엘리야 위 나는 오아린, 혼신의 눈물 연기(Feat. 윤소이 불꽃 따귀)  725808"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 422 entries, 0 to 421\n",
            "Data columns (total 3 columns):\n",
            "episode    422 non-null int64\n",
            "title      422 non-null object\n",
            "play       422 non-null int64\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 10.0+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCdCQjW8D5xZ",
        "colab_type": "code",
        "outputId": "63dde32a-952b-487f-abfc-5ddc09ee501a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "display(df_ep_tot.head())\n",
        "display(df_ep_tot.info())\n",
        "# df_ep_tot[df_ep_tot['episode'] == 1].sample(10).sort_values(by='play', ascending = False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>서지안</td>\n",
              "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>경</td>\n",
              "      <td>ㅏ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       nick                                           contents  ...  rank  target\n",
              "0  rosi****  ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...  ...   1.0       1\n",
              "1    핑크에메랄드  왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...  ...   1.0       1\n",
              "2       서지안                                           ㅋㅋㅋㅋㅋㅋㅋㅋ  ...   1.0       1\n",
              "3         경                                                  ㅏ  ...   1.0       1\n",
              "4        홍홍            이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발  ...   1.0       1\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 40935 entries, 0 to 1845\n",
            "Data columns (total 11 columns):\n",
            "nick           40935 non-null object\n",
            "contents       40801 non-null object\n",
            "recomm         40935 non-null int64\n",
            "unrecomm       40935 non-null int64\n",
            "title          40935 non-null object\n",
            "play           40935 non-null int64\n",
            "like           40935 non-null object\n",
            "reple_count    40935 non-null object\n",
            "episode        40935 non-null int64\n",
            "rank           40935 non-null float64\n",
            "target         40935 non-null int64\n",
            "dtypes: float64(1), int64(5), object(5)\n",
            "memory usage: 3.7+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7MxLLtPX7gN",
        "colab_type": "text"
      },
      "source": [
        "### 1화 댓글 대상 sample set test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cA7gB0gV3HF",
        "colab_type": "code",
        "outputId": "8b04b8a0-d75d-4f56-c755-c37941a2e3ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df_ep_sample = df_ep_tot[df_ep_tot['episode'] == 1].copy()\n",
        "df_ep_sample"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>서지안</td>\n",
              "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>경</td>\n",
              "      <td>ㅏ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>싹스리</td>\n",
              "      <td>ㆍ연기가 좀 오바한다 좀 자연스럽게 스테파니야</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>갓갓</td>\n",
              "      <td>본방 사운드도 이상하더니ㅋㅋㅋㅋㅋㅋㅋ스브스 왜그래.... 노답</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>뿅뿅</td>\n",
              "      <td>영상 소리 안나와요ㅡㅡ 올리고 확인좀...</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>힘내♡</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>ㅋㅋㅋ</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>492 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         nick                                           contents  ...  rank  target\n",
              "0    rosi****  ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...  ...   1.0       1\n",
              "1      핑크에메랄드  왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...  ...   1.0       1\n",
              "2         서지안                                           ㅋㅋㅋㅋㅋㅋㅋㅋ  ...   1.0       1\n",
              "3           경                                                  ㅏ  ...   1.0       1\n",
              "4          홍홍            이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발  ...   1.0       1\n",
              "..        ...                                                ...  ...   ...     ...\n",
              "487       싹스리                          ㆍ연기가 좀 오바한다 좀 자연스럽게 스테파니야  ...   8.0       0\n",
              "488        갓갓                 본방 사운드도 이상하더니ㅋㅋㅋㅋㅋㅋㅋ스브스 왜그래.... 노답  ...   8.0       0\n",
              "489        뿅뿅                            영상 소리 안나와요ㅡㅡ 올리고 확인좀...  ...   8.0       0\n",
              "490       뽀글이                                                힘내♡  ...   8.0       0\n",
              "491       뽀글이                                                ㅋㅋㅋ  ...   8.0       0\n",
              "\n",
              "[492 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52VbARCKYxHd",
        "colab_type": "code",
        "outputId": "eeaaca14-2725-4b74-853d-b2ff770574a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "!pip3 install konlpy"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 381kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.17.4)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 39.2MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Installing collected packages: colorama, beautifulsoup4, JPype1, tweepy, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-0.7.0 beautifulsoup4-4.6.0 colorama-0.4.1 konlpy-0.5.2 tweepy-3.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qHvTttUYJQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Kkma, Okt\n",
        "from konlpy.utils import pprint\n",
        "\n",
        "okt =Okt()\n",
        "kkma = Kkma()\n",
        "# mecab = Mecab()\n",
        "# pprint(kkma.sentences(u'네, 안녕하세요. 반갑습니다.'))\n",
        "\n",
        "# sentence = u'만 6세 이하의 초등학교 취학 전 자녀를 양육하기 위해서는'\n",
        "# words = konlpy.tag.Twitter().pos(sentence)\n",
        "# print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LzY1qM44bVrA",
        "colab": {}
      },
      "source": [
        "def morphs_okt(x):\n",
        "  res = okt.morphs(x)\n",
        "  if len(res) >= 1:\n",
        "    res = [re.sub('[ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ ]', '', res[i]) for i in range(len(res)) if re.sub('[ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ ]', '', res[i]) != '' and len(res[i]) >= 1]\n",
        "  else:\n",
        "    res = ''\n",
        "  res = '' if not res else res\n",
        "  return res\n",
        "\n",
        "def morphs_kkma(x):\n",
        "  res = kkma.morphs(x)\n",
        "  if len(res) >= 1:\n",
        "    res = [re.sub('[ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ ]', '', res[i]) for i in range(len(res)) if re.sub('[ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ ]', '', res[i]) != '' and len(res[i]) >= 1]\n",
        "  else:\n",
        "    res = ''\n",
        "  res = '' if not res else res\n",
        "  return res\n",
        "\n",
        "# def morphs_mecab(x):\n",
        "#   res = kkma.morphs(x)\n",
        "#   if len(res) > 1:\n",
        "#     res = [re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》ㅋㅡ ]', '', res[i]) for i in range(len(res)) if len(res[i]) > 1]\n",
        "#   else:\n",
        "#     res = ''\n",
        "#   return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyG91a40YN8M",
        "colab_type": "code",
        "outputId": "282a055a-5790-4700-df27-cde6b28c7219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "df_ep_sample['contents'] = df_ep_sample['contents'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "df_ep_sample['okt_token'] = df_ep_sample['contents'].apply(lambda x: morphs_okt(x))\n",
        "df_ep_sample['kkma_token'] = df_ep_sample['contents'].apply(lambda x: morphs_kkma(x))\n",
        "df_ep_sample['okt_token_str'] = df_ep_sample['okt_token'].apply(lambda x: ' '.join(x))\n",
        "df_ep_sample['kkma_token_str'] = df_ep_sample['kkma_token'].apply(lambda x: ' '.join(x))\n",
        "df_ep_sample"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "      <th>okt_token</th>\n",
              "      <th>kkma_token</th>\n",
              "      <th>okt_token_str</th>\n",
              "      <th>kkma_token_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[옷, 입고, 목욕탕, 들어가는거, 웃기, 넼, 저, 거, 명품, 일, 텐데]</td>\n",
              "      <td>[옷, 입, 고, 목욕탕, 들어가, 는, 거, 웃기, 넼, 저거, 명품, 이, 터,...</td>\n",
              "      <td>옷 입고 목욕탕 들어가는거 웃기 넼 저 거 명품 일 텐데</td>\n",
              "      <td>옷 입 고 목욕탕 들어가 는 거 웃기 넼 저거 명품 이 터 이 데</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[왠지, 선, 황제, 가, 바람, 은, 못, 폈을거, 같다는, 생각, 이, 든다, ...</td>\n",
              "      <td>[왠지, 선, 황제, 가, 바람, 은, 못, 피, 었, 을, 거, 같, 다는, 생각...</td>\n",
              "      <td>왠지 선 황제 가 바람 은 못 폈을거 같다는 생각 이 든다 태후 가 황후 로 있는한...</td>\n",
              "      <td>왠지 선 황제 가 바람 은 못 피 었 을 거 같 다는 생각 이 들 다 태 후 가 황...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>서지안</td>\n",
              "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>경</td>\n",
              "      <td>ㅏ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[이, 게, 나라, 냐, 방송, 에서, 이딴, 수위, 가, 나오고, 지랄, 이야, ...</td>\n",
              "      <td>[이것, 이, 나라, 이, 냐, 방송, 에서, 이딴, 수위, 가, 나오, 고, 지랄...</td>\n",
              "      <td>이 게 나라 냐 방송 에서 이딴 수위 가 나오고 지랄 이야 진짜 개 좃헬 조선 시발</td>\n",
              "      <td>이것 이 나라 이 냐 방송 에서 이딴 수위 가 나오 고 지랄 이 야 진짜 개 좃 헤...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>싹스리</td>\n",
              "      <td>연기가 좀 오바한다 좀 자연스럽게 스테파니야</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[연기, 가, 좀, 오, 바, 한다, 좀, 자연, 스럽게, 스테파니, 야]</td>\n",
              "      <td>[연기, 가, 좀, 오, 바, 하, 다, 좀, 자연, 스럽, 게, 스테파니, 야]</td>\n",
              "      <td>연기 가 좀 오 바 한다 좀 자연 스럽게 스테파니 야</td>\n",
              "      <td>연기 가 좀 오 바 하 다 좀 자연 스럽 게 스테파니 야</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>갓갓</td>\n",
              "      <td>본방 사운드도 이상하더니ㅋㅋㅋㅋㅋㅋㅋ스브스 왜그래 노답</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[본방, 사운드, 도, 이상하더니, 스브스, 왜, 그래, 노답]</td>\n",
              "      <td>[본방, 사운드, 도, 이상, 하, 더니, 스브스, 왜, 그리하, 여, 노, 답]</td>\n",
              "      <td>본방 사운드 도 이상하더니 스브스 왜 그래 노답</td>\n",
              "      <td>본방 사운드 도 이상 하 더니 스브스 왜 그리하 여 노 답</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>뿅뿅</td>\n",
              "      <td>영상 소리 안나와요ㅡㅡ 올리고 확인좀</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[영상, 소리, 안, 나와요, 올리고, 확인, 좀]</td>\n",
              "      <td>[영상, 소리, 안, 나오, 아요, 올리, 고, 확인, 좀]</td>\n",
              "      <td>영상 소리 안 나와요 올리고 확인 좀</td>\n",
              "      <td>영상 소리 안 나오 아요 올리 고 확인 좀</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>힘내</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[힘내]</td>\n",
              "      <td>[힘내]</td>\n",
              "      <td>힘내</td>\n",
              "      <td>힘내</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>ㅋㅋㅋ</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>492 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         nick  ...                                     kkma_token_str\n",
              "0    rosi****  ...               옷 입 고 목욕탕 들어가 는 거 웃기 넼 저거 명품 이 터 이 데\n",
              "1      핑크에메랄드  ...  왠지 선 황제 가 바람 은 못 피 었 을 거 같 다는 생각 이 들 다 태 후 가 황...\n",
              "2         서지안  ...                                                   \n",
              "3           경  ...                                                   \n",
              "4          홍홍  ...  이것 이 나라 이 냐 방송 에서 이딴 수위 가 나오 고 지랄 이 야 진짜 개 좃 헤...\n",
              "..        ...  ...                                                ...\n",
              "487       싹스리  ...                    연기 가 좀 오 바 하 다 좀 자연 스럽 게 스테파니 야\n",
              "488        갓갓  ...                   본방 사운드 도 이상 하 더니 스브스 왜 그리하 여 노 답\n",
              "489        뿅뿅  ...                            영상 소리 안 나오 아요 올리 고 확인 좀\n",
              "490       뽀글이  ...                                                 힘내\n",
              "491       뽀글이  ...                                                   \n",
              "\n",
              "[492 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nYNT1D03sVx",
        "colab_type": "code",
        "outputId": "7097d45d-bd29-4cac-b193-7533e5fe8e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        }
      },
      "source": [
        "df_ep_token = df_ep_sample.drop(df_ep_sample[df_ep_sample['okt_token'] == ''].index)\n",
        "df_ep_token"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "      <th>okt_token</th>\n",
              "      <th>kkma_token</th>\n",
              "      <th>okt_token_str</th>\n",
              "      <th>kkma_token_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[옷, 입고, 목욕탕, 들어가는거, 웃기, 넼, 저, 거, 명품, 일, 텐데]</td>\n",
              "      <td>[옷, 입, 고, 목욕탕, 들어가, 는, 거, 웃기, 넼, 저거, 명품, 이, 터,...</td>\n",
              "      <td>옷 입고 목욕탕 들어가는거 웃기 넼 저 거 명품 일 텐데</td>\n",
              "      <td>옷 입 고 목욕탕 들어가 는 거 웃기 넼 저거 명품 이 터 이 데</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[왠지, 선, 황제, 가, 바람, 은, 못, 폈을거, 같다는, 생각, 이, 든다, ...</td>\n",
              "      <td>[왠지, 선, 황제, 가, 바람, 은, 못, 피, 었, 을, 거, 같, 다는, 생각...</td>\n",
              "      <td>왠지 선 황제 가 바람 은 못 폈을거 같다는 생각 이 든다 태후 가 황후 로 있는한...</td>\n",
              "      <td>왠지 선 황제 가 바람 은 못 피 었 을 거 같 다는 생각 이 들 다 태 후 가 황...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[이, 게, 나라, 냐, 방송, 에서, 이딴, 수위, 가, 나오고, 지랄, 이야, ...</td>\n",
              "      <td>[이것, 이, 나라, 이, 냐, 방송, 에서, 이딴, 수위, 가, 나오, 고, 지랄...</td>\n",
              "      <td>이 게 나라 냐 방송 에서 이딴 수위 가 나오고 지랄 이야 진짜 개 좃헬 조선 시발</td>\n",
              "      <td>이것 이 나라 이 냐 방송 에서 이딴 수위 가 나오 고 지랄 이 야 진짜 개 좃 헤...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>앵그리햄</td>\n",
              "      <td>이엘리야님 진짜 예쁘다</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[이엘리야, 님, 진짜, 예쁘다]</td>\n",
              "      <td>[이, 엘리야, 님, 진짜, 예쁘, 다]</td>\n",
              "      <td>이엘리야 님 진짜 예쁘다</td>\n",
              "      <td>이 엘리야 님 진짜 예쁘 다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>doloing</td>\n",
              "      <td>오늘은 이거다</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[오늘, 은, 이, 거, 다]</td>\n",
              "      <td>[오늘, 은, 이거, 이, 다]</td>\n",
              "      <td>오늘 은 이 거 다</td>\n",
              "      <td>오늘 은 이거 이 다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>승은</td>\n",
              "      <td>이겨서 돈 받을 차례 아니었나 돈은 받고 쳐들어가지 아깝ㅜㅜㅜㅜㅜㅜㅜ</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[이겨서, 돈, 받을, 차례, 아니었나, 돈, 은, 받고, 쳐, 들어가지, 아깝]</td>\n",
              "      <td>[이기, 어서, 돈, 받, 을, 차례, 아니, 었, 나, 돈, 은, 받, 고, 쳐들...</td>\n",
              "      <td>이겨서 돈 받을 차례 아니었나 돈 은 받고 쳐 들어가지 아깝</td>\n",
              "      <td>이기 어서 돈 받 을 차례 아니 었 나 돈 은 받 고 쳐들어가 지 아 까</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>싹스리</td>\n",
              "      <td>연기가 좀 오바한다 좀 자연스럽게 스테파니야</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[연기, 가, 좀, 오, 바, 한다, 좀, 자연, 스럽게, 스테파니, 야]</td>\n",
              "      <td>[연기, 가, 좀, 오, 바, 하, 다, 좀, 자연, 스럽, 게, 스테파니, 야]</td>\n",
              "      <td>연기 가 좀 오 바 한다 좀 자연 스럽게 스테파니 야</td>\n",
              "      <td>연기 가 좀 오 바 하 다 좀 자연 스럽 게 스테파니 야</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>갓갓</td>\n",
              "      <td>본방 사운드도 이상하더니ㅋㅋㅋㅋㅋㅋㅋ스브스 왜그래 노답</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[본방, 사운드, 도, 이상하더니, 스브스, 왜, 그래, 노답]</td>\n",
              "      <td>[본방, 사운드, 도, 이상, 하, 더니, 스브스, 왜, 그리하, 여, 노, 답]</td>\n",
              "      <td>본방 사운드 도 이상하더니 스브스 왜 그래 노답</td>\n",
              "      <td>본방 사운드 도 이상 하 더니 스브스 왜 그리하 여 노 답</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>뿅뿅</td>\n",
              "      <td>영상 소리 안나와요ㅡㅡ 올리고 확인좀</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[영상, 소리, 안, 나와요, 올리고, 확인, 좀]</td>\n",
              "      <td>[영상, 소리, 안, 나오, 아요, 올리, 고, 확인, 좀]</td>\n",
              "      <td>영상 소리 안 나와요 올리고 확인 좀</td>\n",
              "      <td>영상 소리 안 나오 아요 올리 고 확인 좀</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>힘내</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[힘내]</td>\n",
              "      <td>[힘내]</td>\n",
              "      <td>힘내</td>\n",
              "      <td>힘내</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>484 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         nick  ...                                     kkma_token_str\n",
              "0    rosi****  ...               옷 입 고 목욕탕 들어가 는 거 웃기 넼 저거 명품 이 터 이 데\n",
              "1      핑크에메랄드  ...  왠지 선 황제 가 바람 은 못 피 었 을 거 같 다는 생각 이 들 다 태 후 가 황...\n",
              "4          홍홍  ...  이것 이 나라 이 냐 방송 에서 이딴 수위 가 나오 고 지랄 이 야 진짜 개 좃 헤...\n",
              "5        앵그리햄  ...                                    이 엘리야 님 진짜 예쁘 다\n",
              "6     doloing  ...                                        오늘 은 이거 이 다\n",
              "..        ...  ...                                                ...\n",
              "486        승은  ...           이기 어서 돈 받 을 차례 아니 었 나 돈 은 받 고 쳐들어가 지 아 까\n",
              "487       싹스리  ...                    연기 가 좀 오 바 하 다 좀 자연 스럽 게 스테파니 야\n",
              "488        갓갓  ...                   본방 사운드 도 이상 하 더니 스브스 왜 그리하 여 노 답\n",
              "489        뿅뿅  ...                            영상 소리 안 나오 아요 올리 고 확인 좀\n",
              "490       뽀글이  ...                                                 힘내\n",
              "\n",
              "[484 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyS88sBUT_Tk",
        "colab_type": "text"
      },
      "source": [
        "## 공통 영역: Word Embedding을 위한 Hyper parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT_z_0uNV8fp",
        "colab_type": "code",
        "outputId": "db38c131-b4d3-4fbb-c31b-c8fadb7ac3f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# Hyper Param setting\n",
        "\n",
        "# token의 Histogram 분포를 바탕으로 대부분의 단어 길이 cover 가능한 단어 개수 찾기\n",
        "# 신경망 학습을 위한 input 벡터 길이로 사용 - 적정 길이는 tokenizng 이후 분포를 보고 결정(코드 하단)\n",
        "totalLenSent = [len(x) for x in df_ep_sample['okt_token']] # 각 document의 단어 길이를 check\n",
        "plt.hist(totalLenSent,bins = np.arange(0,max(totalLenSent),max(totalLenSent)/20))\n",
        "\n",
        "print(np.percentile(totalLenSent, 95)) # 95%를 커버하는 수치는 41\n",
        "\n",
        "# MAX_LEN = int(np.percentile(totalLenSent, 95)) but bert는 128 embedding 사용\n",
        "MAX_LEN = 128\n",
        "print(MAX_LEN)\n",
        "\n",
        "# pre-trained Embedding을 몇 개 사용할 지 결정\n",
        "NUM_MODELS = 1\n",
        "\n",
        "# input data 원문에서 보존할 최대 단어 개수 \n",
        "# 전체 데이터셋에서 나타나는 unique 한 단어 수(넉넉하게 백단위 올림하여 setting)\n",
        "from itertools import chain\n",
        "\n",
        "sum_lists = list(chain.from_iterable(df_ep_sample['okt_token']))\n",
        "totalCntWords = int(math.ceil(len(set(sum_lists))/100)*100)\n",
        "\n",
        "MAX_FEATURES = totalCntWords\n",
        "print(len(set(sum_lists)), MAX_FEATURES)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28.799999999999955\n",
            "128\n",
            "1528 1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAATT0lEQVR4nO3df0zV973H8dfhIIkrioCccypCWyxt\nGm3pHzMbaYsZBrD8mGe0LDFzmWcu3YyRIEvXUm/cyoqyqyPE/tHKzDabuKWNdZyN41oBf2CmK7Ju\nw25qYloSWOSc5hTxVxU9nvuH955dJi1wvuf06Ifn4y/Oh3M4b/IlT04+nO8XWzgcDgsAYJSkRA8A\nAIg94g4ABiLuAGAg4g4ABiLuAGCg5EQPIEkPP/xwokcAgLvSmTNnJly/I+IuffaAAICJfd4LY7Zl\nAMBAxB0ADETcAcBAxB0ADETcAcBAxB0ADETcAcBAxB0ADETcAcBAd8wZqlZcPzek0MfDUT/enuXS\nrHsXxnAiAEgsI+Ie+nhYHzf8IOrHZ219nbgDMArbMgBgIOIOAAYi7gBgIOIOAAYi7gBgIOIOAAYi\n7gBgIOIOAAYi7gBgIOIOAAaa9PIDDQ0NOnz4sDIzM9XR0SFJqqur00cffSRJunjxoubMmSOv16uh\noSGVl5frgQcekCQVFBSosbExjuMDACYyadyrq6u1evVqvfDCC5G11tbWyMfNzc1KTU2N3M7NzZXX\n643xmACA6Zh0W2bp0qVKS0ub8HPhcFh//OMfVVlZGfPBAADRs7Tn3tfXp8zMTN1///2RtaGhIbnd\nbq1evVp9fX1W5wMARMHSJX87OjrGvWp3OBw6dOiQ0tPT9cEHH2j9+vXy+Xzjtm0AAPEX9Sv3Gzdu\nqLOzU+Xl5ZG1lJQUpaenS5KWLFmi3NzcyB9eAQBfnKjjfuzYMeXl5cnlckXWPvnkE4VCIUnS4OCg\nBgYGlJOTY31KAMC0TLotU19fr97eXo2MjKioqEgbNmxQTU2N9u/fr4qKinH3PXHihHbs2KHk5GQl\nJSXp5Zdf1rx58+I2PABgYpPGvaWlZcL15ubm29bKyspUVlZmfSoAgCWcoQoABiLuAGAg4g4ABiLu\nAGAg4g4ABiLuAGAg4g4ABiLuAGAg4g4ABrJ0VUhj2O262h/95YntWS7NundhDAcCAGuIu6SbF84r\n+MrzUT8+a+vrxB3AHYVtGQAwEHEHAAMRdwAwEHEHAAMRdwAwEHEHAAMRdwAw0KRxb2hoUGFhoSor\nKyNrr776qp566imtXLlSK1eu1JEjRyKf27lzp0pKSlRWVqajR4/GZ2oAwOea9CSm6upqrV69Wi+8\n8MK49TVr1mjt2rXj1s6ePSufzyefzye/3y+Px6N3331Xdrs9tlMDAD7XpK/cly5dqrS0tCl9se7u\nblVUVCglJUU5OTm677771N/fb3lIAMD0RL3nvmfPHlVVVamhoUGjo6OSJL/fL5fLFbmP0+mU3++3\nPiUAYFqiivuqVavU2dkpr9crh8Oh5ubmWM8FALAgqrjPnz9fdrtdSUlJqqmp0cmTJyXdeqU+PDwc\nuZ/f75fT6YzNpACAKYsq7oFAIPJxV1eX8vPzJUnFxcXy+XwaGxvT4OCgBgYG9Nhjj8VmUgDAlE36\nbpn6+nr19vZqZGRERUVF2rBhg3p7e3X69GlJUnZ2thobGyVJ+fn5evrpp1VeXi673a7NmzfzThkA\nSIBJ497S0nLbWk1NzWfef926dVq3bp21qQAAlnCGKgAYiLgDgIGIOwAYiLgDgIGIOwAYiLgDgIGI\nOwAYiLgDgIGIOwAYiLgDgIGIOwAYiLgDgIGIOwAYiLgDgIGIOwAYiLgDgIGIOwAYiLgDgIGIOwAY\naNL/odrQ0KDDhw8rMzNTHR0dkqSf/exnOnTokGbNmqXc3Fxt3bpVc+fO1dDQkMrLy/XAAw9IkgoK\nCiL/PBsA8MWZ9JV7dXW1du3aNW7tiSeeUEdHh/7whz/o/vvv186dOyOfy83NldfrldfrJewAkCCT\nxn3p0qVKS0sbt/bkk08qOfnWi/7HH39cw8PD8ZkOABAVy3vub7/9toqKiiK3h4aG5Ha7tXr1avX1\n9Vn98gCAKEy65/55XnvtNdntdn3961+XJDkcDh06dEjp6en64IMPtH79evl8PqWmpsZkWADA1ET9\nyn3fvn06fPiwtm/fLpvNJklKSUlRenq6JGnJkiXKzc3VRx99FJtJAQBTFlXce3p6tGvXLr322mua\nPXt2ZP2TTz5RKBSSJA0ODmpgYEA5OTmxmRQAMGWTbsvU19ert7dXIyMjKioq0oYNG9TW1qaxsTF5\nPB5J/37L44kTJ7Rjxw4lJycrKSlJL7/8subNmxf3bwIAMN6kcW9pabltraamZsL7lpWVqayszPpU\nAABLOEMVAAxE3AHAQMQdAAxE3AHAQMQdAAxE3AHAQMQdAAxE3AHAQMQdAAxE3AHAQMQdAAxE3AHA\nQMQdAAxE3AHAQMQdAAxE3AHAQMQdAAxE3AHAQFOKe0NDgwoLC1VZWRlZO3/+vDwej0pLS+XxeDQ6\nOipJCofDeuWVV1RSUqKqqir94x//iM/kAIDPNKW4V1dXa9euXePW2traVFhYqAMHDqiwsFBtbW2S\npJ6eHg0MDOjAgQP66U9/qp/85CcxHxoA8PmmFPelS5cqLS1t3Fp3d7fcbrckye12q6ura9y6zWbT\n448/rgsXLigQCMR4bADA54l6zz0YDMrhcEiSsrKyFAwGJUl+v18ulytyP5fLJb/fb3FMAMB0xOQP\nqjabTTabLRZfCgAQA1HHPTMzM7LdEggElJGRIUlyOp0aHh6O3G94eFhOp9PimACA6Yg67sXFxWpv\nb5cktbe3a/ny5ePWw+Gw/va3v2nOnDmR7RsAwBcjeSp3qq+vV29vr0ZGRlRUVKQNGzboueeeU11d\nnfbu3asFCxaotbVVkrRs2TIdOXJEJSUlmj17trZs2RLXbwAAcLspxb2lpWXC9d27d9+2ZrPZ9OMf\n/9jaVAAASzhDFQAMRNwBwEDEHQAMRNwBwEDEHQAMRNwBwEDEHQAMRNwBwEDEHQAMRNwBwEDEHQAM\nRNwBwEDEHQAMRNwBwEDEHQAMRNwBwEDEHQAMRNwBwEBT+jd7E/nwww+1cePGyO3BwUHV1tbq4sWL\neuutt5SRkSHp1v9fXbZsmfVJAQBTFnXc8/Ly5PV6JUmhUEhFRUUqKSnRvn37tGbNGq1duzZmQwIA\npicm2zLHjx9XTk6OsrOzY/HlAAAWxSTuPp9PlZWVkdt79uxRVVWVGhoaNDo6GounAABMg+W4j42N\n6eDBg1qxYoUkadWqVers7JTX65XD4VBzc7PlIQEA02M57j09PVq8eLHmz58vSZo/f77sdruSkpJU\nU1OjkydPWh4SADA9luPu8/lUUVERuR0IBCIfd3V1KT8/3+pTAACmKep3y0jSlStXdOzYMTU2NkbW\ntm3bptOnT0uSsrOzx30OAPDFsBT3L33pS3rvvffGrW3bts3SQAAA6zhDFQAMRNwBwEDEHQAMZGnP\nHf/LbtfV/r7oH57l0qx7F8ZwIAAzHXGPgZsXziv4yvNRPz5r6+vEHUBMsS0DAAYi7gBgIOIOAAYi\n7gBgIOIOAAYi7gBgIOIOAAYi7gBgIOIOAAYi7gBgIOIOAAYi7gBgIOIOAAYi7gBgIMuX/C0uLtY9\n99yjpKQk2e127du3T+fPn9fGjRv1r3/9S9nZ2WptbVVaWlos5gUATEFMXrnv3r1bXq9X+/btkyS1\ntbWpsLBQBw4cUGFhodra2mLxNACAKYrLtkx3d7fcbrckye12q6urKx5PAwD4DDGJ+9q1a1VdXa03\n33xTkhQMBuVwOCRJWVlZCgaDsXgaAMAUWd5z/+1vfyun06lgMCiPx6O8vLxxn7fZbLLZbFafBgAw\nDZZfuTudTklSZmamSkpK1N/fr8zMTAUCAUlSIBBQRkaG1acBAEyDpbhfuXJFly5dinz8pz/9Sfn5\n+SouLlZ7e7skqb29XcuXL7c+KQBgyixtywSDQa1fv16SFAqFVFlZqaKiIj366KOqq6vT3r17tWDB\nArW2tsZkWADA1FiKe05Ojn7/+9/ftp6enq7du3db+dIAAAs4QxUADETcAcBAxB0ADETcAcBAxB0A\nDETcAcBAxB0ADETcAcBAxB0ADETcAcBAxB0ADETcAcBAxB0ADETcAcBAxB0ADETcAcBAxB0ADETc\nAcBAUf+bvXPnzulHP/qRgsGgbDabvvnNb+o73/mOXn31Vb311lvKyMiQJNXX12vZsmUxGxgAMLmo\n42632/Xiiy9q8eLFunTpkp555hk98cQTkqQ1a9Zo7dq1MRvSeHa7rvb3Rf/wLJdm3bswhgMBuNtF\nHXeHwyGHwyFJSk1NVV5envx+f8wGm0luXjiv4CvPR/34rK2vE3cA48Rkz31oaEinTp1SQUGBJGnP\nnj2qqqpSQ0ODRkdHY/EUAIBpsBz3y5cvq7a2Vi+99JJSU1O1atUqdXZ2yuv1yuFwqLm5ORZzAgCm\nwVLcr1+/rtraWlVVVam0tFSSNH/+fNntdiUlJammpkYnT56MyaAAgKmLOu7hcFibNm1SXl6ePB5P\nZD0QCEQ+7urqUn5+vrUJAQDTFvUfVP/yl7/I6/XqoYce0sqVKyXdettjR0eHTp8+LUnKzs5WY2Nj\nbCYFAExZ1HH/8pe/rDNnzty2znvaASDxOEMVAAxE3AHAQMQdAAwU9Z477iBcvgDAfyDuBuDyBQD+\nE9syAGAg4g4ABiLuAGAg4g4ABiLuAGAg4g4ABiLuAGAg4g4ABuIkJiTc9XNDCn08HPXjOcMWuB1x\nR8KFPh7Wxw0/iPrxnGEL3I64g2vTAAYi7rB+bZr//oWlbZXw2LWoHwtgYsQdlln95ZD5X9tiOA0A\niXfLAICR4vbKvaenR01NTbp586Zqamr03HPPxeupgLsa7xZCPMQl7qFQSI2NjfrVr34lp9OpZ599\nVsXFxXrwwQfj8XSY6Sz+Qdh2zxyFL1+M/uktxpV3C1nDL8eJxSXu/f39uu+++5STkyNJqqioUHd3\nN3FHXMRiz59/dnL34pfjxGzhcDgc6y/6zjvv6OjRo2pqapIktbe3q7+/X5s3b57w/g8//HCsRwCA\nGeHMmTMTrt8R75b5rOEAANGJy7tlnE6nhof/vQfm9/vldDrj8VQAgAnEJe6PPvqoBgYGNDg4qLGx\nMfl8PhUXF8fjqQAAE4jLtkxycrI2b96s733vewqFQnrmmWeUn58fj6cCAEwgLn9QBQAkFmeoAoCB\niDsAGOiujntPT4/KyspUUlKitra2RI8Td+fOndO3v/1tlZeXq6KiQrt375YknT9/Xh6PR6WlpfJ4\nPBodHU3wpPEVCoXkdrv1/e9/X5I0ODiompoalZSUqK6uTmNjYwmeMH4uXLig2tparVixQk8//bT+\n+te/zqjj/+tf/1oVFRWqrKxUfX29rl27NqOO/3TctXH/v0sc7Nq1Sz6fTx0dHTp79myix4oru92u\nF198Ufv379ebb76p3/zmNzp79qza2tpUWFioAwcOqLCw0PhfdG+88YYWLVoUub19+3atWbNGnZ2d\nmjt3rvbu3ZvA6eKrqalJTz31lN555x15vV4tWrRoxhx/v9+vN954Q2+//bY6OjoUCoXk8/lm1PGf\njrs27v//EgcpKSmRSxyYzOFwaPHixZKk1NRU5eXlye/3q7u7W263W5LkdrvV1dWVyDHjanh4WIcP\nH9azzz4rSQqHw/rzn/+ssrIySdI3vvENY38OLl68qBMnTkS+95SUFM2dO3dGHf9QKKSrV6/qxo0b\nunr1qrKysmbM8Z+uuzbufr9fLpcrctvpdMrv9ydwoi/W0NCQTp06pYKCAgWDQTkcDklSVlaWgsFg\ngqeLny1btuj5559XUtKtH92RkRHNnTtXycm33tXrcrmM/TkYGhpSRkaGGhoa5Ha7tWnTJl25cmXG\nHH+n06nvfve7+trXvqYnn3xSqampWrx48Yw5/tN118Z9Jrt8+bJqa2v10ksvKTU1ddznbDabbDZb\ngiaLr0OHDikjI0NLlixJ9CgJcePGDf3zn//UqlWr1N7ertmzZ9+2BWPy8R8dHVV3d7e6u7t19OhR\nffrppzp69Giix7pj3RHXlonGTL3EwfXr11VbW6uqqiqVlpZKkjIzMxUIBORwOBQIBJSRkZHgKePj\n/fff18GDB9XT06Nr167p0qVLampq0oULF3Tjxg0lJydreHjY2J8Dl8sll8ulgoICSdKKFSvU1tY2\nY47/sWPHtHDhwsj3V1paqvfff3/GHP/pumtfuc/ESxyEw2Ft2rRJeXl58ng8kfXi4mK1t7dLunUF\nzuXLlydqxLj64Q9/qJ6eHh08eFAtLS366le/qp///Of6yle+onfffVeS9Lvf/c7Yn4OsrCy5XC59\n+OGHkqTjx49r0aJFM+b4L1iwQH//+9/16aefKhwO6/jx43rwwQdnzPGfrrv6DNUjR45oy5YtkUsc\nrFu3LtEjxVVfX5++9a1v6aGHHorsOdfX1+uxxx5TXV2dzp07pwULFqi1tVXz5s1L8LTx9d577+mX\nv/yldu7cqcHBQW3cuFGjo6N65JFHtH37dqWkpCR6xLg4deqUNm3apOvXrysnJ0dbt27VzZs3Z8zx\n37Fjh/bv36/k5GQ98sgjampqkt/vnzHHfzru6rgDACZ2127LAAA+G3EHAAMRdwAwEHEHAAMRdwAw\nEHEHAAMRdwAw0P8AC8GXvzLTw74AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwIUNatxUKw0",
        "colab_type": "text"
      },
      "source": [
        "## Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZkCMh37IwK",
        "colab_type": "code",
        "outputId": "5dec2b86-1233-46e1-a815-ac108da11e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!pip install sacremoses sentencepiece "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 9.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.28.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=581eea7d08e2137c767e34dab7890be9a9812ee29d2129c9b867f3252ae83d0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece\n",
            "Successfully installed sacremoses-0.0.35 sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46mrhFm373Ag",
        "colab_type": "code",
        "outputId": "a3369dd3-0a1c-4f9e-8cd1-c5b63ba240c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# !ls ../gdrive/My\\ Drive/data/transformers\n",
        "!ls ../gdrive/My\\ Drive/data/bert/bert-base-multilingual-cased"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-multilingual-cased-vocab.txt\tbert_config.json  pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7LqtYkGG3YZ",
        "colab_type": "text"
      },
      "source": [
        "### Pytorch 환경 내에서 BERT를 사용하기 위한 BERT 관련 Library Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW3fUFWuZoBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_PRETRAINED = \"../gdrive/My Drive/data/bert/bert-base-multilingual-cased\"\n",
        "PATH_VOCAB = \"../gdrive/My Drive/data/bert/bert-base-multilingual-cased\"\n",
        "PATH_BERT = \"../gdrive/My Drive/data/bert\"\n",
        "sys.path.append(PATH_PRETRAINED)\n",
        "sys.path.append(PATH_BERT)\n",
        "\n",
        "import sacremoses\n",
        "import sentencepiece\n",
        "\n",
        "import pickle\n",
        "import shutil\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn # for neural net\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell # jupyter에서 마지막 값만 출력하는게 아니라 모든 출력값을 매번 연속적으로 출력\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # all, last, last_expr, none (기본값은 'last_expr')\n",
        "\n",
        "# from transformers import convert_tf_checkpoint_to_pytorch\n",
        "from transformers import convert_bert_original_tf_checkpoint_to_pytorch\n",
        "\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification, BertAdam\n",
        "from transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel, BertConfig\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "from transformers import BertConfig # This is the Bert configuration file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QScZ6eLIHP6m",
        "colab_type": "text"
      },
      "source": [
        "### BERT 사용 관련 Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FI7QIBsBbMX",
        "colab_type": "code",
        "outputId": "0d0d3f60-8645-4ec8-a48b-7b8ff4e95fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "seed = 42\n",
        "MAX_SEQ_LEN = MAX_LEN # token분포 바탕으로 128 선정 (대부분의 단어 길이 cover)\n",
        "\n",
        "NUM_LABELS = len(df_ep_sample['target'].unique()) #2 If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy)\n",
        "training_epochs = 3\n",
        "lr = 2e-5\n",
        "warmup = 0.05\n",
        "batch_size = 32\n",
        "\n",
        "bert_model_config = PATH_PRETRAINED+'/bert_config.json'\n",
        "\n",
        "bert_model = 'bert-base-multilingual-cased'\n",
        "do_lower_case = 'uncased' in bert_model\n",
        "device = torch.device('cuda') # GPU 사용 setting\n",
        "\n",
        "output_model_file = 'bert_pytorch.bin'\n",
        "output_optimizer_file = 'bert_pytorch_optimizer.bin'\n",
        "# output_amp_file = 'bert_pytorch_amp.bin'\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe2cdb2b7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5dISpkhc1sS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
        "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification (or regression if config.num_labels==1) loss.\n",
        "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
        "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForSequenceClassification, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
        "\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids,\n",
        "                            position_ids=position_ids,\n",
        "                            head_mask=head_mask,\n",
        "                            inputs_embeds=inputs_embeds)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSKjJZl_IzLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the lines to BERT format # do token-convert-to-ids\n",
        "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
        "def convert_bert_token(example, max_seq_length,tokenizer):\n",
        "    max_seq_length -=2\n",
        "    all_tokens = []\n",
        "    longer = 0\n",
        "    for text in tqdm_notebook(example):\n",
        "        # print(\"text :\", text)\n",
        "        tokens_a = tokenizer.tokenize(text)\n",
        "        # print(\"tokens_a : \", tokens_a)\n",
        "        if len(tokens_a)>max_seq_length:  #token의 길이가 max_seq_length보다 길면 max_seq_length 뒤로는 잘라내고, longer 변수를 1증가 시킴\n",
        "            tokens_a = tokens_a[:max_seq_length]\n",
        "            longer += 1\n",
        "        # token의 앞 뒤에 [CLS]와 [SEP]을 추가 시키고 남는자리는 zero padding\n",
        "        # print(\"max_seq_length: \", max_seq_length, \"len(tokens_a): \", len(tokens_a), \"max_seq_length - len(tokens_a) : \", max_seq_length - len(tokens_a))\n",
        "\n",
        "        # \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+ [0] * int(max_seq_length - len(tokens_a)) # token을 vocab을 이용하여 id로 convert\n",
        "        all_tokens.append(one_token) # all_tokens에 추가\n",
        "    # print(longer)\n",
        "    return np.array(all_tokens)\n",
        "\n",
        "def create_attention_masks(sequence):\n",
        "# Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # Create a mask of 1s for each token followed by 0s for padding\n",
        "  for seq in sequence:\n",
        "    # print(\"seq is \", seq)\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "  # print(attention_masks)\n",
        "  return attention_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGQSs3C29ki",
        "colab_type": "code",
        "outputId": "1227d7e4-a175-45b3-eab1-e07972bcd349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267,
          "referenced_widgets": [
            "dd676e870a48466389004f2f8342632a",
            "f908e03921234e529efa121320b88f72",
            "6de3dc76d666471ea1b8d816ceec3506",
            "03cb2dd3ef7b43d9831b9014ae8c7fb9",
            "58076072a951474e830e581a30c9ba09",
            "c3d5238c25b04e10b59b68c7a5bc2a26",
            "2a17d65e220048869231be3d046e8f9c",
            "5072f4a0de1f40ffa4ff8132b0036935"
          ]
        }
      },
      "source": [
        "# OKT로 Tokenize 한 데이터를 string으로 붙인 뒤 이를 다시 bert 형태로 tokenizing\n",
        "\n",
        "%%time\n",
        "tokenizer = BertTokenizer.from_pretrained(PATH_VOCAB+'/bert-base-multilingual-cased-vocab.txt',\n",
        "                                          do_lower_case=do_lower_case)\n",
        "\n",
        "# tokens_a :  ['옷', '입', '##고', '목', '##욕', '##탕', '들어', '##가는', '##거', '웃', '##기', '[UNK]', '저', '거', '명', '##품', '일', '텐', '##데']\n",
        "# tokens_a :  ['[UNK]', '선', '황', '##제', '가', '바', '##람', '은', '못', '[UNK]', '같다', '##는', '생', '##각', '이', '든', '##다', '태', '##후', '가', '황', '##후', '로', '있는', '##한', '감', '##히', '생', '##각', '도', '못', '했', '##을', '##듯']\n",
        "\n",
        "# train_df의 \"comment_text\"에서 na를 \"DUMMY_VALUE\"로 채우고, 최대 MAX_SEQUENCE_LENGTH 만큼 잘라냄\n",
        "sequences = convert_bert_token(df_ep_sample[\"okt_token_str\"].fillna(\"DUMMY_VALUE\"),MAX_SEQ_LEN, tokenizer)\n",
        "attention_masks = np.asarray(create_attention_masks(sequences))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.tokenization_utils:Model name '../gdrive/My Drive/data/bert/bert-base-multilingual-cased/bert-base-multilingual-cased-vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '../gdrive/My Drive/data/bert/bert-base-multilingual-cased/bert-base-multilingual-cased-vocab.txt' is a path or url to a directory containing tokenizer files.\n",
            "INFO:transformers.tokenization_utils:Didn't find file ../gdrive/My Drive/data/bert/bert-base-multilingual-cased/added_tokens.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils:Didn't find file ../gdrive/My Drive/data/bert/bert-base-multilingual-cased/special_tokens_map.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils:Didn't find file ../gdrive/My Drive/data/bert/bert-base-multilingual-cased/tokenizer_config.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils:loading file ../gdrive/My Drive/data/bert/bert-base-multilingual-cased/bert-base-multilingual-cased-vocab.txt\n",
            "INFO:transformers.tokenization_utils:loading file None\n",
            "INFO:transformers.tokenization_utils:loading file None\n",
            "INFO:transformers.tokenization_utils:loading file None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd676e870a48466389004f2f8342632a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CPU times: user 296 ms, sys: 21 ms, total: 318 ms\n",
            "Wall time: 1.23 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uaF3knUfY_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BertTokenization 한 Embedding + Target 변수 (Series를 numpy로 변환하면 (x,) 형태의 출력이기 때문에 열 추가를 위해 reshape 해줌)\n",
        "# all_lines = np.hstack((sequences, df_ep_sample['target'].to_numpy().reshape(-1,1)))\n",
        "# all_lines.shape\n",
        "\n",
        "X = sequences\n",
        "Y = df_ep_sample['target'].to_numpy()\n",
        "# Y = df_ep_sample['target'].to_numpy().reshape(-1,1)\n",
        "\n",
        "# Train & Test Set 분리\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.15, random_state=seed)\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.15, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DZFwNyQ1Q0I",
        "colab_type": "code",
        "outputId": "6fc7d4db-4e7d-4ad6-e320-742dd73adfb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Dataset을 상속한 TensorDataset은 train data x와 레이블 y를 묶어놓은 컨테이너로 tensor만 전달 가능함\n",
        "# X는 torch.long 형태의 텐서로, y는 torch.float 타입의 텐서로 입력하여 pytorch에서 연산할 수 있는 기본 구조로 변경하여 train_dataset으로 할당\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_x,dtype=torch.long), torch.tensor(train_y,dtype=torch.float))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_x,dtype=torch.long), torch.tensor(test_y,dtype=torch.float))\n",
        "# train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_x), torch.tensor(train_y))\n",
        "# test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_x), torch.tensor(test_y))\n",
        "\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(train_x.dtype)\n",
        "print(train_y.dtype)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(355, 128)\n",
            "(355,)\n",
            "int64\n",
            "int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7_FsJ-CF_lA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c462603d-a321-410d-eac7-a7052d90a2b4"
      },
      "source": [
        "train_dataset.tensors[1].shape\n",
        "train_dataset.tensors[1].unsqueeze(1).shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([355])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([355, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2tlEqUDDehg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "89cd474d-3a54-4d16-ac3e-4c2fbe2323c7"
      },
      "source": [
        "# m = nn.Sigmoid()\n",
        "# input = torch.randn(2)\n",
        "# print(input, input.shape)\n",
        "# output = m(input)\n",
        "# print(output, output.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.5896, -0.1942]) torch.Size([2])\n",
            "tensor([0.6433, 0.4516]) torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2--Gr8kLFyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(y_preds, real_ys):\n",
        "    pred_flat = np.argmax(y_preds, axis=1).flatten()\n",
        "    labels_flat = real_ys.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# 평가 헬퍼 함수\n",
        "def bert_eval_net(net, data_loader, device=\"cuda\"):\n",
        "  # Dropout 및 BatchNorm 무효화\n",
        "  net.eval()\n",
        "\n",
        "  eval_losse = 0\n",
        "  eval_acc = 0\n",
        "\n",
        "  running_loss = 0.\n",
        "  running_acc = 0.\n",
        "\n",
        "  for x, y in data_loader:\n",
        "    x=x.to(device)\n",
        "    y=y.to(device)\n",
        "    with torch.no_grad():\n",
        "      y_preds = net(x, token_type_ids=None, attention_mask=(x>0).to(device), labels=None)  # forward\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    y_preds = y_preds.detach().cpu().numpy()\n",
        "    real_ys = y.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(y_preds, real_ys)\n",
        "\n",
        "    eval_acc += tmp_eval_accuracy\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_acc/len(x)))\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "# Train 함수\n",
        "def bert_train_net(net, train_loader, optimizer, device=\"cuda\"):\n",
        "  tq = tqdm_notebook(range(training_epochs))\n",
        "\n",
        "  for epoch in tq:\n",
        "    running_loss = 0.\n",
        "\n",
        "    # 신경망을 훈련 모드로 설정\n",
        "    net.train()\n",
        "    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
        "    \n",
        "    # iteration 1회에 train_loader의 batch_size (여기서는 64)만큼씩 읽어와 한꺼번에 batch처리 batch_size * i (여기서는 i = ) 가 전체 train data set의 크기가 될때까지 loop\n",
        "    for i,(x, y) in tk0:\n",
        "      x=x.to(device) # len(x)는 batch_size\n",
        "      y=y.to(device)\n",
        "\n",
        "      y_pred = net(x, token_type_ids=None, attention_mask=(x>0).to(device), labels=None)  # forward\n",
        "      print(y_pred, \"----------------\", y_pred.view(-1,NUM_LABELS).shape)\n",
        "      print(y.shape, \"----------------\", y.view(-1).shape)\n",
        "      loss = F.binary_cross_entropy_with_logits(y_pred,y.to(device))\n",
        "      # loss = F.binary_cross_entropy_with_logits(y_pred[:,1],y.to(device))\n",
        "      # loss = net(x, token_type_ids=None, attention_mask=(x>0).to(device), labels=y) \n",
        "      # print(loss)\n",
        "      print(\"optimizer.zero_grad().................\")\n",
        "      optimizer.zero_grad() # step과 zero_grad는 쌍을 이루는 것이라고 생각하면 됨 # optimizer의 gradient를 0으로 초기화\n",
        "\n",
        "      loss.backward() # backpropagation\n",
        "      optimizer.step() # update gradients\n",
        "      running_loss += loss.item() # loss calculate\n",
        "\n",
        "    train_losses.append(running_loss/len(train_loader))\n",
        "\n",
        "    torch.save(model.state_dict(), output_model_file)\n",
        "    print(\"epoch: {}/{} | avg_train_loss: {:.4f} \".format(epoch, training_epochs, train_losses[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMkB5nm7GoA9",
        "colab_type": "code",
        "outputId": "93505b6d-9947-45e6-f1f2-ab21d6b3f713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# BertForSequenceClassification is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. \n",
        "# As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "bert_config = BertConfig.from_json_file(bert_model_config)\n",
        "bert_config.num_labels = 1\n",
        "\n",
        "# load pre-trained BERT model's weight in ../\n",
        "model = BertForSequenceClassification.from_pretrained(PATH_PRETRAINED, config=bert_config)\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', cache_dir=None, num_labels=len(np.unique(Y)))\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "# no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "# optimizer = AdamW(optimizer_grouped_parameters, lr=lr, warmup=.1)\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.modeling_utils:loading weights file ../gdrive/My Drive/data/bert/bert-base-multilingual-cased/pytorch_model.bin\n",
            "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twe-4xUHvIwk",
        "colab_type": "code",
        "outputId": "7d773c2b-062f-4cfa-9da2-5e6a87f6445d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f6f83c7be57f4cef8894c634382f0f04",
            "c8c1542381a94a9aa4665a9ce508b4fb",
            "3f148d85f820483f8312afae56575d7a",
            "f2d1c979061f41ecb72b4f6e6a821111",
            "e2f85d3eb2754ef5bf48d0d17559ccee",
            "73ca5bbab4f74be380f98f06233b9045",
            "a5fa2f828a23467587117843adea161d",
            "0cbf9562a20b465b9b23a96e315b5d0b",
            "06c6354c51584136ad4e394900178fa0",
            "ebfc5f510ca04571877be6a588d1d7db",
            "d433bceb730347e9a1bab3b1a22ccbf6",
            "94bebbe6c0f44697939ae3c5091fc6e8",
            "8e31dac415874791941a857eb8db0c56",
            "41dae93ae9f245429b2118fab7953fde",
            "fdb9d32617f64d529ab210e0757e82bf",
            "898fa2bf60644d20a9392f82b0121482"
          ]
        }
      },
      "source": [
        "model.to(device) # GPU 연산을 위해 cuda로 전송\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "bert_train_net(model, train_loader, optimizer, device)\n",
        "bert_eval_net(model, test_loader, device)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6f83c7be57f4cef8894c634382f0f04",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06c6354c51584136ad4e394900178fa0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f2e602b0244a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbert_train_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbert_eval_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-5b30aa78c0bc>\u001b[0m in \u001b[0;36mbert_train_net\u001b[0;34m(net, train_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"----------------\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"----------------\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-4479ec70129e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m     44\u001b[0m                             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                             inputs_embeds=inputs_embeds)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/My Drive/data/bert/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    725\u001b[0m                                        \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                                        \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m                                        encoder_attention_mask=encoder_extended_attention_mask)\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/My Drive/data/bert/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/My Drive/data/bert/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/My Drive/data/bert/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/gdrive/My Drive/data/bert/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0msee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 15.90 GiB total capacity; 15.08 GiB already allocated; 13.88 MiB free; 113.76 MiB cached)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqXS2alKvNZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_losses)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QMVH-Ei_Yl0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}