{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Media_Project_#최종_ver1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30fd4be8b80642d592d32b0abd4fc3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_64ea3acbdd574bd2bb3795d9254e873d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_99b96ba89ccd471dac28fab5afd74d87",
              "IPY_MODEL_5d2b725ecfd845758bb8f1c17bebdc9d"
            ]
          }
        },
        "64ea3acbdd574bd2bb3795d9254e873d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99b96ba89ccd471dac28fab5afd74d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5ffc7fff35df4ecb8dfc9f62dcd0f036",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f06bf171dfdb470c9ea2aa913753116e"
          }
        },
        "5d2b725ecfd845758bb8f1c17bebdc9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59675feb4d4f44ba8ebb03d47516cc22",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32% 9/28 [00:13&lt;00:00, 37.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ca71af5af0e4ec5b9eca14030607ad3"
          }
        },
        "5ffc7fff35df4ecb8dfc9f62dcd0f036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f06bf171dfdb470c9ea2aa913753116e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59675feb4d4f44ba8ebb03d47516cc22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ca71af5af0e4ec5b9eca14030607ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaehyun0220/Colab/blob/master/Media_Project__%EC%B5%9C%EC%A2%85_ver1_kb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmdavC_fLDrU",
        "colab_type": "text"
      },
      "source": [
        "#10조. 네이버 댓글 분석을 통한 상위, 하위 클립 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnLbyZflC1R1",
        "colab_type": "code",
        "outputId": "45b5ab9d-33cd-433c-df35-42db030d872c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Auth 인증 및 Google Drive 활용 Data load\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5oQGfjLC-Jb",
        "colab_type": "code",
        "outputId": "12bada97-7806-4dfc-8f0c-1c91c7a235f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# 파일리스트 확인\n",
        "!ls ../gdrive/My\\ Drive/output"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_ep10.csv\t file_ep16.csv\tfile_ep22.csv  file_ep4.csv\n",
            "file_ep11.csv\t file_ep17.csv\tfile_ep23.csv  file_ep5.csv\n",
            "file_ep12.csv\t file_ep18.csv\tfile_ep24.csv  file_ep6.csv\n",
            "file_ep13.csv\t file_ep19.csv\tfile_ep25.csv  file_ep7.csv\n",
            "file_ep14_1.csv  file_ep1.csv\tfile_ep26.csv  file_ep8.csv\n",
            "file_ep14.csv\t file_ep20.csv\tfile_ep2.csv   file_ep9.csv\n",
            "file_ep15.csv\t file_ep21.csv\tfile_ep3.csv   TheLastEmpress.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bdQCRGv7AX6",
        "colab_type": "code",
        "outputId": "75c10b29-7c1c-446d-8040-7f11de6755f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!pip install regex"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\r\u001b[K     |▌                               | 10kB 29.5MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 552kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 573kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: regex\n",
            "Successfully installed regex-2019.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEGh8u0qDI6h",
        "colab_type": "code",
        "outputId": "f984232d-09e3-4c6e-bb9c-1d1631a6211d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "# 기본 라이브러리 로드\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import regex as re\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#데이터 전처리 관련 라이브러리 로드\n",
        "from sklearn import preprocessing\n",
        "\n",
        "#모델 평가를 위한 라이브러리 로드\n",
        "from sklearn import metrics, model_selection\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "\n",
        "#데이터 분리를 위한 라이브러리 로드\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#수학 & 통계 관련 라이브러리 로드\n",
        "import scipy.stats as st\n",
        "import math\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl  # 기본 설정 만지는 용도\n",
        "import matplotlib.pyplot as plt  # 그래프 그리는 용도\n",
        "import matplotlib.font_manager as fm  # 폰트 관련 용도\n",
        "\n",
        "\n",
        "#Configure Visualization Defaults\n",
        "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
        "%matplotlib inline\n",
        "mpl.style.use('ggplot')\n",
        "sns.set_style('white')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpUzHCYqMYp1",
        "colab_type": "code",
        "outputId": "fb776137-15bb-4e12-971d-8b801de57307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "filelist = os.listdir('../gdrive/My Drive/output')\n",
        "filelist"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['file_ep1.csv',\n",
              " 'file_ep16.csv',\n",
              " 'file_ep17.csv',\n",
              " 'file_ep10.csv',\n",
              " 'file_ep14.csv',\n",
              " 'file_ep12.csv',\n",
              " 'file_ep11.csv',\n",
              " 'file_ep14_1.csv',\n",
              " 'file_ep15.csv',\n",
              " 'file_ep13.csv',\n",
              " 'file_ep18.csv',\n",
              " 'file_ep19.csv',\n",
              " 'file_ep21.csv',\n",
              " 'file_ep20.csv',\n",
              " 'file_ep2.csv',\n",
              " 'file_ep22.csv',\n",
              " 'file_ep23.csv',\n",
              " 'file_ep25.csv',\n",
              " 'file_ep24.csv',\n",
              " 'file_ep26.csv',\n",
              " 'file_ep3.csv',\n",
              " 'file_ep4.csv',\n",
              " 'file_ep5.csv',\n",
              " 'file_ep6.csv',\n",
              " 'file_ep9.csv',\n",
              " 'file_ep7.csv',\n",
              " 'TheLastEmpress.csv',\n",
              " 'file_ep8.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Ink4z2DVoG",
        "colab_type": "code",
        "outputId": "445ac1c8-fcb6-4467-f3cc-48cf05be9589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "30fd4be8b80642d592d32b0abd4fc3be",
            "64ea3acbdd574bd2bb3795d9254e873d",
            "99b96ba89ccd471dac28fab5afd74d87",
            "5d2b725ecfd845758bb8f1c17bebdc9d",
            "5ffc7fff35df4ecb8dfc9f62dcd0f036",
            "f06bf171dfdb470c9ea2aa913753116e",
            "59675feb4d4f44ba8ebb03d47516cc22",
            "9ca71af5af0e4ec5b9eca14030607ad3"
          ]
        }
      },
      "source": [
        "# 총 26회차 491개 하이라이트 클립 존재 (전체 재생수 = 107,221,654 / 클립 당 평균 재생수 = 218,374), \n",
        "# 이 중에서 예고편, 미공개, 인터뷰 등 클립 제외하고 총 422회 클립 대상\n",
        "df_title = pd.read_csv('../gdrive/My Drive/output/TheLastEmpress.csv', encoding = 'euc-kr')\n",
        "df_title.rename(columns=lambda x: re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》 ]', '', x), inplace=True)\n",
        "\n",
        "# 조회수 분포를 고려하여 각 회차별로 조회수 상위 4개, 하위 4개 클립을 샘플링 - 총 208개 클립\n",
        "# 좋아요수, 댓글 수, 댓글 내용, 댓글 작성자 정보 (웹크롤링 통한 추출)\n",
        "\n",
        "# 상위 하위 클립을 나누어 조회수 내림차순으로 rank를 매김 (1~4는 상위 클립, 5~8은 하위 클립)\n",
        "df_ep_tot = pd.DataFrame()\n",
        "for i in tqdm_notebook(filelist):\n",
        "  if (i[:4] == 'file'):\n",
        "    df_ep_temp = pd.read_csv('../gdrive/My Drive/output/'+i)\n",
        "    df_ep_temp['play'] = df_ep_temp['play'].apply(lambda x: int(re.sub(',','', x[4:])))\n",
        "    df_ep_temp['rank'] = df_ep_temp['play'].rank(method='dense', ascending=False)\n",
        "    df_ep_tot = df_ep_tot.append(df_ep_temp)\n",
        "\n",
        "\n",
        "# 크롤링 한 데이터에서 불필요한 열 삭제\n",
        "df_ep_tot.drop(columns='Unnamed: 0', inplace=True)\n",
        "\n",
        "# 상위 클립은 1로 하위 클립은 0으로 분류\n",
        "df_ep_tot['target'] = np.where(df_ep_tot['rank']<=4,1,0)\n",
        "df_ep_tot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30fd4be8b80642d592d32b0abd4fc3be",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=28), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY1H1LINq65r",
        "colab_type": "text"
      },
      "source": [
        "### 댓글 중 분석 대상이 되는 contens null data 삭제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cA7gB0gV3HF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ep_sample = df_ep_tot.copy()\n",
        "df_ep_sample.dropna(how='any', inplace=True)\n",
        "df_ep_sample.info()\n",
        "df_ep_sample['target'].value_counts() # 3:1 imbalanced dataset (조회수가 많은 클립에 댓글이 많기 때문임))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyG91a40YN8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ep_sample['contents'] = df_ep_sample['contents'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "df_ep_sample.dropna(how='any',inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVe3C1sFrOjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 5:5 샘플링\n",
        "df_sample_temp = df_ep_sample[df_ep_sample['target'] == 1].sample(10000)\n",
        "df_sample_final = pd.concat([df_sample_temp,df_ep_sample[df_ep_sample['target'] == 0].sample(10000)])\n",
        "\n",
        "df_sample_final.info()\n",
        "df_sample_final.sample(4)\n",
        "df_sample_final['target'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyS88sBUT_Tk",
        "colab_type": "text"
      },
      "source": [
        "## 공통 영역: Word Embedding을 위한 Hyper parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT_z_0uNV8fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Param setting\n",
        "\n",
        "# token의 Histogram 분포를 바탕으로 대부분의 단어 길이 cover 가능한 단어 개수 찾기\n",
        "# # 신경망 학습을 위한 input 벡터 길이로 사용 - 적정 길이는 tokenizng 이후 분포를 보고 결정(코드 하단)\n",
        "# totalLenSent = [len(x) for x in df_ep_sample['okt_token']] # 각 document의 단어 길이를 check\n",
        "# plt.hist(totalLenSent,bins = np.arange(0,max(totalLenSent),max(totalLenSent)/20))\n",
        "\n",
        "# print(np.percentile(totalLenSent, 95)) # 95%를 커버하는 수치는 41\n",
        "\n",
        "# MAX_LEN = int(np.percentile(totalLenSent, 95)) but bert는 128 embedding 사용\n",
        "MAX_LEN = 128\n",
        "print(MAX_LEN)\n",
        "\n",
        "# pre-trained Embedding을 몇 개 사용할 지 결정\n",
        "NUM_MODELS = 1\n",
        "\n",
        "# # input data 원문에서 보존할 최대 단어 개수 \n",
        "# # 전체 데이터셋에서 나타나는 unique 한 단어 수(넉넉하게 백단위 올림하여 setting)\n",
        "# from itertools import chain\n",
        "\n",
        "# sum_lists = list(chain.from_iterable(df_ep_sample['okt_token']))\n",
        "# totalCntWords = int(math.ceil(len(set(sum_lists))/100)*100)\n",
        "\n",
        "MAX_FEATURES = 37000\n",
        "# MAX_FEATURES = totalCntWords\n",
        "# print(len(set(sum_lists)), MAX_FEATURES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwIUNatxUKw0",
        "colab_type": "text"
      },
      "source": [
        "## Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZkCMh37IwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sacremoses sentencepiece "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46mrhFm373Ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls ../gdrive/My\\ Drive/data/bert/bert-base-multilingual-cased"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7LqtYkGG3YZ",
        "colab_type": "text"
      },
      "source": [
        "### Pytorch 환경 내에서 BERT를 사용하기 위한 BERT 관련 Library Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dJuak78KBqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM0QsyTVlmy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp -r ../gdrive/My\\ Drive/data/bert/multi_cased_L-12_H-768_A-12 ./bert-base-multilingual-cased\n",
        "!mkdir ../bert_work/\n",
        "PATH_PRETRAINED = \"../gdrive/My Drive/data/bert/multi_cased_L-12_H-768_A-12/\"\n",
        "FILE_VOCAB = \"../gdrive/My Drive/data/bert/bert-base-multilingual-cased/bert-base-multilingual-cased-vocab.txt\"\n",
        "PATH_WORK = '../bert_work/'\n",
        "sys.path.append(PATH_PRETRAINED)\n",
        "sys.path.append(PATH_WORK)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW3fUFWuZoBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sacremoses\n",
        "import sentencepiece\n",
        "\n",
        "import pickle\n",
        "import shutil\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn # for neural net\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell # jupyter에서 마지막 값만 출력하는게 아니라 모든 출력값을 매번 연속적으로 출력\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # all, last, last_expr, none (기본값은 'last_expr')\n",
        "\n",
        "# from transformers import convert_tf_checkpoint_to_pytorch\n",
        "from transformers import convert_bert_original_tf_checkpoint_to_pytorch\n",
        "\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification, BertAdam\n",
        "from transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel, BertConfig\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "from transformers import BertConfig # This is the Bert configuration file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QScZ6eLIHP6m",
        "colab_type": "text"
      },
      "source": [
        "### BERT 사용 관련 Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FI7QIBsBbMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 42\n",
        "MAX_SEQ_LEN = MAX_LEN # token분포 바탕으로 128 선정 (대부분의 단어 길이 cover)\n",
        "\n",
        "NUM_LABELS = len(df_sample_final['target'].unique()) #2 If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy)\n",
        "training_epochs = 3\n",
        "lr = 5e-5\n",
        "batch_size = 32\n",
        "\n",
        "bert_model_config = PATH_PRETRAINED+'bert_config.json'\n",
        "\n",
        "bert_model = 'bert-base-multilingual-cased'\n",
        "do_lower_case = 'uncased' in bert_model\n",
        "device = torch.device('cuda') # GPU 사용 setting\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM54ZY3SYMGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
        "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification (or regression if config.num_labels==1) loss.\n",
        "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
        "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForSequenceClassification, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
        "\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids,\n",
        "                            position_ids=position_ids,\n",
        "                            head_mask=head_mask,\n",
        "                            inputs_embeds=inputs_embeds)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSKjJZl_IzLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the lines to BERT format # do token-convert-to-ids\n",
        "def convert_bert_token(example, max_seq_length,tokenizer):\n",
        "    max_seq_length -=2\n",
        "    all_tokens = []\n",
        "    for text in tqdm_notebook(example):\n",
        "        tokens_a = tokenizer.tokenize(text)\n",
        "        # print(tokens_a)\n",
        "        if len(tokens_a)>max_seq_length:  #token의 길이가 max_seq_length보다 길면 max_seq_length 뒤로는 잘라내고, longer 변수를 1증가 시킴\n",
        "            tokens_a = tokens_a[:max_seq_length]\n",
        "        # \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+ [0] * int(max_seq_length - len(tokens_a)) # token을 vocab을 이용하여 id로 convert\n",
        "        # print(one_token)\n",
        "        all_tokens.append(one_token) # all_tokens에 추가\n",
        "    # print(longer)\n",
        "    return np.array(all_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yRZn7DeLoqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls ../gdrive/My\\ Drive/data/bert/multi_cased_L-12_H-768_A-12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owHYd99sWXQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls ../bert_work/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znafyzm6NtAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convert_bert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
        "    PATH_PRETRAINED + 'bert_model.ckpt',\n",
        "    PATH_PRETRAINED + 'bert_config.json',\n",
        "    PATH_WORK + 'pytorch_model.bin')\n",
        "\n",
        "# 읽어온 BERT_MODEL_PATH bert_config.json file의 설정을 그대로 WORK_DIR의 bert_config.json이라는 이름으로 저장\n",
        "shutil.copyfile(PATH_PRETRAINED + 'bert_config.json', PATH_WORK + 'config.json')\n",
        "# shutil.copyfile(PATH_PRETRAINED + 'vocab.txt', PATH_WORK + 'vocab.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGQSs3C29ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OKT로 Tokenize 한 데이터를 string으로 붙인 뒤 이를 다시 bert 형태로 tokenizing\n",
        "\n",
        "%%time\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=do_lower_case)\n",
        "# ['니', '##는', '[UNK]', '황', '##후', '##는', '이', '##짓', '##하면', '##안', '##돼', '##냐', '진', '##실을', '말', '##한', '##것', '뿐', '##인', '##데']\n",
        "# ['정', '##말', '잘', '##하', '##셨', '##사', '##옵', '##니다', '태', '##태', '##마', '##마', '시', '##원', '##하게', '묵', '##은', '체', '##증', '##을', '날', '##리는', '듯', '하', '##셨', '##어', '##요', '민', '##유', '##라', '##한', '##테', '연속', '따', '##귀', '##를', '세', '##게', '갈', '##기는', '것', '##도', '모', '##자', '##라', '이', '##혁', '##한', '##테', '##도', '[UNK]', '까', '##지', '냈', '##으', '##니', '[UNK]', '진', '##짜', '끝', '##판', '##대', '##장이', '##시', '##다']\n",
        "# ['진', '##짜', '이름을', '되', '찾', '##을', '수', '있', '##었', '##어', '##요']\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained(os.path.join(PATH_WORK, 'vocab.txt'), cache_dir=None, do_lower_case=do_lower_case, do_basic_tokenize=False)\n",
        "# ['니', '##는', '[UNK]', '황', '##후', '##는', '이', '##짓', '##하면', '##안', '##돼', '##냐', '진', '##실을', '말', '##한', '##것', '뿐', '##인', '##데']\n",
        "# ['정', '##말', '잘', '##하', '##셨', '##사', '##옵', '##니다', '태', '##태', '##마', '##마', '시', '##원', '##하게', '묵', '##은', '체', '##증', '##을', '날', '##리는', '듯', '하', '##셨', '##어', '##요', '민', '##유', '##라', '##한', '##테', '연속', '따', '##귀', '##를', '세', '##게', '갈', '##기는', '것', '##도', '모', '##자', '##라', '이', '##혁', '##한', '##테', '##도', '[UNK]', '까', '##지', '냈', '##으', '##니', '[UNK]', '진', '##짜', '끝', '##판', '##대', '##장이', '##시', '##다']\n",
        "# ['진', '##짜', '이름을', '되', '찾', '##을', '수', '있', '##었', '##어', '##요']\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(FILE_VOCAB, cache_dir=None, do_lower_case=do_lower_case)\n",
        "\n",
        "# train_df의 \"comment_text\"에서 na를 \"DUMMY_VALUE\"로 채우고, 최대 MAX_SEQUENCE_LENGTH 만큼 잘라냄\n",
        "sequences = convert_bert_token(df_sample_final[\"contents\"].fillna(\"DUMMY_VALUE\"),MAX_SEQ_LEN, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk2-CMM-zeTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uaF3knUfY_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = sequences\n",
        "Y = df_sample_final['target'].to_numpy()\n",
        "\n",
        "# Train & Test Set 분리\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.15, random_state=seed)\n",
        "# train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.15, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DZFwNyQ1Q0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset을 상속한 TensorDataset은 train data x와 레이블 y를 묶어놓은 컨테이너로 tensor만 전달 가능함\n",
        "# X는 torch.long 형태의 텐서로, y는 torch.float 타입의 텐서로 입력하여 pytorch에서 연산할 수 있는 기본 구조로 변경하여 train_dataset으로 할당\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_x,dtype=torch.long), torch.tensor(train_y,dtype=torch.long))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_x,dtype=torch.long), torch.tensor(test_y,dtype=torch.long))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2--Gr8kLFyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_losses = []\n",
        "\n",
        "# Train 함수\n",
        "def bert_train_net(net, train_loader, optimizer, device=\"cuda\"):\n",
        "  tq = tqdm_notebook(range(training_epochs))\n",
        "\n",
        "  for epoch in tq:\n",
        "    tr_loss = 0.\n",
        "    nb_tr_examples, nb_tr_steps = 0,0\n",
        "\n",
        "    # 신경망을 훈련 모드로 설정\n",
        "    net.train()\n",
        "    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
        "    \n",
        "    # iteration 1회에 train_loader의 batch_size (여기서는 64)만큼씩 읽어와 한꺼번에 batch처리 batch_size * i (여기서는 i = ) 가 전체 train data set의 크기가 될때까지 loop\n",
        "    for i,(x, y) in tk0:\n",
        "      x=x.to(device) # len(x)는 batch_size\n",
        "      y=y.to(device)\n",
        "\n",
        "      loss, logit = net(x, token_type_ids=None, attention_mask=(x>0).to(device), labels=y)  # forward\n",
        "\n",
        "      train_losses.append(loss.item())\n",
        "      \n",
        "      optimizer.zero_grad() # step과 zero_grad는 쌍을 이루는 것이라고 생각하면 됨 # optimizer의 gradient를 0으로 초기화\n",
        "      loss.backward() # backpropagation\n",
        "      optimizer.step() # update gradients\n",
        "\n",
        "      # update tracking variables\n",
        "      tr_loss += loss.item()\n",
        "      nb_tr_examples += x.size(0)\n",
        "      nb_tr_steps += 1\n",
        "      train_losses.append(tr_loss/nb_tr_steps)\n",
        "\n",
        "    print(\"epoch: {}/{} | train_loss: {:.4f} \".format(epoch, training_epochs, tr_loss/nb_tr_steps))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMkB5nm7GoA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BertForSequenceClassification is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. \n",
        "# As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "bert_config = BertConfig.from_json_file(bert_model_config)\n",
        "bert_config.num_labels = NUM_LABELS\n",
        "\n",
        "# load pre-trained BERT model's weight in ../\n",
        "model = BertForSequenceClassification.from_pretrained(PATH_WORK, config=bert_config)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twe-4xUHvIwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to(device) # GPU 연산을 위해 cuda로 전송\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "bert_train_net(model, train_loader, optimizer, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr6ddOojQOy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_losses)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKU6jC1k9fLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = []\n",
        "labels = []\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(y_preds, real_ys):\n",
        "    pred_flat = np.argmax(y_preds, axis=1).flatten()\n",
        "    labels_flat = real_ys.flatten()\n",
        "    print(\"pred_flat is \", pred_flat)\n",
        "    print(\"labels_flat is \", labels_flat)\n",
        "    preds.append(pred_flat)\n",
        "    labels.append(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzJUqt5G9HEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 평가 헬퍼 함수\n",
        "def bert_eval_net(net, data_loader, device=\"cuda\"):\n",
        "  # Dropout 및 BatchNorm 무효화\n",
        "  net.eval()\n",
        "\n",
        "  eval_acc = 0\n",
        "  nb_eval_examples, nb_eval_steps = 0,0\n",
        "\n",
        "  for x, y in data_loader:\n",
        "    x=x.to(device)\n",
        "    y=y.to(device)\n",
        "    with torch.no_grad():\n",
        "      y_preds = net(x, token_type_ids=None, attention_mask=(x>0).to(device), labels=None)  # forward\n",
        "\n",
        "    #Move logits and labels to CPU\n",
        "    y_preds = y_preds[0].detach().cpu().numpy()\n",
        "    real_ys = y.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(y_preds, real_ys)\n",
        "    print(\"tmp_eval_accuracy : \", tmp_eval_accuracy)\n",
        "\n",
        "    eval_acc += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_acc/nb_eval_steps))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNmmhDse9IMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "bert_eval_net(model, test_loader, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bOKNHDPJ82b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def result_summary(pred_y, real_y):\n",
        "  print(\"Accuracy: {:.4f}\".format(accuracy_score(real_y, pred_y)))\n",
        "  print(\"Confusion Matrix: \\n\", confusion_matrix(real_y, pred_y))\n",
        "  print(\"Classification Report Matrix: \\n\", classification_report(real_y, pred_y, digits=3))\n",
        "\n",
        "  roc_auc = roc_auc_score(real_y, pred_y)\n",
        "  print(\"roc_auc score is : {:.4f}\".format(roc_auc))\n",
        "\n",
        "  fpr, tpr, threshold = roc_curve(real_y, pred_y)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  auc_graph(roc_auc, fpr, tpr)\n",
        "\n",
        "def auc_graph(roc_auc, fpr, tpr):\n",
        "  plt.title('Receiver Operating Characteristic')\n",
        "  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "  plt.legend(loc = 'lower right')\n",
        "  plt.plot([0, 1], [0, 1],'r--')\n",
        "  plt.xlim([0, 1])\n",
        "  plt.ylim([0, 1])\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMC800aRKAPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "preds_flat = list(itertools.chain(*preds))\n",
        "labels_flat = list(itertools.chain(*labels))\n",
        "\n",
        "result_summary(preds_flat, labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euhZi04CFa5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}