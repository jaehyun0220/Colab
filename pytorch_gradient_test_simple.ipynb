{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_gradient_test_simple.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaehyun0220/Colab/blob/master/pytorch_gradient_test_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPe4dXkBszxx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "473731bb-49d8-4b5c-8144-ac2e942ba749"
      },
      "source": [
        "!sudo apt-get install nvidia-384"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  nvidia-384\n",
            "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 6,852 B of archives.\n",
            "After this operation, 18.4 kB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-384 418.67-0ubuntu1 [6,852 B]\n",
            "Fetched 6,852 B in 0s (67.0 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package nvidia-384.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../nvidia-384_418.67-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-384 (418.67-0ubuntu1) ...\n",
            "Setting up nvidia-384 (418.67-0ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McYmzxA0tOJg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e4ab7c26-60b2-4aa9-e055-6667da9e7959"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun  8 01:30:41 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7asi7Lx7cR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell # jupyter에서 마지막 값만 출력하는게 아니라 모든 출력값을 매번 연속적으로 출력\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # all, last, last_expr, none (기본값은 'last_expr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naHgL-L_tVm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fe31cc43-8bcb-4d17-bf50-a268f0226c5f"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "NUM_ARR = 5\n",
        "NUM_EPOCH = 20\n",
        "\n",
        "#참의 계수\n",
        "w_true = torch.tensor([1,2,3.])\n",
        "print(type(w_true[0]), w_true[0])\n",
        "\n",
        "#X 데이터 준비, 절편을 회귀 계수에 포함하기 위해 X의 최소차원에 1을 추가\n",
        "X = torch. cat([torch.ones(NUM_ARR,1), torch.randn(NUM_ARR,2)],1)\n",
        "print(type(X[0][0]), X[0][0])"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'> tensor(1.)\n",
            "<class 'torch.Tensor'> tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAyE1agD5VFD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4XbFFRC5VQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4751
        },
        "outputId": "1f902116-1eaf-45db-ed96-fa0be9bbcec6"
      },
      "source": [
        "# 참의 계수와 각 X의 내적을 행렬과 벡터의 곱으로 모아서 계산\n",
        "y = torch.mv(X, w_true) + torch.randn(NUM_ARR) * 0.5\n",
        "print(\"y is \",y)\n",
        "\n",
        "# 기울기 하강으로 최적화 위해 파라미터 Tensor를 난수로 초기화 해서 생성\n",
        "w= torch.randn(3, requires_grad=True)\n",
        "print(\"w is \", w)\n",
        "print(\"*\"*200)\n",
        "\n",
        "# 학습률\n",
        "gamma = 0.1\n",
        "\n",
        "# 손실함수의 로그\n",
        "losses = []\n",
        "\n",
        "# NUM_ARR회 반복\n",
        "for epoch in range(NUM_EPOCH):\n",
        "  print(\"{} epoch info\".format(epoch))\n",
        "  print(\"start w val : \", w)\n",
        "  print(\"[[original]]: \\tw.data - {}, \\tw.grad - {} \".format(w.data, w.grad))\n",
        "  \n",
        "  # 전 회의 backward 메서드로 계산된 grad 경사값을 초기화\n",
        "  w.grad = None\n",
        "  print(\"*** Execute  w.grad = None ***\")\n",
        "  print(\"[grad to None]: w.data - {}, \\tw.grad - {} \".format(w.data, w.grad))\n",
        "  \n",
        "  # 선형 모델로 y 예측값을 계산\n",
        "  y_pred = torch.mv(X,w)\n",
        "  \n",
        "  # MSE losses와 w에 의한 미분을 계산으로 현재 Network의 output과 실제 y 값간의 오차, 즉, Error를 계산 (loss (error function) = (실제값 - 모델 예측값)**2/ n )\n",
        "  loss = torch.mean((y-y_pred)**2)\n",
        "  \n",
        "  # Loss(Error) 값을 기준으로 Back propagation 시행하여 각 weight의 비중 결정\n",
        "  loss.backward()\n",
        "  # 기존 w값(weight 값)에서 현재 오류에 weight가 얼마만큼의 영향을 끼치는지 계산\n",
        "  # = 오차에 대한 각 weight의 비중으로 weight가 0이 되면 즉, gradient의 극점에 가면 error와 연관이 없다는 뜻으로 최적화 된것임, 즉, gredient를 최소화 시키는 것이 목표\n",
        "  # = loss fuction에서(산출 된 Error값에 대해) 각 weight에 대해 미분 = loss.backward()\n",
        "\n",
        "  # 경사 갱신\n",
        "  \n",
        "  # learning rate을 곱한 것 만큼만 weight값을 깎아 오차를 줄여준다.\n",
        "  \n",
        "  # w를 그대로 대입해서 갱신하면 다른 텐서가 되어 계산 그래프가 망가진다\n",
        "  # 따라서 data만 갱신한다.\n",
        "  \n",
        "  print(\"optimizer is \\t{} = {} - {} * {}\".format(w.data, w.data, gamma, w.grad.data))\n",
        "  \n",
        "  w.data = w.data - gamma * w.grad.data\n",
        "  \n",
        "  # 수렴 확인을 위한 loss를 기록\n",
        "  losses.append(loss.item())\n",
        "  \n",
        "  print(\"[[after BP]]: \\tw.data - {}, \\tw.grad - {}\".format(w.data, w.grad))\n",
        "  print(\"\\ny_pred - {}, \\ty_pred.grad_fn  - {}, \\ny_real - {}\".format(y_pred.data, y_pred.grad_fn, y))\n",
        "  print(\"\\nloss   - {}, \\tloss.grad_fn   - {}\".format(loss, loss.grad_fn))\n",
        "  print(\"#\" * 200)\n",
        "  \n",
        "  # 다시 epoch 상단으로 감(수정 된 network에 대한 오류 계산하기 위해)\n",
        "  # -> ** 이전 epoch에서의 gradient가 누적 될 수 있으니, 다시 현재 network에서의 오류에 대한 weight 비중을 계산하기 위해 w.grade = None으로 초기화\n",
        "  # -> 현재 네트워크에서의 오류 계산 후 오류에 대한 weight의 비중 즉, gradient 계산(Back Propagation) \n",
        "  # -> 현재 weight - learning rate * gradient 적용하여 network update \n",
        "  # -> 다시 epoch 실행 반복"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y is  tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "w is  tensor([-1.6300,  1.0355,  0.8944], requires_grad=True)\n",
            "********************************************************************************************************************************************************************************************************\n",
            "0 epoch info\n",
            "start w val :  tensor([-1.6300,  1.0355,  0.8944], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([-1.6300,  1.0355,  0.8944]), \tw.grad - None \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([-1.6300,  1.0355,  0.8944]), \tw.grad - None \n",
            "optimizer is \ttensor([-1.6300,  1.0355,  0.8944]) = tensor([-1.6300,  1.0355,  0.8944]) - 0.1 * tensor([-6.4892, -4.9724, -3.5884])\n",
            "[[after BP]]: \tw.data - tensor([-0.9811,  1.5327,  1.2533]), \tw.grad - tensor([-6.4892, -4.9724, -3.5884])\n",
            "\n",
            "y_pred - tensor([-3.0391, -0.2135, -2.7369,  0.4504, -0.7449]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6bd2ecf98>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 15.735517501831055, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "1 epoch info\n",
            "start w val :  tensor([-0.9811,  1.5327,  1.2533], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([-0.9811,  1.5327,  1.2533]), \tw.grad - tensor([-6.4892, -4.9724, -3.5884]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([-0.9811,  1.5327,  1.2533]), \tw.grad - None \n",
            "optimizer is \ttensor([-0.9811,  1.5327,  1.2533]) = tensor([-0.9811,  1.5327,  1.2533]) - 0.1 * tensor([-4.8346, -3.3606, -2.8332])\n",
            "[[after BP]]: \tw.data - tensor([-0.4976,  1.8688,  1.5366]), \tw.grad - tensor([-4.8346, -3.3606, -2.8332])\n",
            "\n",
            "y_pred - tensor([-3.0714,  1.0799, -2.5068,  2.0374,  0.3134]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8ce48>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 8.837430000305176, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "2 epoch info\n",
            "start w val :  tensor([-0.4976,  1.8688,  1.5366], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([-0.4976,  1.8688,  1.5366]), \tw.grad - tensor([-4.8346, -3.3606, -2.8332]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([-0.4976,  1.8688,  1.5366]), \tw.grad - None \n",
            "optimizer is \ttensor([-0.4976,  1.8688,  1.5366]) = tensor([-0.4976,  1.8688,  1.5366]) - 0.1 * tensor([-3.6256, -2.2215, -2.2631])\n",
            "[[after BP]]: \tw.data - tensor([-0.1351,  2.0910,  1.7629]), \tw.grad - tensor([-3.6256, -2.2215, -2.2631])\n",
            "\n",
            "y_pred - tensor([-3.0457,  2.0195, -2.3715,  3.1900,  1.0826]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 5.1324005126953125, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6c5e8ce48>\n",
            "########################################################################################################################################################################################################\n",
            "3 epoch info\n",
            "start w val :  tensor([-0.1351,  2.0910,  1.7629], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([-0.1351,  2.0910,  1.7629]), \tw.grad - tensor([-3.6256, -2.2215, -2.2631]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([-0.1351,  2.0910,  1.7629]), \tw.grad - None \n",
            "optimizer is \ttensor([-0.1351,  2.0910,  1.7629]) = tensor([-0.1351,  2.0910,  1.7629]) - 0.1 * tensor([-2.7396, -1.4209, -1.8290])\n",
            "[[after BP]]: \tw.data - tensor([0.1389, 2.2330, 1.9458]), \tw.grad - tensor([-2.7396, -1.4209, -1.8290])\n",
            "\n",
            "y_pred - tensor([-2.9831,  2.7034, -2.3013,  4.0286,  1.6426]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 3.1108996868133545, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "4 epoch info\n",
            "start w val :  tensor([0.1389, 2.2330, 1.9458], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([0.1389, 2.2330, 1.9458]), \tw.grad - tensor([-2.7396, -1.4209, -1.8290]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([0.1389, 2.2330, 1.9458]), \tw.grad - None \n",
            "optimizer is \ttensor([0.1389, 2.2330, 1.9458]) = tensor([0.1389, 2.2330, 1.9458]) - 0.1 * tensor([-2.0878, -0.8623, -1.4954])\n",
            "[[after BP]]: \tw.data - tensor([0.3477, 2.3193, 2.0953]), \tw.grad - tensor([-2.0878, -0.8623, -1.4954])\n",
            "\n",
            "y_pred - tensor([-2.8986,  3.2021, -2.2753,  4.6399,  2.0513]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 1.9834423065185547, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "5 epoch info\n",
            "start w val :  tensor([0.3477, 2.3193, 2.0953], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([0.3477, 2.3193, 2.0953]), \tw.grad - tensor([-2.0878, -0.8623, -1.4954]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([0.3477, 2.3193, 2.0953]), \tw.grad - None \n",
            "optimizer is \ttensor([0.3477, 2.3193, 2.0953]) = tensor([0.3477, 2.3193, 2.0953]) - 0.1 * tensor([-1.6063, -0.4762, -1.2364])\n",
            "[[after BP]]: \tw.data - tensor([0.5083, 2.3669, 2.2190]), \tw.grad - tensor([-1.6063, -0.4762, -1.2364])\n",
            "\n",
            "y_pred - tensor([-2.8022,  3.5668, -2.2784,  5.0867,  2.3504]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 1.335854172706604, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "6 epoch info\n",
            "start w val :  tensor([0.5083, 2.3669, 2.2190], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([0.5083, 2.3669, 2.2190]), \tw.grad - tensor([-1.6063, -0.4762, -1.2364]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([0.5083, 2.3669, 2.2190]), \tw.grad - None \n",
            "optimizer is \ttensor([0.5083, 2.3669, 2.2190]) = tensor([0.5083, 2.3669, 2.2190]) - 0.1 * tensor([-1.2488, -0.2128, -1.0332])\n",
            "[[after BP]]: \tw.data - tensor([0.6332, 2.3882, 2.3223]), \tw.grad - tensor([-1.2488, -0.2128, -1.0332])\n",
            "\n",
            "y_pred - tensor([-2.7010,  3.8343, -2.3004,  5.4142,  2.5700]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8ce48>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.9498285055160522, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "7 epoch info\n",
            "start w val :  tensor([0.6332, 2.3882, 2.3223], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([0.6332, 2.3882, 2.3223]), \tw.grad - tensor([-1.2488, -0.2128, -1.0332]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([0.6332, 2.3882, 2.3223]), \tw.grad - None \n",
            "optimizer is \ttensor([0.6332, 2.3882, 2.3223]) = tensor([0.6332, 2.3882, 2.3223]) - 0.1 * tensor([-0.9817, -0.0363, -0.8719])\n",
            "[[after BP]]: \tw.data - tensor([0.7314, 2.3918, 2.4095]), \tw.grad - tensor([-0.9817, -0.0363, -0.8719])\n",
            "\n",
            "y_pred - tensor([-2.5995,  4.0312, -2.3338,  5.6551,  2.7319]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.7094882130622864, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "8 epoch info\n",
            "start w val :  tensor([0.7314, 2.3918, 2.4095], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([0.7314, 2.3918, 2.4095]), \tw.grad - tensor([-0.9817, -0.0363, -0.8719]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([0.7314, 2.3918, 2.4095]), \tw.grad - None \n",
            "optimizer is \ttensor([0.7314, 2.3918, 2.4095]) = tensor([0.7314, 2.3918, 2.4095]) - 0.1 * tensor([-0.7807,  0.0788, -0.7424])\n",
            "[[after BP]]: \tw.data - tensor([0.8094, 2.3839, 2.4837]), \tw.grad - tensor([-0.7807,  0.0788, -0.7424])\n",
            "\n",
            "y_pred - tensor([-2.5008,  4.1769, -2.3737,  5.8331,  2.8518]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.5526780486106873, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "9 epoch info\n",
            "start w val :  tensor([0.8094, 2.3839, 2.4837], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([0.8094, 2.3839, 2.4837]), \tw.grad - tensor([-0.7807,  0.0788, -0.7424]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([0.8094, 2.3839, 2.4837]), \tw.grad - None \n",
            "optimizer is \ttensor([0.8094, 2.3839, 2.4837]) = tensor([0.8094, 2.3839, 2.4837]) - 0.1 * tensor([-0.6284,  0.1510, -0.6372])\n",
            "[[after BP]]: \tw.data - tensor([0.8723, 2.3688, 2.5474]), \tw.grad - tensor([-0.6284,  0.1510, -0.6372])\n",
            "\n",
            "y_pred - tensor([-2.4067,  4.2851, -2.4165,  5.9653,  2.9410]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.44555243849754333, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "10 epoch info\n",
            "start w val :  tensor([0.8723, 2.3688, 2.5474], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([0.8723, 2.3688, 2.5474]), \tw.grad - tensor([-0.6284,  0.1510, -0.6372]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([0.8723, 2.3688, 2.5474]), \tw.grad - None \n",
            "optimizer is \ttensor([0.8723, 2.3688, 2.5474]) = tensor([0.8723, 2.3688, 2.5474]) - 0.1 * tensor([-0.5117,  0.1933, -0.5507])\n",
            "[[after BP]]: \tw.data - tensor([0.9234, 2.3495, 2.6025]), \tw.grad - tensor([-0.5117,  0.1933, -0.5507])\n",
            "\n",
            "y_pred - tensor([-2.3183,  4.3661, -2.4600,  6.0640,  3.0079]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8ce48>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.36928510665893555, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6c5e8c240>\n",
            "########################################################################################################################################################################################################\n",
            "11 epoch info\n",
            "start w val :  tensor([0.9234, 2.3495, 2.6025], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([0.9234, 2.3495, 2.6025]), \tw.grad - tensor([-0.5117,  0.1933, -0.5507]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([0.9234, 2.3495, 2.6025]), \tw.grad - None \n",
            "optimizer is \ttensor([0.9234, 2.3495, 2.6025]) = tensor([0.9234, 2.3495, 2.6025]) - 0.1 * tensor([-0.4216,  0.2151, -0.4789])\n",
            "[[after BP]]: \tw.data - tensor([0.9656, 2.3280, 2.6504]), \tw.grad - tensor([-0.4216,  0.2151, -0.4789])\n",
            "\n",
            "y_pred - tensor([-2.2361,  4.4271, -2.5027,  6.1382,  3.0585]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.31310027837753296, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "12 epoch info\n",
            "start w val :  tensor([0.9656, 2.3280, 2.6504], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([0.9656, 2.3280, 2.6504]), \tw.grad - tensor([-0.4216,  0.2151, -0.4789]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([0.9656, 2.3280, 2.6504]), \tw.grad - None \n",
            "optimizer is \ttensor([0.9656, 2.3280, 2.6504]) = tensor([0.9656, 2.3280, 2.6504]) - 0.1 * tensor([-0.3512,  0.2230, -0.4186])\n",
            "[[after BP]]: \tw.data - tensor([1.0007, 2.3057, 2.6923]), \tw.grad - tensor([-0.3512,  0.2230, -0.4186])\n",
            "\n",
            "y_pred - tensor([-2.1603,  4.4735, -2.5437,  6.1945,  3.0969]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.2706025540828705, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "13 epoch info\n",
            "start w val :  tensor([1.0007, 2.3057, 2.6923], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([1.0007, 2.3057, 2.6923]), \tw.grad - tensor([-0.3512,  0.2230, -0.4186]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([1.0007, 2.3057, 2.6923]), \tw.grad - None \n",
            "optimizer is \ttensor([1.0007, 2.3057, 2.6923]) = tensor([1.0007, 2.3057, 2.6923]) - 0.1 * tensor([-0.2956,  0.2217, -0.3675])\n",
            "[[after BP]]: \tw.data - tensor([1.0303, 2.2835, 2.7290]), \tw.grad - tensor([-0.2956,  0.2217, -0.3675])\n",
            "\n",
            "y_pred - tensor([-2.0909,  4.5090, -2.5822,  6.2376,  3.1265]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8ce48>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.2378295361995697, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "14 epoch info\n",
            "start w val :  tensor([1.0303, 2.2835, 2.7290], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([1.0303, 2.2835, 2.7290]), \tw.grad - tensor([-0.2956,  0.2217, -0.3675]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([1.0303, 2.2835, 2.7290]), \tw.grad - None \n",
            "optimizer is \ttensor([1.0303, 2.2835, 2.7290]) = tensor([1.0303, 2.2835, 2.7290]) - 0.1 * tensor([-0.2512,  0.2145, -0.3238])\n",
            "[[after BP]]: \tw.data - tensor([1.0554, 2.2620, 2.7614]), \tw.grad - tensor([-0.2512,  0.2145, -0.3238])\n",
            "\n",
            "y_pred - tensor([-2.0275,  4.5365, -2.6182,  6.2708,  3.1495]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.2122085988521576, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "15 epoch info\n",
            "start w val :  tensor([1.0554, 2.2620, 2.7614], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([1.0554, 2.2620, 2.7614]), \tw.grad - tensor([-0.2512,  0.2145, -0.3238]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([1.0554, 2.2620, 2.7614]), \tw.grad - None \n",
            "optimizer is \ttensor([1.0554, 2.2620, 2.7614]) = tensor([1.0554, 2.2620, 2.7614]) - 0.1 * tensor([-0.2152,  0.2036, -0.2862])\n",
            "[[after BP]]: \tw.data - tensor([1.0769, 2.2417, 2.7900]), \tw.grad - tensor([-0.2152,  0.2036, -0.2862])\n",
            "\n",
            "y_pred - tensor([-1.9698,  4.5580, -2.6514,  6.2968,  3.1676]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.19199100136756897, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "16 epoch info\n",
            "start w val :  tensor([1.0769, 2.2417, 2.7900], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([1.0769, 2.2417, 2.7900]), \tw.grad - tensor([-0.2152,  0.2036, -0.2862]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([1.0769, 2.2417, 2.7900]), \tw.grad - None \n",
            "optimizer is \ttensor([1.0769, 2.2417, 2.7900]) = tensor([1.0769, 2.2417, 2.7900]) - 0.1 * tensor([-0.1857,  0.1907, -0.2536])\n",
            "[[after BP]]: \tw.data - tensor([1.0955, 2.2226, 2.8154]), \tw.grad - tensor([-0.1857,  0.1907, -0.2536])\n",
            "\n",
            "y_pred - tensor([-1.9176,  4.5750, -2.6819,  6.3172,  3.1819]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8ce48>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.17593666911125183, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "17 epoch info\n",
            "start w val :  tensor([1.0955, 2.2226, 2.8154], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([1.0955, 2.2226, 2.8154]), \tw.grad - tensor([-0.1857,  0.1907, -0.2536]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([1.0955, 2.2226, 2.8154]), \tw.grad - None \n",
            "optimizer is \ttensor([1.0955, 2.2226, 2.8154]) = tensor([1.0955, 2.2226, 2.8154]) - 0.1 * tensor([-0.1613,  0.1769, -0.2252])\n",
            "[[after BP]]: \tw.data - tensor([1.1116, 2.2049, 2.8379]), \tw.grad - tensor([-0.1613,  0.1769, -0.2252])\n",
            "\n",
            "y_pred - tensor([-1.8703,  4.5887, -2.7097,  6.3336,  3.1934]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8ce48>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.16313475370407104, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "18 epoch info\n",
            "start w val :  tensor([1.1116, 2.2049, 2.8379], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([1.1116, 2.2049, 2.8379]), \tw.grad - tensor([-0.1613,  0.1769, -0.2252]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([1.1116, 2.2049, 2.8379]), \tw.grad - None \n",
            "optimizer is \ttensor([1.1116, 2.2049, 2.8379]) = tensor([1.1116, 2.2049, 2.8379]) - 0.1 * tensor([-0.1409,  0.1629, -0.2003])\n",
            "[[after BP]]: \tw.data - tensor([1.1257, 2.1886, 2.8579]), \tw.grad - tensor([-0.1409,  0.1629, -0.2003])\n",
            "\n",
            "y_pred - tensor([-1.8276,  4.5997, -2.7349,  6.3468,  3.2028]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8c240>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.15289869904518127, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n",
            "19 epoch info\n",
            "start w val :  tensor([1.1257, 2.1886, 2.8579], requires_grad=True)\n",
            "[[original]]: \tw.data - tensor([1.1257, 2.1886, 2.8579]), \tw.grad - tensor([-0.1409,  0.1629, -0.2003]) \n",
            "*** Execute  w.grad = None ***\n",
            "[grad to None]: w.data - tensor([1.1257, 2.1886, 2.8579]), \tw.grad - None \n",
            "optimizer is \ttensor([1.1257, 2.1886, 2.8579]) = tensor([1.1257, 2.1886, 2.8579]) - 0.1 * tensor([-0.1237,  0.1491, -0.1785])\n",
            "[[after BP]]: \tw.data - tensor([1.1381, 2.1737, 2.8758]), \tw.grad - tensor([-0.1237,  0.1491, -0.1785])\n",
            "\n",
            "y_pred - tensor([-1.7891,  4.6088, -2.7579,  6.3576,  3.2105]), \ty_pred.grad_fn  - <MvBackward object at 0x7fb6c5e8ce48>, \n",
            "y_real - tensor([-1.4648,  4.0837, -2.9829,  6.6205,  3.6826])\n",
            "\n",
            "loss   - 0.14469937980175018, \tloss.grad_fn   - <MeanBackward0 object at 0x7fb6bd2ecf98>\n",
            "########################################################################################################################################################################################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zimX99ktrqt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "1dfbadbc-774f-42af-bf9e-59b0d7ac7b01"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb6b91602e8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAdw81Rfe+FM7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lieWlykC56E_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}