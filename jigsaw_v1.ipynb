{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jigsaw_v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaehyun0220/Colab/blob/master/jigsaw_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwSigEpsgSCR",
        "colab_type": "code",
        "outputId": "185e88f0-5ff8-462c-a5fb-c6f456bad981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# 데이터 활용에 필요한 기본 패키지 로딩\n",
        "import sys #access to system parameters \n",
        "print(\"Python version: {}\". format(sys.version))\n",
        "\n",
        "import pandas as pd\n",
        "print(\"pandas version: {}\". format(pd.__version__))\n",
        "\n",
        "import sklearn #collection of machine learning algorithms\n",
        "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
        "\n",
        "import numpy as np #foundational package for scientific computing\n",
        "print(\"NumPy version: {}\". format(np.__version__))\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"tensorflow version: {}\".format(tf.__version__))\n",
        "\n",
        "import keras\n",
        "print(\"keras version: {}\".format(keras.__version__))\n",
        "\n",
        "import os\n",
        "#import io"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version: 3.6.7 (default, Oct 22 2018, 11:32:17) \n",
            "[GCC 8.2.0]\n",
            "pandas version: 0.24.2\n",
            "scikit-learn version: 0.21.1\n",
            "NumPy version: 1.16.3\n",
            "tensorflow version: 1.13.1\n",
            "keras version: 2.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55XLlqJ1gSCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#데이터 전처리 관련 라이브러리 로드\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Deep Learning Model 로드\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation \n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "#Visualization\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#HyperParameter Tuning을 위한 라이브러리 로드\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#모델 평가를 위한 라이브러리 로드\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn import model_selection\n",
        "\n",
        "#수학 & 통계 관련 라이브러리 로드\n",
        "import scipy.stats as st\n",
        "from collections import Counter\n",
        "\n",
        "#Configure Visualization Defaults\n",
        "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
        "%matplotlib inline\n",
        "mpl.style.use('ggplot')\n",
        "sns.set_style('white')\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCOcsXzdRMsr",
        "colab_type": "code",
        "outputId": "8cbded4f-8938-43c3-a857-3715ec4a9a6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "# Auth 인증 및 Google Drive 활용 Data load\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "\n",
        "# Google Drive 내 Custom Class 경로 지정\n",
        "import sys\n",
        "sys.path.insert(0, '/gdrive/My Drive/CustomClasses')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oggu2U3w06uD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# files.upload()\n",
        "# !cp ./kaggle.json /root/.kaggle/kaggle.json\n",
        "# !cp ./kaggle.json /gdrive/My\\ Drive/kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5kPIpd72Ovi",
        "colab_type": "code",
        "outputId": "80a73903-09d4-4cdc-bf56-1c85969d412b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# kaggle 실행을 위한 json file copy & 권한 부여\n",
        "!mkdir /root/.kaggle\n",
        "!cp /gdrive/My\\ Drive/kaggle/kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!cat /root/.kaggle/kaggle.json\n",
        "# !cat /gdrive/My\\ Drive/CustomClasses/.kaggle/kaggle.json\n",
        "# !kaggle competitions list"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"username\":\"claypark\",\"key\":\"bb187008d01e8abfeaabdb973ebfa7a9\"}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ZV0skpmas-",
        "colab_type": "code",
        "outputId": "56b6a174-8881-430a-bce0-14463fbe4eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# !ls /gdrive/My\\ Drive/kaggle\n",
        "!ls /gdrive/My\\ Drive/kaggle/jigsaw"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_submission.csv.zip  test.csv.zip  train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNrkrsgURSBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Kaggle Competition Data Download - JIGSAW competition\n",
        "\n",
        "# Directory 생성 후 데이터 다운로드\n",
        "# !mkdir /gdrive/My\\ Drive/kaggle/jigsaw  \n",
        "# !kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification -p /gdrive/My\\ Drive/kaggle/jigsaw \n",
        "\n",
        "# 파일 확인 후 각각 unzip 후 zip 파일 삭제\n",
        "# !ls /gdrive/My\\ Drive/kaggle/jigsaw\n",
        "# !unzip /gdrive/My\\ Drive/kaggle/jigsaw/train.csv.zip -d /gdrive/My\\ Drive/kaggle/jigsaw\n",
        "# !unzip /gdrive/My\\ Drive/kaggle/jigsaw/test.csv.zip -d /gdrive/My\\ Drive/kaggle/jigsaw\n",
        "# !unzip /gdrive/My\\ Drive/kaggle/jigsaw/sample_submission.csv.zip -d /gdrive/My\\ Drive/kaggle/jigsaw\n",
        "# !rm /gdrive/My\\ Drive/kaggle/jigsaw/*.zip\n",
        "\n",
        "# !rm /gdrive/My\\ Drive/kaggle/jigsaw/*.csv\n",
        "# !ls /gdrive/My\\ Drive/kaggle/jigsaw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJt-yU5uAvhC",
        "colab_type": "code",
        "outputId": "45048124-038e-47a8-ebe3-cfb7204d1901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!ls /gdrive/My\\ Drive/data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crawl-300d-2M.vec    sample_submission.csv  train.csv\n",
            "glove.840B.300d.txt  test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmaToi7GBWnh",
        "colab_type": "text"
      },
      "source": [
        "# **EDA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKjkZus-8aWt",
        "colab_type": "text"
      },
      "source": [
        "필요한 Package Load 및 학습 데이터 Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQfcH0ZVPyD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud ,STOPWORDS\n",
        "from PIL import Image\n",
        "train = pd.read_csv('/gdrive/My Drive/kaggle/jigsaw/train.csv.zip', encoding='utf-8')\n",
        "test = pd.read_csv('/gdrive/My Drive/kaggle/jigsaw/test.csv.zip', encoding='utf-8')\n",
        "sub = pd.read_csv('/gdrive/My Drive/kaggle/jigsaw/sample_submission.csv.zip', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6qm862tP4h6",
        "colab_type": "code",
        "outputId": "262eeee0-6b74-4b99-8a46-36d0e9cc12d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# 데이터 정상 Load 확인\n",
        "train.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>severe_toxicity</th>\n",
              "      <th>obscene</th>\n",
              "      <th>identity_attack</th>\n",
              "      <th>insult</th>\n",
              "      <th>threat</th>\n",
              "      <th>asian</th>\n",
              "      <th>atheist</th>\n",
              "      <th>bisexual</th>\n",
              "      <th>black</th>\n",
              "      <th>buddhist</th>\n",
              "      <th>christian</th>\n",
              "      <th>female</th>\n",
              "      <th>heterosexual</th>\n",
              "      <th>hindu</th>\n",
              "      <th>homosexual_gay_or_lesbian</th>\n",
              "      <th>intellectual_or_learning_disability</th>\n",
              "      <th>jewish</th>\n",
              "      <th>latino</th>\n",
              "      <th>male</th>\n",
              "      <th>muslim</th>\n",
              "      <th>other_disability</th>\n",
              "      <th>other_gender</th>\n",
              "      <th>other_race_or_ethnicity</th>\n",
              "      <th>other_religion</th>\n",
              "      <th>other_sexual_orientation</th>\n",
              "      <th>physical_disability</th>\n",
              "      <th>psychiatric_or_mental_illness</th>\n",
              "      <th>transgender</th>\n",
              "      <th>white</th>\n",
              "      <th>created_date</th>\n",
              "      <th>publication_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>article_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>funny</th>\n",
              "      <th>wow</th>\n",
              "      <th>sad</th>\n",
              "      <th>likes</th>\n",
              "      <th>disagree</th>\n",
              "      <th>sexual_explicit</th>\n",
              "      <th>identity_annotator_count</th>\n",
              "      <th>toxicity_annotator_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59848</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:41.987077+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>59849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Thank you!! This would make my life a lot less...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:42.870083+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>This is such an urgent design problem; kudos t...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:45.222647+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59855</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Is this something I'll be able to install on m...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-29 10:50:47.601894+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59856</td>\n",
              "      <td>0.893617</td>\n",
              "      <td>haha you guys are a bunch of losers.</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.87234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2015-09-29 10:50:48.488476+00</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006</td>\n",
              "      <td>rejected</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id    target  ... identity_annotator_count  toxicity_annotator_count\n",
              "0  59848  0.000000  ...                        0                         4\n",
              "1  59849  0.000000  ...                        0                         4\n",
              "2  59852  0.000000  ...                        0                         4\n",
              "3  59855  0.000000  ...                        0                         4\n",
              "4  59856  0.893617  ...                        4                        47\n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b65mfch8tA9",
        "colab_type": "code",
        "outputId": "f072e942-5e3f-42f7-9aa2-c46ba6c75c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        }
      },
      "source": [
        "print(train.isnull().any())\n",
        "print(test.isnull().any())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id                                     False\n",
            "target                                 False\n",
            "comment_text                           False\n",
            "severe_toxicity                        False\n",
            "obscene                                False\n",
            "identity_attack                        False\n",
            "insult                                 False\n",
            "threat                                 False\n",
            "asian                                   True\n",
            "atheist                                 True\n",
            "bisexual                                True\n",
            "black                                   True\n",
            "buddhist                                True\n",
            "christian                               True\n",
            "female                                  True\n",
            "heterosexual                            True\n",
            "hindu                                   True\n",
            "homosexual_gay_or_lesbian               True\n",
            "intellectual_or_learning_disability     True\n",
            "jewish                                  True\n",
            "latino                                  True\n",
            "male                                    True\n",
            "muslim                                  True\n",
            "other_disability                        True\n",
            "other_gender                            True\n",
            "other_race_or_ethnicity                 True\n",
            "other_religion                          True\n",
            "other_sexual_orientation                True\n",
            "physical_disability                     True\n",
            "psychiatric_or_mental_illness           True\n",
            "transgender                             True\n",
            "white                                   True\n",
            "created_date                           False\n",
            "publication_id                         False\n",
            "parent_id                               True\n",
            "article_id                             False\n",
            "rating                                 False\n",
            "funny                                  False\n",
            "wow                                    False\n",
            "sad                                    False\n",
            "likes                                  False\n",
            "disagree                               False\n",
            "sexual_explicit                        False\n",
            "identity_annotator_count               False\n",
            "toxicity_annotator_count               False\n",
            "dtype: bool\n",
            "id              False\n",
            "comment_text    False\n",
            "dtype: bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04-BdHCjQI6B",
        "colab_type": "code",
        "outputId": "c1b4e213-6270-423e-c176-1bef6402625d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1025
        }
      },
      "source": [
        "print(\"Train null value counts are \\n {}\".format(train.isnull().sum()))\n",
        "print(\"Train total length is \", len(train))\n",
        "print(\"-\"*50)\n",
        "print(\"Test null value counts are \\n {}\".format(test.isnull().sum()))\n",
        "print(\"Test total length is \", len(test))\n",
        "print(\"-\"*50)\n",
        "print(\"Submission total length is \", len(sub))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train null value counts are \n",
            " id                                           0\n",
            "target                                       0\n",
            "comment_text                                 0\n",
            "severe_toxicity                              0\n",
            "obscene                                      0\n",
            "identity_attack                              0\n",
            "insult                                       0\n",
            "threat                                       0\n",
            "asian                                  1399744\n",
            "atheist                                1399744\n",
            "bisexual                               1399744\n",
            "black                                  1399744\n",
            "buddhist                               1399744\n",
            "christian                              1399744\n",
            "female                                 1399744\n",
            "heterosexual                           1399744\n",
            "hindu                                  1399744\n",
            "homosexual_gay_or_lesbian              1399744\n",
            "intellectual_or_learning_disability    1399744\n",
            "jewish                                 1399744\n",
            "latino                                 1399744\n",
            "male                                   1399744\n",
            "muslim                                 1399744\n",
            "other_disability                       1399744\n",
            "other_gender                           1399744\n",
            "other_race_or_ethnicity                1399744\n",
            "other_religion                         1399744\n",
            "other_sexual_orientation               1399744\n",
            "physical_disability                    1399744\n",
            "psychiatric_or_mental_illness          1399744\n",
            "transgender                            1399744\n",
            "white                                  1399744\n",
            "created_date                                 0\n",
            "publication_id                               0\n",
            "parent_id                               778646\n",
            "article_id                                   0\n",
            "rating                                       0\n",
            "funny                                        0\n",
            "wow                                          0\n",
            "sad                                          0\n",
            "likes                                        0\n",
            "disagree                                     0\n",
            "sexual_explicit                              0\n",
            "identity_annotator_count                     0\n",
            "toxicity_annotator_count                     0\n",
            "dtype: int64\n",
            "Train total length is  1804874\n",
            "--------------------------------------------------\n",
            "Test null value counts are \n",
            " id              0\n",
            "comment_text    0\n",
            "dtype: int64\n",
            "Test total length is  97320\n",
            "--------------------------------------------------\n",
            "Submission total length is  97320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCP8-A9FHaC_",
        "colab_type": "text"
      },
      "source": [
        "## 테스트를 위한 Mini Data로 변환 (Train, Test 각 1,000개 씩)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_58vYu0lenX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mini Size Test용 Dataset 생성\n",
        "train_mini = train[:1000]\n",
        "test_mini = test[:1000]\n",
        "sub_mini = sub[:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EKlLgrFpC1C",
        "colab_type": "code",
        "outputId": "61a9c8ea-d590-4668-8ebd-831d0dc0d623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "# Null값이 있는 컬럼 체크 후 Null 개수 출력\n",
        "for col in train_mini:\n",
        "   if train_mini[col].isnull().sum() > 0:\n",
        "      print(\"column: {:20} \\t # of null value {:3} \\t dtype {}\".format(col, train_mini[col].isnull().sum(), train_mini[col].dtype))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "column: asian                \t # of null value 834 \t dtype float64\n",
            "column: atheist              \t # of null value 834 \t dtype float64\n",
            "column: bisexual             \t # of null value 834 \t dtype float64\n",
            "column: black                \t # of null value 834 \t dtype float64\n",
            "column: buddhist             \t # of null value 834 \t dtype float64\n",
            "column: christian            \t # of null value 834 \t dtype float64\n",
            "column: female               \t # of null value 834 \t dtype float64\n",
            "column: heterosexual         \t # of null value 834 \t dtype float64\n",
            "column: hindu                \t # of null value 834 \t dtype float64\n",
            "column: homosexual_gay_or_lesbian \t # of null value 834 \t dtype float64\n",
            "column: intellectual_or_learning_disability \t # of null value 834 \t dtype float64\n",
            "column: jewish               \t # of null value 834 \t dtype float64\n",
            "column: latino               \t # of null value 834 \t dtype float64\n",
            "column: male                 \t # of null value 834 \t dtype float64\n",
            "column: muslim               \t # of null value 834 \t dtype float64\n",
            "column: other_disability     \t # of null value 834 \t dtype float64\n",
            "column: other_gender         \t # of null value 834 \t dtype float64\n",
            "column: other_race_or_ethnicity \t # of null value 834 \t dtype float64\n",
            "column: other_religion       \t # of null value 834 \t dtype float64\n",
            "column: other_sexual_orientation \t # of null value 834 \t dtype float64\n",
            "column: physical_disability  \t # of null value 834 \t dtype float64\n",
            "column: psychiatric_or_mental_illness \t # of null value 834 \t dtype float64\n",
            "column: transgender          \t # of null value 834 \t dtype float64\n",
            "column: white                \t # of null value 834 \t dtype float64\n",
            "column: parent_id            \t # of null value 638 \t dtype float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaVNxgj1AcZ9",
        "colab_type": "text"
      },
      "source": [
        "# **SIMPLE LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jQZKULdAUEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
        "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiylLqLFowGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#crawl-300d-2M.vec--> https://fasttext.cc/docs/en/english-vectors.html\n",
        "#When pre-train embedding is helpful? https://www.aclweb.org/anthology/N18-2084\n",
        "#There are many pretrained word embedding models: \n",
        "#fasttext, GloVe, Word2Vec, etc\n",
        "#crawl-300d-2M.vec is trained from Common Crawl (a website that collects almost everything)\n",
        "#it has 2 million words. Each word is represent by a vector of 300 dimensions.\n",
        "\n",
        "#https://nlp.stanford.edu/projects/glove/\n",
        "#GloVe is similar to crawl-300d-2M.vec. Probably, they use different algorithms.\n",
        "#glove.840B.300d.zip: Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
        "#tokens mean words. It has 2.2M different words and 840B (likely duplicated) words in total\n",
        "\n",
        "#note that these two pre-trained models give 300d vectors.\n",
        "\n",
        "# 300차원의 pre-trained word vector 사용\n",
        "EMBEDDING_FILES = [\n",
        "    '/gdrive/My Drive/data/crawl-300d-2M.vec',\n",
        "    '/gdrive/My Drive/data/glove.840B.300d.txt'\n",
        "]\n",
        "\n",
        "#we will convert each word in a comment_text to a number.\n",
        "#So a comment_text is a list of number. How many numbers in this list?\n",
        "#we want the length of this list is a constant -> MAX_LEN\n",
        "# 상기 Histogram을 바탕으로 200개의 단어길이면 대부분의 단어 길이 cover 가능\n",
        "# 신경망 학습을 위한 input 벡터 길이로 사용 - 적정 길이는 tokenizng 이후 분포를 보고 결정(코드 하단)\n",
        "## - totalNumWords = [len(one_comment) for one_comment in x_train] # 각 document의 단어 길이를 check\n",
        "## - plt.hist(totalNumWords,bins = np.arange(0,max(totalNumWords),max(totalNumWords)/20))\n",
        "MAX_LEN = 200\n",
        "\n",
        "# pre-trained Embedding을 2개 사용하여 이를 바탕으로 2개의 model을 생성\n",
        "NUM_MODELS = 2\n",
        "\n",
        "# the maximum number of different words to keep in the original texts\n",
        "# 40_000 is a normal number & 100_000 seems good too\n",
        "# MAX_FEATURES = 100000\n",
        "\n",
        "# input data 원문에서 보존할 최대 단어 개수 \n",
        "# len(tokenizer.word_index)을 활용한 결과 12,164개의 단어 출현이지만 넉넉하게 20,000으로 세팅\n",
        "MAX_FEATURES = 20000\n",
        "\n",
        "#this is the number of training sample to put in the model each step\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "#units parameters in Keras.layers.LSTM/cuDNNLSTM\n",
        "#it it the dimension of the output vector of each LSTM cell.\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "\n",
        "EPOCHS = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCBQ7-UZHqvV",
        "colab_type": "text"
      },
      "source": [
        "### N/A 값을 공백으로 치환하고 y값 및 보조 y 값을 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9YLB5fSrEw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take the columns 'comment_text' from train,\n",
        "# then fillall NaN values by emtpy string '' (redundant)\n",
        "x_train = train_mini['comment_text'].fillna('').values\n",
        "\n",
        "#if true, y_train[i] =1, if false, it is = 0\n",
        "y_train = np.where(train_mini['target'] >= 0.5, 1, 0)\n",
        "\n",
        "# target, 심각한 toxicity, 외설적, identity attack, 모욕, 위협\n",
        "y_aux_train = train_mini[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
        "\n",
        "#Take the columns 'comment_text' from test,\n",
        "# then fillall NaN values by emtpy string '' (redundant)\n",
        "x_test = test_mini['comment_text'].fillna('').values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDvzVCCnr-Yu",
        "colab_type": "code",
        "outputId": "1b9764bf-779d-4043-b3e3-a31897a86400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "print(x_train[:5], \"\\n\", x_train.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\"\n",
            " \"Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\"\n",
            " 'This is such an urgent design problem; kudos to you for taking it on. Very impressive!'\n",
            " \"Is this something I'll be able to install on my site? When will you be releasing it?\"\n",
            " 'haha you guys are a bunch of losers.'] \n",
            " (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MuD9zJLx_GD",
        "colab_type": "text"
      },
      "source": [
        "## **Tokeninzing 이후, Tokenizing 결과 값 탐색**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmaeIJZO7qNz",
        "colab_type": "text"
      },
      "source": [
        "LSTM 의 input 으로 sentence 를 넣어야 하지만, 그냥 문장 자체로 넣을 수는 없어서 적절한 변환이 필요하다.\n",
        "\n",
        "1. Tokenization \n",
        "- 문장을 word 단위로 나누어줌. “나는 밥을 먹었다.” => (나는, 밥을, 먹었다)\n",
        "2. Indexing \n",
        "- 단어를 쪼개기는 했지만, 쪼개진 단어를 신경망에 입력으로 넣으려면 정수형으로 표현해주는 indexing 과정이 필요\n",
        "(나는 = 1, 밥을 = 2, 먹었다 = 3)\n",
        "3. Representation \n",
        "- 이제 “나는 밥을 먹었다” 라는 문장을 (1,2,3) 으로 표현할 수 있다. \n",
        "\n",
        "Keras 에서는 이 모든 것을 스스로 해주는 모듈을 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH86JINAuHsw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "8d34aa2e-a04f-4e4b-f58b-ed1832442b66"
      },
      "source": [
        "# https://keras.io/preprocessing/text/\n",
        "# tokenizer is a class with some method\n",
        "tokenizer = text.Tokenizer(num_words=MAX_FEATURES) # MAX_FEATURE = 100000\n",
        "\n",
        "#we apply method fit_on_texts of tokenizer on x_train and x_test\n",
        "#it will initialize some parameters/attribute inside tokenizer\n",
        "#https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L139\n",
        "#https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L210\n",
        "\n",
        "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
        "#for example, after fit_on_texts, we can type\n",
        "\n",
        "# give a OderedDict\n",
        "# dictionary mapping words (str) to the number of times they appeared on during fit\n",
        "#tokenizer.word_counts \n",
        "# result: OrderedDict([('this', 838),('is', 1680), ('so', 373), ('cool', 14), 같은 key, value 형태 \n",
        "\n",
        "# an int Number of documents (texts/sequences) the tokenizer was trained on. \n",
        "# Only set after fit_on_texts or fit_on_sequences was called\n",
        "#tokenizer.document_count \n",
        "# result: 2,000 <- no of documents \n",
        "\n",
        "# a dict of words with correponding indices\n",
        "# dictionary mapping words (str) to their rank/index (int). \n",
        "#tokenizer.word_index \n",
        "# result: {'the': 1, 'to': 2, 'and': 3, 'a': 4, 'of': 5, 'is': 6, 와 같은 key, value 형태\n",
        "\n",
        "# different words in all 'comment_text'\n",
        "#len(tokenizer.word_index) == 410_046\n",
        "# result: 12,164\n",
        "\n",
        "#these words come from all 'comment_text' in training.csv and test.csv\n",
        "#tokenizer.index_word: the inverse of tokenizer.word_index"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bae07c0fe0d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# MAX_FEATURE = 100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#we apply method fit_on_texts of tokenizer on x_train and x_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#it will initialize some parameters/attribute inside tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L139\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MAX_FEATURES' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtph8mAW5b_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L267\n",
        "# we will convert each word in a comment_text to a number.\n",
        "# So a comment_text is a list of number.\n",
        "\n",
        "# tokenizer.texts_to_sequences([\"This is so cool\"]) -> results [[13, 6, 33, 875]]\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# x_train results like\n",
        "# [[13,  6,  33,  875,  50, .....], [337,  11,  13, ......], [...],...]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdUZP2qAAb_D",
        "colab_type": "text"
      },
      "source": [
        "문장의 길이는 정해져있지 않기 때문에,  어떤 댓글은 3개의 단어로 표현될 수도 있고, 어떤글은 100장의 분량으로 표현될수도 있다. \n",
        "\n",
        "뉴럴 네트워크의 input의 길이는 고정되어 있는데, 문장의 수가 다르다고 해서 다른 갯수의 값을 입력할 수 없다.\n",
        "즉, input sentence 에 대해 적절한 처리가 필요함. \n",
        "- 길이가 모자를 시 zero padding 시행\n",
        "- 주어진 최대 길이보다 문장이 길다면, 잘라냄\n",
        "\n",
        "최적의 word 갯수는 어떻게 정해야 할까? \n",
        "너무 길면 overfitting 할 것이고, 너무 적으면 중요한 정보를 포함하지 않을 수 있다.\n",
        "\n",
        "결론은, 데이터 분포를 보고, 모든 데이터의 대부분을 다룰 수 있는 최적의 갯수를 살펴보면 된다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk7AQC11CWtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "totalNumWords = [len(one_comment) for one_comment in x_train] # 각 document의 단어 길이를 check\n",
        "plt.hist(totalNumWords,bins = np.arange(0,max(totalNumWords),max(totalNumWords)/20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwD76UGiPdW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://keras.io/preprocessing/sequence/\n",
        "# https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py\n",
        "# each comment_text is now a list of word\n",
        "# we want the length of this list is a constant -> MAX_LEN\n",
        "# if the list is longer, then we cut/trim it \n",
        "# if shorter, then we add/pad it with 0's at the beginning\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN) # MAX_LEN= 200\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c0i7MjuBO2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coefs(word, *arr):\n",
        "    print(\"get_coefs function in....\")\n",
        "    print(word, np.asarray(arr, dtype='float32'))\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "\n",
        "def load_embeddings(path):\n",
        "    # each line in the file looks like \n",
        "    # apple 0.3 0.4 0.5 0.6 ...\n",
        "    # that is a word followed by 300 float numbers\n",
        "    with open(path) as f:\n",
        "        # return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
        "        # get_coef 함수가 가변인자를 담고 있기 때문에 리스트 데이터를 모두\n",
        "        # unpacking 해서 전달해야 함. 만약 o.strip().split(\" \")을 전달하면,\n",
        "        # 이 자체가 하나의 값으로 쓰여 word에만 해당 값이 하나로 전달 된다.\n",
        "        return dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(f))\n",
        "        # word와 그에 따르는 \n",
        "\n",
        "def build_matrix(word_index, path):\n",
        "    # path: a path that contains embedding matrix\n",
        "    # word_index is a dict of the form ('apple': 123, 'banana': 349, etc)\n",
        "    # that means word_index[word] gives the index of the word\n",
        "    # word_index was built from all commment_texts\n",
        "\n",
        "    # we will construct an embedding_matrix for the words in word_index\n",
        "    # using pre-trained embedding word vectors from 'path'\n",
        "\n",
        "    embedding_index = load_embeddings(path)\n",
        "\n",
        "    #embedding_matrix is a matrix of len(word_index)+1  x 300\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "\n",
        "    # word_index is a dict. Each element is (word:i) where i is the index\n",
        "    # of the word\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            #RHS is a vector of 300d\n",
        "            embedding_matrix[i] = embedding_index[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def build_model(embedding_matrix, num_aux_targets):\n",
        "   # a simpler version can be found here\n",
        "   # https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
        "\n",
        "   # Trainable params of the model: 1,671,687\n",
        "   # Recall that the number of samples in train.csv is 1_804_874\n",
        "\n",
        "    #words is a vector of MAX_LEN dimension\n",
        "    words = Input(shape=(MAX_LEN,))\n",
        "\n",
        "    #Embedding is the keras layer. We use the pre-trained embbeding_matrix\n",
        "    # https://keras.io/layers/embeddings/\n",
        "    # have to say that parameters in this layer are not trainable\n",
        "    # x is a vector of 600 dimension\n",
        "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
        "\n",
        "    #*embedding_matrix.shape is a short way for \n",
        "    #input_dim = embedding_matrix.shape[0], output_dim  = embedding_matrix.shape[1]\n",
        "\n",
        "    #here the author used pre-train embedding matrix.\n",
        "    #instead of train from begining like in tensorflow example\n",
        "\n",
        "    #https://stackoverflow.com/questions/50393666/how-to-understand-spatialdropout1d-and-when-to-use-it\n",
        "    x = SpatialDropout1D(0.25)(x)\n",
        "\n",
        "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    hidden = concatenate([\n",
        "        GlobalMaxPooling1D()(x),\n",
        "        GlobalAveragePooling1D()(x),\n",
        "    ])\n",
        "\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='tanh')(hidden)])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    result = Dense(1, activation='sigmoid', name = 'main_output')(hidden)\n",
        "\n",
        "    #num_aux_targets = 6 since y_aux_train has 6 columns\n",
        "    aux_result = Dense(num_aux_targets, activation='sigmoid', name = 'aux_ouput')(hidden)\n",
        "\n",
        "    model = Model(inputs=words, outputs=[result, aux_result])\n",
        "\n",
        "    #model.summary() will gives a good view of the model structure\n",
        "\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=Adam(clipnorm=0.1),\n",
        "        metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6nq2ogHBUgP",
        "colab_type": "code",
        "outputId": "857df64a-e970-4a76-b9f7-02386e913b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# create an embedding_matrix \n",
        "#after this, embedding_matrix is a matrix of size\n",
        "# len(tokenizer.word_index)+1 x 600\n",
        "# we concatenate two matrices, 600 = 300+300\n",
        "embedding_matrix = np.concatenate(\n",
        "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
        "#embedding_matrix.shape \n",
        "#== (410047, 600)\n",
        "\n",
        "#embedding_matrix[i] is a 600d vector representation of the word whose index is i\n",
        "#embedding_matrix[10]\n",
        "#tokenizer.index_word[10] == 'you'\n",
        "\n",
        "\n",
        "checkpoint_predictions = []\n",
        "weights = []\n",
        "\n",
        "\n",
        "#https://keras.io/callbacks/#learningratescheduler\n",
        "\n",
        "for model_idx in range(NUM_MODELS):\n",
        "  # build the same models\n",
        "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
        "  # We train each model EPOCHS times\n",
        "  # After each epoch, we reset learning rate (we are using Adam Optimizer)  \n",
        "  # https://towardsdatascience.com/learning-rate-scheduler-d8a55747dd90\n",
        "\n",
        "  # https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L921\n",
        "  # learningrate is the attribute 'lr' from Adam optimizer\n",
        "  # see https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L460\n",
        "  # In Adam Optimizer, learning rate is changing after each batch\n",
        "    for global_epoch in range(EPOCHS):\n",
        "        model.fit(\n",
        "            x_train,\n",
        "            [y_train, y_aux_train],\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=[\n",
        "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch), verbose = 1)\n",
        "            ]\n",
        "        )\n",
        "        #model.predict will give two outputs: main_output (target) and aux_output\n",
        "        #we only take main_output\n",
        "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
        "        weights.append(2 ** global_epoch)\n",
        "\n",
        "\n",
        "#take average (with weights) of predictions from two models\n",
        "#predictions is an np.array\n",
        "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9cf7289df0cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# we concatenate two matrices, 600 = 300+300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m embedding_matrix = np.concatenate(\n\u001b[0;32m---> 58\u001b[0;31m     [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m#embedding_matrix.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#== (410047, 600)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-9cf7289df0cb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# we concatenate two matrices, 600 = 300+300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m embedding_matrix = np.concatenate(\n\u001b[0;32m---> 58\u001b[0;31m     [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m#embedding_matrix.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#== (410047, 600)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-5d11d9ca4f76>\u001b[0m in \u001b[0;36mbuild_matrix\u001b[0;34m(word_index, path)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#using pre-trained embedding word vectors from 'path'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#embedding_matrix is a matrix of len(word_index)+1  x 300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-5d11d9ca4f76>\u001b[0m in \u001b[0;36mload_embeddings\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# that is a word followed by 300 float numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m#return dict(get_coefs(*line.strip().split(' ')) for line in f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gdrive/My\\\\ Drive/data/crawl-300d-2M.vec'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPT8Ig2OqLNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.DataFrame.from_dict({\n",
        "    'id': test['id'],\n",
        "    'prediction': predictions\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}