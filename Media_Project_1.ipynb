{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Media_Project #1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "735556af7eea4d5bbd50e71f1b9c317d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_44536f925af44c419ee14088a0e3e9aa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_afb468f216fd401ab39b2ef16dc865c6",
              "IPY_MODEL_392c994eae5342c684a3cfd095b4ab0a"
            ]
          }
        },
        "44536f925af44c419ee14088a0e3e9aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afb468f216fd401ab39b2ef16dc865c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bd1ca52e7d964b91b77caa946b2d2239",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a222138974764c2ab8984b09225b6fec"
          }
        },
        "392c994eae5342c684a3cfd095b4ab0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_878773ef20bd4b4cbd3eef47cc87cb94",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 28/28 [00:05&lt;00:00,  5.50it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_feb7073b2a2f4e73b86909b797fed079"
          }
        },
        "bd1ca52e7d964b91b77caa946b2d2239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a222138974764c2ab8984b09225b6fec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "878773ef20bd4b4cbd3eef47cc87cb94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "feb7073b2a2f4e73b86909b797fed079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "741d72d3c2bd469c850d9b5a78e93b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b00d3afbf41d463b8f7ac7a0ce555885",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cfa82dd712474e72ac67866dc0f6a6cf",
              "IPY_MODEL_7b5cb9da93dd4a73a5fecc0ce34376c8"
            ]
          }
        },
        "b00d3afbf41d463b8f7ac7a0ce555885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cfa82dd712474e72ac67866dc0f6a6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2c5632f83e8544239b93d98aad57afef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 492,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 492,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_042642f1a72c4762880c78dc63a52b00"
          }
        },
        "7b5cb9da93dd4a73a5fecc0ce34376c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_494f2aa8591e46a280937ead9b093e37",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 492/492 [00:00&lt;00:00, 1000.27it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2266487323844963ae5d437fe36183a9"
          }
        },
        "2c5632f83e8544239b93d98aad57afef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "042642f1a72c4762880c78dc63a52b00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "494f2aa8591e46a280937ead9b093e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2266487323844963ae5d437fe36183a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaehyun0220/Colab/blob/master/Media_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmdavC_fLDrU",
        "colab_type": "text"
      },
      "source": [
        "#10조. 네이버 댓글 분석을 통한 상위, 하위 클립 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnLbyZflC1R1",
        "colab_type": "code",
        "outputId": "9b41c525-6fca-496c-d31e-05e9d017e3b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Auth 인증 및 Google Drive 활용 Data load\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5oQGfjLC-Jb",
        "colab_type": "code",
        "outputId": "1f2c0241-b343-49c3-8907-5b1ca361bcf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!ls ../gdrive/My\\ Drive/output"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_ep10.csv\t file_ep16.csv\tfile_ep22.csv  file_ep4.csv\n",
            "file_ep11.csv\t file_ep17.csv\tfile_ep23.csv  file_ep5.csv\n",
            "file_ep12.csv\t file_ep18.csv\tfile_ep24.csv  file_ep6.csv\n",
            "file_ep13.csv\t file_ep19.csv\tfile_ep25.csv  file_ep7.csv\n",
            "file_ep14_1.csv  file_ep1.csv\tfile_ep26.csv  file_ep8.csv\n",
            "file_ep14.csv\t file_ep20.csv\tfile_ep2.csv   file_ep9.csv\n",
            "file_ep15.csv\t file_ep21.csv\tfile_ep3.csv   TheLastEmpress.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bdQCRGv7AX6",
        "colab_type": "code",
        "outputId": "93eec62f-1918-4235-b8c3-4612dd014c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!pip install regex"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: regex\n",
            "Successfully installed regex-2019.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEGh8u0qDI6h",
        "colab_type": "code",
        "outputId": "18ad03d3-0ade-4576-b2ac-9e1ba10bebc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "# 기본 라이브러리 로드\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import regex as re\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "#데이터 전처리 관련 라이브러리 로드\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "#모델 알고리즘 로드\n",
        "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "\n",
        "# Deep Learning Model 로드\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "#차원축소 알고리즘 로드\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#HyperParameter Tuning을 위한 라이브러리 로드\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#모델 평가를 위한 라이브러리 로드\n",
        "from sklearn import metrics, model_selection\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "\n",
        "#데이터 분리를 위한 라이브러리 로드\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#수학 & 통계 관련 라이브러리 로드\n",
        "import scipy.stats as st\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl  # 기본 설정 만지는 용도\n",
        "import matplotlib.pyplot as plt  # 그래프 그리는 용도\n",
        "import matplotlib.font_manager as fm  # 폰트 관련 용도\n",
        "\n",
        "\n",
        "#Configure Visualization Defaults\n",
        "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
        "%matplotlib inline\n",
        "mpl.style.use('ggplot')\n",
        "sns.set_style('white')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpUzHCYqMYp1",
        "colab_type": "code",
        "outputId": "17d6f7c8-c49e-471e-c197-7ae3eba84f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "filelist = os.listdir('../gdrive/My Drive/output')\n",
        "filelist"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['file_ep1.csv',\n",
              " 'file_ep16.csv',\n",
              " 'file_ep17.csv',\n",
              " 'file_ep10.csv',\n",
              " 'file_ep14.csv',\n",
              " 'file_ep12.csv',\n",
              " 'file_ep11.csv',\n",
              " 'file_ep14_1.csv',\n",
              " 'file_ep15.csv',\n",
              " 'file_ep13.csv',\n",
              " 'file_ep18.csv',\n",
              " 'file_ep19.csv',\n",
              " 'file_ep21.csv',\n",
              " 'file_ep20.csv',\n",
              " 'file_ep2.csv',\n",
              " 'file_ep22.csv',\n",
              " 'file_ep23.csv',\n",
              " 'file_ep25.csv',\n",
              " 'file_ep24.csv',\n",
              " 'file_ep26.csv',\n",
              " 'file_ep3.csv',\n",
              " 'file_ep4.csv',\n",
              " 'file_ep5.csv',\n",
              " 'file_ep6.csv',\n",
              " 'file_ep9.csv',\n",
              " 'file_ep7.csv',\n",
              " 'TheLastEmpress.csv',\n",
              " 'file_ep8.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Ink4z2DVoG",
        "colab_type": "code",
        "outputId": "35b41d1f-bdc1-43c6-a10b-a137d8a2a86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "735556af7eea4d5bbd50e71f1b9c317d",
            "44536f925af44c419ee14088a0e3e9aa",
            "afb468f216fd401ab39b2ef16dc865c6",
            "392c994eae5342c684a3cfd095b4ab0a",
            "bd1ca52e7d964b91b77caa946b2d2239",
            "a222138974764c2ab8984b09225b6fec",
            "878773ef20bd4b4cbd3eef47cc87cb94",
            "feb7073b2a2f4e73b86909b797fed079"
          ]
        }
      },
      "source": [
        "# 총 26회차 491개 하이라이트 클립 존재 (전체 재생수 = 107,221,654 / 클립 당 평균 재생수 = 218,374), \n",
        "# 이 중에서 예고편, 미공개, 인터뷰 등 클립 제외하고 총 422회 클립 대상\n",
        "df_title = pd.read_csv('../gdrive/My Drive/output/TheLastEmpress.csv', encoding = 'euc-kr')\n",
        "df_title.rename(columns=lambda x: re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》 ]', '', x), inplace=True)\n",
        "\n",
        "# 조회수 분포를 고려하여 각 회차별로 조회수 상위 4개, 하위 4개 클립을 샘플링 - 총 208개 클립\n",
        "# 좋아요수, 댓글 수, 댓글 내용, 댓글 작성자 정보 (웹크롤링 통한 추출)\n",
        "\n",
        "df_ep_tot = pd.DataFrame()\n",
        "for i in tqdm_notebook(filelist):\n",
        "  if (i[:4] == 'file'):\n",
        "    df_ep_temp = pd.read_csv('../gdrive/My Drive/output/'+i)\n",
        "    df_ep_temp['play'] = df_ep_temp['play'].apply(lambda x: int(re.sub(',','', x[4:])))\n",
        "    df_ep_temp['rank'] = df_ep_temp['play'].rank(method='dense', ascending=False)\n",
        "    df_ep_tot = df_ep_tot.append(df_ep_temp)\n",
        "\n",
        "df_ep_tot.drop(columns='Unnamed: 0', inplace=True)\n",
        "df_ep_tot['target'] = np.where(df_ep_tot['rank']<=4,1,0)\n",
        "df_ep_tot"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "735556af7eea4d5bbd50e71f1b9c317d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=28), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>서지안</td>\n",
              "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>경</td>\n",
              "      <td>ㅏ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1841</th>\n",
              "      <td>Major</td>\n",
              "      <td>와 ㅅㅂ...피지컬봐....</td>\n",
              "      <td>550</td>\n",
              "      <td>1</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1842</th>\n",
              "      <td>스폰지밥</td>\n",
              "      <td>이쁘십니다 할때 나만설렜냐,,,,,,,</td>\n",
              "      <td>691</td>\n",
              "      <td>4</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1843</th>\n",
              "      <td>박한별</td>\n",
              "      <td>반했네</td>\n",
              "      <td>372</td>\n",
              "      <td>4</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1844</th>\n",
              "      <td>김민정</td>\n",
              "      <td>죄송하지만 이 분 이용합시다 ! 어쨌든 좋은게 좋은거죠 ..</td>\n",
              "      <td>593</td>\n",
              "      <td>4</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1845</th>\n",
              "      <td>블링블링</td>\n",
              "      <td>우빈이한테 전부 들이대는군</td>\n",
              "      <td>338</td>\n",
              "      <td>3</td>\n",
              "      <td>이희진, 근육남 최진혁에 공주표 애교 “마이 아포”</td>\n",
              "      <td>155236</td>\n",
              "      <td>645</td>\n",
              "      <td>173</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40935 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          nick                                           contents  ...  rank  target\n",
              "0     rosi****  ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...  ...   1.0       1\n",
              "1       핑크에메랄드  왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...  ...   1.0       1\n",
              "2          서지안                                           ㅋㅋㅋㅋㅋㅋㅋㅋ  ...   1.0       1\n",
              "3            경                                                  ㅏ  ...   1.0       1\n",
              "4           홍홍            이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발  ...   1.0       1\n",
              "...        ...                                                ...  ...   ...     ...\n",
              "1841     Major                                    와 ㅅㅂ...피지컬봐....  ...   8.0       0\n",
              "1842      스폰지밥                              이쁘십니다 할때 나만설렜냐,,,,,,,  ...   8.0       0\n",
              "1843       박한별                                                반했네  ...   8.0       0\n",
              "1844       김민정                  죄송하지만 이 분 이용합시다 ! 어쨌든 좋은게 좋은거죠 ..  ...   8.0       0\n",
              "1845      블링블링                                     우빈이한테 전부 들이대는군  ...   8.0       0\n",
              "\n",
              "[40935 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-6DDOI-DyW_",
        "colab_type": "code",
        "outputId": "976d23c4-499d-4071-cd2c-e52e033f5960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "display(df_title.head())\n",
        "display(df_title.info())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14</td>\n",
              "      <td>[숨멎 엔딩] 신성록, 설렘 폭발하는 반전 섹시미!</td>\n",
              "      <td>892539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>장나라, 악녀 이엘리야 때려잡는 카리스마 “얻다대고 반말”</td>\n",
              "      <td>852569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19</td>\n",
              "      <td>신은경 완벽 빙의한 ‘오아린 더빙 연기’</td>\n",
              "      <td>811897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>“사랑해요 폐하” 장나라, 신성록 계획 박살 내며 ‘흑화 스위치 ON’</td>\n",
              "      <td>757992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>뛰는 이엘리야 위 나는 오아린, 혼신의 눈물 연기(Feat. 윤소이 불꽃 따귀)</td>\n",
              "      <td>725808</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   episode                                         title    play\n",
              "0       14                  [숨멎 엔딩] 신성록, 설렘 폭발하는 반전 섹시미!  892539\n",
              "1        8              장나라, 악녀 이엘리야 때려잡는 카리스마 “얻다대고 반말”  852569\n",
              "2       19                        신은경 완벽 빙의한 ‘오아린 더빙 연기’  811897\n",
              "3       10       “사랑해요 폐하” 장나라, 신성록 계획 박살 내며 ‘흑화 스위치 ON’  757992\n",
              "4       10  뛰는 이엘리야 위 나는 오아린, 혼신의 눈물 연기(Feat. 윤소이 불꽃 따귀)  725808"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 422 entries, 0 to 421\n",
            "Data columns (total 3 columns):\n",
            "episode    422 non-null int64\n",
            "title      422 non-null object\n",
            "play       422 non-null int64\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 10.0+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCdCQjW8D5xZ",
        "colab_type": "code",
        "outputId": "15369205-a1f7-4fb2-9403-b70a18ed2650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "display(df_ep_tot.head())\n",
        "display(df_ep_tot.info())\n",
        "# df_ep_tot[df_ep_tot['episode'] == 1].sample(10).sort_values(by='play', ascending = False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>서지안</td>\n",
              "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>경</td>\n",
              "      <td>ㅏ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       nick                                           contents  ...  rank  target\n",
              "0  rosi****  ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...  ...   1.0       1\n",
              "1    핑크에메랄드  왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...  ...   1.0       1\n",
              "2       서지안                                           ㅋㅋㅋㅋㅋㅋㅋㅋ  ...   1.0       1\n",
              "3         경                                                  ㅏ  ...   1.0       1\n",
              "4        홍홍            이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발  ...   1.0       1\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 40935 entries, 0 to 1845\n",
            "Data columns (total 11 columns):\n",
            "nick           40935 non-null object\n",
            "contents       40801 non-null object\n",
            "recomm         40935 non-null int64\n",
            "unrecomm       40935 non-null int64\n",
            "title          40935 non-null object\n",
            "play           40935 non-null int64\n",
            "like           40935 non-null object\n",
            "reple_count    40935 non-null object\n",
            "episode        40935 non-null int64\n",
            "rank           40935 non-null float64\n",
            "target         40935 non-null int64\n",
            "dtypes: float64(1), int64(5), object(5)\n",
            "memory usage: 3.7+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EoZ0vAPlXk6E",
        "colab": {}
      },
      "source": [
        "# for name, group in df_ep.groupby(by='title'):\n",
        "#   print(name)\n",
        "#   print(group['like'].rank(method='min'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7MxLLtPX7gN",
        "colab_type": "text"
      },
      "source": [
        "### 1화 댓글 대상 sample set test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cA7gB0gV3HF",
        "colab_type": "code",
        "outputId": "2591eae7-8005-4660-d022-36ee8ebd33cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df_ep_sample = df_ep_tot[df_ep_tot['episode'] == 1].copy()\n",
        "df_ep_sample"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>서지안</td>\n",
              "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>경</td>\n",
              "      <td>ㅏ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>싹스리</td>\n",
              "      <td>ㆍ연기가 좀 오바한다 좀 자연스럽게 스테파니야</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>갓갓</td>\n",
              "      <td>본방 사운드도 이상하더니ㅋㅋㅋㅋㅋㅋㅋ스브스 왜그래.... 노답</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>뿅뿅</td>\n",
              "      <td>영상 소리 안나와요ㅡㅡ 올리고 확인좀...</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>힘내♡</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>ㅋㅋㅋ</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>492 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         nick                                           contents  ...  rank  target\n",
              "0    rosi****  ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...  ...   1.0       1\n",
              "1      핑크에메랄드  왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...  ...   1.0       1\n",
              "2         서지안                                           ㅋㅋㅋㅋㅋㅋㅋㅋ  ...   1.0       1\n",
              "3           경                                                  ㅏ  ...   1.0       1\n",
              "4          홍홍            이게 나라냐? 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발  ...   1.0       1\n",
              "..        ...                                                ...  ...   ...     ...\n",
              "487       싹스리                          ㆍ연기가 좀 오바한다 좀 자연스럽게 스테파니야  ...   8.0       0\n",
              "488        갓갓                 본방 사운드도 이상하더니ㅋㅋㅋㅋㅋㅋㅋ스브스 왜그래.... 노답  ...   8.0       0\n",
              "489        뿅뿅                            영상 소리 안나와요ㅡㅡ 올리고 확인좀...  ...   8.0       0\n",
              "490       뽀글이                                                힘내♡  ...   8.0       0\n",
              "491       뽀글이                                                ㅋㅋㅋ  ...   8.0       0\n",
              "\n",
              "[492 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52VbARCKYxHd",
        "colab_type": "code",
        "outputId": "5a0af68f-e26b-4ef2-d9d4-f76407356490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "!pip3 install konlpy"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 410kB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.17.4)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 28.9MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, JPype1, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-0.7.0 beautifulsoup4-4.6.0 colorama-0.4.1 konlpy-0.5.2 tweepy-3.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qHvTttUYJQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Kkma, Okt\n",
        "from konlpy.utils import pprint\n",
        "\n",
        "okt =Okt()\n",
        "kkma = Kkma()\n",
        "# mecab = Mecab()\n",
        "# pprint(kkma.sentences(u'네, 안녕하세요. 반갑습니다.'))\n",
        "\n",
        "# sentence = u'만 6세 이하의 초등학교 취학 전 자녀를 양육하기 위해서는'\n",
        "# words = konlpy.tag.Twitter().pos(sentence)\n",
        "# print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LzY1qM44bVrA",
        "colab": {}
      },
      "source": [
        "def morphs_okt(x):\n",
        "  res = okt.morphs(x)\n",
        "  if len(res) >= 1:\n",
        "    res = [re.sub('[ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ ]', '', res[i]) for i in range(len(res)) if re.sub('[ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ ]', '', res[i]) != '' and len(res[i]) >= 1]\n",
        "  else:\n",
        "    res = ''\n",
        "  res = '' if not res else res\n",
        "  return res\n",
        "\n",
        "def morphs_kkma(x):\n",
        "  res = kkma.morphs(x)\n",
        "  if len(res) >= 1:\n",
        "    res = [re.sub('[ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ ]', '', res[i]) for i in range(len(res)) if re.sub('[ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ ]', '', res[i]) != '' and len(res[i]) >= 1]\n",
        "  else:\n",
        "    res = ''\n",
        "  res = '' if not res else res\n",
        "  return res\n",
        "\n",
        "# def morphs_mecab(x):\n",
        "#   res = kkma.morphs(x)\n",
        "#   if len(res) > 1:\n",
        "#     res = [re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》ㅋㅡ ]', '', res[i]) for i in range(len(res)) if len(res[i]) > 1]\n",
        "#   else:\n",
        "#     res = ''\n",
        "#   return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyG91a40YN8M",
        "colab_type": "code",
        "outputId": "692d7310-6013-4d2c-af6f-1ea69882540e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "df_ep_sample['contents'] = df_ep_sample['contents'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "df_ep_sample['okt_token'] = df_ep_sample['contents'].apply(lambda x: morphs_okt(x))\n",
        "df_ep_sample['kkma_token'] = df_ep_sample['contents'].apply(lambda x: morphs_kkma(x))\n",
        "df_ep_sample['okt_token_str'] = df_ep_sample['okt_token'].apply(lambda x: ' '.join(x))\n",
        "df_ep_sample['kkma_token_str'] = df_ep_sample['kkma_token'].apply(lambda x: ' '.join(x))\n",
        "df_ep_sample"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "      <th>okt_token</th>\n",
              "      <th>kkma_token</th>\n",
              "      <th>okt_token_str</th>\n",
              "      <th>kkma_token_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[옷, 입고, 목욕탕, 들어가는거, 웃기, 넼, 저, 거, 명품, 일, 텐데]</td>\n",
              "      <td>[옷, 입, 고, 목욕탕, 들어가, 는, 거, 웃기, 넼, 저거, 명품, 이, 터,...</td>\n",
              "      <td>옷 입고 목욕탕 들어가는거 웃기 넼 저 거 명품 일 텐데</td>\n",
              "      <td>옷 입 고 목욕탕 들어가 는 거 웃기 넼 저거 명품 이 터 이 데</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[왠지, 선, 황제, 가, 바람, 은, 못, 폈을거, 같다는, 생각, 이, 든다, ...</td>\n",
              "      <td>[왠지, 선, 황제, 가, 바람, 은, 못, 피, 었, 을, 거, 같, 다는, 생각...</td>\n",
              "      <td>왠지 선 황제 가 바람 은 못 폈을거 같다는 생각 이 든다 태후 가 황후 로 있는한...</td>\n",
              "      <td>왠지 선 황제 가 바람 은 못 피 었 을 거 같 다는 생각 이 들 다 태 후 가 황...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>서지안</td>\n",
              "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>경</td>\n",
              "      <td>ㅏ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[이, 게, 나라, 냐, 방송, 에서, 이딴, 수위, 가, 나오고, 지랄, 이야, ...</td>\n",
              "      <td>[이것, 이, 나라, 이, 냐, 방송, 에서, 이딴, 수위, 가, 나오, 고, 지랄...</td>\n",
              "      <td>이 게 나라 냐 방송 에서 이딴 수위 가 나오고 지랄 이야 진짜 개 좃헬 조선 시발</td>\n",
              "      <td>이것 이 나라 이 냐 방송 에서 이딴 수위 가 나오 고 지랄 이 야 진짜 개 좃 헤...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>싹스리</td>\n",
              "      <td>연기가 좀 오바한다 좀 자연스럽게 스테파니야</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[연기, 가, 좀, 오, 바, 한다, 좀, 자연, 스럽게, 스테파니, 야]</td>\n",
              "      <td>[연기, 가, 좀, 오, 바, 하, 다, 좀, 자연, 스럽, 게, 스테파니, 야]</td>\n",
              "      <td>연기 가 좀 오 바 한다 좀 자연 스럽게 스테파니 야</td>\n",
              "      <td>연기 가 좀 오 바 하 다 좀 자연 스럽 게 스테파니 야</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>갓갓</td>\n",
              "      <td>본방 사운드도 이상하더니ㅋㅋㅋㅋㅋㅋㅋ스브스 왜그래 노답</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[본방, 사운드, 도, 이상하더니, 스브스, 왜, 그래, 노답]</td>\n",
              "      <td>[본방, 사운드, 도, 이상, 하, 더니, 스브스, 왜, 그리하, 여, 노, 답]</td>\n",
              "      <td>본방 사운드 도 이상하더니 스브스 왜 그래 노답</td>\n",
              "      <td>본방 사운드 도 이상 하 더니 스브스 왜 그리하 여 노 답</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>뿅뿅</td>\n",
              "      <td>영상 소리 안나와요ㅡㅡ 올리고 확인좀</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[영상, 소리, 안, 나와요, 올리고, 확인, 좀]</td>\n",
              "      <td>[영상, 소리, 안, 나오, 아요, 올리, 고, 확인, 좀]</td>\n",
              "      <td>영상 소리 안 나와요 올리고 확인 좀</td>\n",
              "      <td>영상 소리 안 나오 아요 올리 고 확인 좀</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>힘내</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[힘내]</td>\n",
              "      <td>[힘내]</td>\n",
              "      <td>힘내</td>\n",
              "      <td>힘내</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>ㅋㅋㅋ</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>492 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         nick  ...                                     kkma_token_str\n",
              "0    rosi****  ...               옷 입 고 목욕탕 들어가 는 거 웃기 넼 저거 명품 이 터 이 데\n",
              "1      핑크에메랄드  ...  왠지 선 황제 가 바람 은 못 피 었 을 거 같 다는 생각 이 들 다 태 후 가 황...\n",
              "2         서지안  ...                                                   \n",
              "3           경  ...                                                   \n",
              "4          홍홍  ...  이것 이 나라 이 냐 방송 에서 이딴 수위 가 나오 고 지랄 이 야 진짜 개 좃 헤...\n",
              "..        ...  ...                                                ...\n",
              "487       싹스리  ...                    연기 가 좀 오 바 하 다 좀 자연 스럽 게 스테파니 야\n",
              "488        갓갓  ...                   본방 사운드 도 이상 하 더니 스브스 왜 그리하 여 노 답\n",
              "489        뿅뿅  ...                            영상 소리 안 나오 아요 올리 고 확인 좀\n",
              "490       뽀글이  ...                                                 힘내\n",
              "491       뽀글이  ...                                                   \n",
              "\n",
              "[492 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nYNT1D03sVx",
        "colab_type": "code",
        "outputId": "d26450e3-8747-4cb4-b806-e4e2023d5212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        }
      },
      "source": [
        "df_ep_token = df_ep_sample.drop(df_ep_sample[df_ep_sample['okt_token'] == ''].index)\n",
        "df_ep_token"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nick</th>\n",
              "      <th>contents</th>\n",
              "      <th>recomm</th>\n",
              "      <th>unrecomm</th>\n",
              "      <th>title</th>\n",
              "      <th>play</th>\n",
              "      <th>like</th>\n",
              "      <th>reple_count</th>\n",
              "      <th>episode</th>\n",
              "      <th>rank</th>\n",
              "      <th>target</th>\n",
              "      <th>okt_token</th>\n",
              "      <th>kkma_token</th>\n",
              "      <th>okt_token_str</th>\n",
              "      <th>kkma_token_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rosi****</td>\n",
              "      <td>ㅅㅂ옷입고 목욕탕 들어가는거 ㅈㄴ웃기넼ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ저거 명품일 텐데ㅋㅋㅋㅋ...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[옷, 입고, 목욕탕, 들어가는거, 웃기, 넼, 저, 거, 명품, 일, 텐데]</td>\n",
              "      <td>[옷, 입, 고, 목욕탕, 들어가, 는, 거, 웃기, 넼, 저거, 명품, 이, 터,...</td>\n",
              "      <td>옷 입고 목욕탕 들어가는거 웃기 넼 저 거 명품 일 텐데</td>\n",
              "      <td>옷 입 고 목욕탕 들어가 는 거 웃기 넼 저거 명품 이 터 이 데</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>핑크에메랄드</td>\n",
              "      <td>왠지 선황제가 바람은 못폈을거 같다는 생각이 든다 태후가 황후로 있는한 감히 생각도...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[왠지, 선, 황제, 가, 바람, 은, 못, 폈을거, 같다는, 생각, 이, 든다, ...</td>\n",
              "      <td>[왠지, 선, 황제, 가, 바람, 은, 못, 피, 었, 을, 거, 같, 다는, 생각...</td>\n",
              "      <td>왠지 선 황제 가 바람 은 못 폈을거 같다는 생각 이 든다 태후 가 황후 로 있는한...</td>\n",
              "      <td>왠지 선 황제 가 바람 은 못 피 었 을 거 같 다는 생각 이 들 다 태 후 가 황...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홍홍</td>\n",
              "      <td>이게 나라냐 방송에서 이딴수위가 나오고 지랄이야 진짜 개좃헬조선 시발</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[이, 게, 나라, 냐, 방송, 에서, 이딴, 수위, 가, 나오고, 지랄, 이야, ...</td>\n",
              "      <td>[이것, 이, 나라, 이, 냐, 방송, 에서, 이딴, 수위, 가, 나오, 고, 지랄...</td>\n",
              "      <td>이 게 나라 냐 방송 에서 이딴 수위 가 나오고 지랄 이야 진짜 개 좃헬 조선 시발</td>\n",
              "      <td>이것 이 나라 이 냐 방송 에서 이딴 수위 가 나오 고 지랄 이 야 진짜 개 좃 헤...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>앵그리햄</td>\n",
              "      <td>이엘리야님 진짜 예쁘다</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[이엘리야, 님, 진짜, 예쁘다]</td>\n",
              "      <td>[이, 엘리야, 님, 진짜, 예쁘, 다]</td>\n",
              "      <td>이엘리야 님 진짜 예쁘다</td>\n",
              "      <td>이 엘리야 님 진짜 예쁘 다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>doloing</td>\n",
              "      <td>오늘은 이거다</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>신은경, 핸드폰 너머 이엘리야의 의도적 신음에 ‘분노 폭발’</td>\n",
              "      <td>474498</td>\n",
              "      <td>842</td>\n",
              "      <td>185</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>[오늘, 은, 이, 거, 다]</td>\n",
              "      <td>[오늘, 은, 이거, 이, 다]</td>\n",
              "      <td>오늘 은 이 거 다</td>\n",
              "      <td>오늘 은 이거 이 다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>승은</td>\n",
              "      <td>이겨서 돈 받을 차례 아니었나 돈은 받고 쳐들어가지 아깝ㅜㅜㅜㅜㅜㅜㅜ</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[이겨서, 돈, 받을, 차례, 아니었나, 돈, 은, 받고, 쳐, 들어가지, 아깝]</td>\n",
              "      <td>[이기, 어서, 돈, 받, 을, 차례, 아니, 었, 나, 돈, 은, 받, 고, 쳐들...</td>\n",
              "      <td>이겨서 돈 받을 차례 아니었나 돈 은 받고 쳐 들어가지 아깝</td>\n",
              "      <td>이기 어서 돈 받 을 차례 아니 었 나 돈 은 받 고 쳐들어가 지 아 까</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>싹스리</td>\n",
              "      <td>연기가 좀 오바한다 좀 자연스럽게 스테파니야</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[연기, 가, 좀, 오, 바, 한다, 좀, 자연, 스럽게, 스테파니, 야]</td>\n",
              "      <td>[연기, 가, 좀, 오, 바, 하, 다, 좀, 자연, 스럽, 게, 스테파니, 야]</td>\n",
              "      <td>연기 가 좀 오 바 한다 좀 자연 스럽게 스테파니 야</td>\n",
              "      <td>연기 가 좀 오 바 하 다 좀 자연 스럽 게 스테파니 야</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>갓갓</td>\n",
              "      <td>본방 사운드도 이상하더니ㅋㅋㅋㅋㅋㅋㅋ스브스 왜그래 노답</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[본방, 사운드, 도, 이상하더니, 스브스, 왜, 그래, 노답]</td>\n",
              "      <td>[본방, 사운드, 도, 이상, 하, 더니, 스브스, 왜, 그리하, 여, 노, 답]</td>\n",
              "      <td>본방 사운드 도 이상하더니 스브스 왜 그래 노답</td>\n",
              "      <td>본방 사운드 도 이상 하 더니 스브스 왜 그리하 여 노 답</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>뿅뿅</td>\n",
              "      <td>영상 소리 안나와요ㅡㅡ 올리고 확인좀</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[영상, 소리, 안, 나와요, 올리고, 확인, 좀]</td>\n",
              "      <td>[영상, 소리, 안, 나오, 아요, 올리, 고, 확인, 좀]</td>\n",
              "      <td>영상 소리 안 나와요 올리고 확인 좀</td>\n",
              "      <td>영상 소리 안 나오 아요 올리 고 확인 좀</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>뽀글이</td>\n",
              "      <td>힘내</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>스테파니 리, 철딱서니 없는 도박 중독 윤다훈에 격노 ‘그 손 잘라버려!’</td>\n",
              "      <td>55267</td>\n",
              "      <td>172</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>[힘내]</td>\n",
              "      <td>[힘내]</td>\n",
              "      <td>힘내</td>\n",
              "      <td>힘내</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>484 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         nick  ...                                     kkma_token_str\n",
              "0    rosi****  ...               옷 입 고 목욕탕 들어가 는 거 웃기 넼 저거 명품 이 터 이 데\n",
              "1      핑크에메랄드  ...  왠지 선 황제 가 바람 은 못 피 었 을 거 같 다는 생각 이 들 다 태 후 가 황...\n",
              "4          홍홍  ...  이것 이 나라 이 냐 방송 에서 이딴 수위 가 나오 고 지랄 이 야 진짜 개 좃 헤...\n",
              "5        앵그리햄  ...                                    이 엘리야 님 진짜 예쁘 다\n",
              "6     doloing  ...                                        오늘 은 이거 이 다\n",
              "..        ...  ...                                                ...\n",
              "486        승은  ...           이기 어서 돈 받 을 차례 아니 었 나 돈 은 받 고 쳐들어가 지 아 까\n",
              "487       싹스리  ...                    연기 가 좀 오 바 하 다 좀 자연 스럽 게 스테파니 야\n",
              "488        갓갓  ...                   본방 사운드 도 이상 하 더니 스브스 왜 그리하 여 노 답\n",
              "489        뿅뿅  ...                            영상 소리 안 나오 아요 올리 고 확인 좀\n",
              "490       뽀글이  ...                                                 힘내\n",
              "\n",
              "[484 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyS88sBUT_Tk",
        "colab_type": "text"
      },
      "source": [
        "## 공통 영역: Word Embedding을 위한 Hyper parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT_z_0uNV8fp",
        "colab_type": "code",
        "outputId": "2b598840-b9e2-4f45-eade-8362513440a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# Hyper Param setting\n",
        "\n",
        "# token의 Histogram 분포를 바탕으로 대부분의 단어 길이 cover 가능한 단어 개수 찾기\n",
        "# 신경망 학습을 위한 input 벡터 길이로 사용 - 적정 길이는 tokenizng 이후 분포를 보고 결정(코드 하단)\n",
        "totalLenSent = [len(x) for x in df_ep_sample['kkma_token']] # 각 document의 단어 길이를 check\n",
        "plt.hist(totalLenSent,bins = np.arange(0,max(totalLenSent),max(totalLenSent)/20))\n",
        "\n",
        "print(np.percentile(totalLenSent, 95)) # 95%를 커버하는 수치는 41\n",
        "\n",
        "# MAX_LEN = int(np.percentile(totalLenSent, 95))\n",
        "MAX_LEN = 128\n",
        "print(MAX_LEN)\n",
        "\n",
        "# pre-trained Embedding을 몇 개 사용할 지 결정\n",
        "NUM_MODELS = 1\n",
        "\n",
        "# input data 원문에서 보존할 최대 단어 개수 \n",
        "# 전체 데이터셋에서 나타나는 unique 한 단어 수(넉넉하게 백단위 올림하여 setting)\n",
        "from itertools import chain\n",
        "\n",
        "sum_lists = list(chain.from_iterable(df_ep_sample['kkma_token']))\n",
        "totalCntWords = int(math.ceil(len(set(sum_lists))/100)*100)\n",
        "\n",
        "MAX_FEATURES = totalCntWords\n",
        "print(len(set(sum_lists)), MAX_FEATURES)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41.0\n",
            "128\n",
            "1196 1200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwIUNatxUKw0",
        "colab_type": "text"
      },
      "source": [
        "## Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46mrhFm373Ag",
        "colab_type": "code",
        "outputId": "e01bf915-75ec-4626-d130-c7314f9d6efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# !ls ../gdrive/My\\ Drive/data/transformers\n",
        "!ls ../gdrive/My\\ Drive/data/bert-based-multilingual-cased"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '../gdrive/My Drive/data/bert-based-multilingual-cased': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZkCMh37IwK",
        "colab_type": "code",
        "outputId": "35e1cc81-d028-4f31-a31b-7bb838d5facf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!pip install sacremoses sentencepiece "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 3.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.28.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=14395f1289f7d353ee0af26649fb7f4ebcde0ffd08062ba2586b4b07e5f2e4c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece\n",
            "Successfully installed sacremoses-0.0.35 sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7LqtYkGG3YZ",
        "colab_type": "text"
      },
      "source": [
        "### Pytorch 환경 내에서 BERT를 사용하기 위한 BERT 관련 Library Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW3fUFWuZoBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sacremoses\n",
        "import sentencepiece\n",
        "\n",
        "import pickle\n",
        "import shutil\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "package_dir_pretrained = \"../gdrive/My Drive/data/bert-base-multilingual-cased/\"\n",
        "package_dir_pytorch_bert = \"../gdrive/My Drive/data/\"\n",
        "sys.path.append(package_dir_pretrained)\n",
        "sys.path.append(package_dir_pytorch_bert)\n",
        "\n",
        "# from transformers import convert_tf_checkpoint_to_pytorch\n",
        "from transformers import convert_bert_original_tf_checkpoint_to_pytorch\n",
        "\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification, BertAdam\n",
        "from transformers.modeling_bert import BertForSequenceClassification\n",
        "from transformers.tokenization_bert import BertTokenizer\n",
        "from transformers.optimization import AdamW\n",
        "\n",
        "from transformers import BertConfig # This is the Bert configuration file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSsvbhVH04BL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "9583b2ef-6533-4ea2-db45-82ebfd3aab6c"
      },
      "source": [
        "!ls /gdrive/My\\ Drive/data/bert-base-multilingual-cased/"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-multilingual-cased-vocab.txt\tbert_model.ckpt.meta\n",
            "bert_config.json\t\t\tconfig.json\n",
            "bert_model.ckpt.data-00000-of-00001\tpytorch_model.bin\n",
            "bert_model.ckpt.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ-9efrPGAMH",
        "colab_type": "text"
      },
      "source": [
        "### Bert 관련 PATH 설정 (모델 PATH, TF CHECK POINT, BERT_CONFIG.JSON 등 로드)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FI7QIBsBbMX",
        "colab_type": "code",
        "outputId": "84da7728-ea2d-42c5-b856-68ba4d0d2e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Translate model from tensorflow to pytorch\n",
        "BERT_MODEL_PATH = package_dir_pretrained\n",
        "vocab = 'bert-base-multilingual-cased-vocab.txt'\n",
        "# BERT_MODEL_PATH bert_config.json file의 설정대로, BERT_MODEL_PATH의 bert_model.ckpt 파일을 load하여 WORK_DIR의 pytorch_model.bin이라는 model로 저장\n",
        "# A TensorFlow checkpoint (bert_model.ckpt) containing the pre-trained weights (which is actually 3 files).\n",
        "# A config file (bert_config.json) which specifies the hyperparameters of the model.\n",
        "\n",
        "print(BERT_MODEL_PATH)\n",
        "\n",
        "# bert_model.ckpt: tf에서 학습한 bert_model의 check point file,  contains pre-trained weights (with 3 files inside) - bert_model.ckpt.data-00000-of-00001, bert_model.ckpt.index, bert_model.ckpt.meta\n",
        "# bert_config.json: store hyperparameters\n",
        "# A vocab file (vocab.txt): map WordPiece to word id\n",
        "convert_bert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
        "    BERT_MODEL_PATH + 'bert_model.ckpt', \n",
        "    BERT_MODEL_PATH + 'bert_config.json',\n",
        "    BERT_MODEL_PATH + 'pytorch_model.bin')\n",
        "\n",
        "# 읽어온 BERT_MODEL_PATH bert_config.json file의 설정을 그대로 WORK_DIR(현재 작업 디렉터리)의 bert_config.json이라는 이름으로 저장\n",
        "# following files must be located in that folder: vocab.txt - vocabulary file/ pytorch_model.bin - the PyTorch-compatible (and converted) model/ config.json - json-based model configuration\n",
        "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', BERT_MODEL_PATH + 'config.json')\n",
        "bert_config = BertConfig(BERT_MODEL_PATH+'config.json')\n",
        "\n",
        "os.listdir(\"./\") # ['bert_config.json', 'pytorch_bert_model.bin']"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "../gdrive/My Drive/data/bert-base-multilingual-cased/\n",
            "Building PyTorch model from configuration: {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.modeling_bert:Converting TensorFlow checkpoint from /gdrive/My Drive/data/bert-base-multilingual-cased/bert_model.ckpt\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings with shape [119547, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/output_bias with shape [119547]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
            "INFO:transformers.modeling_bert:Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Save PyTorch model to ../gdrive/My Drive/data/bert-base-multilingual-cased/pytorch_model.bin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QScZ6eLIHP6m",
        "colab_type": "text"
      },
      "source": [
        "### BERT 사용 관련 Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q7cEtBQ9Aab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = MAX_LEN # token분포 바탕으로 128 선정 (대부분의 단어 길이 cover)\n",
        "SEED = 1234 # random seed\n",
        "# NUM_TO_LOAD = len(df_ep_sample) * 0.8 # 전체 데이터셋의 80%를 Train Set으로 활용\n",
        "EPOCHS = 3\n",
        "OUTPUT_MODEL_FILE = \"../gdrive/My Drive/data/pytorch_bert_media_project_result_model.bin\"\n",
        "\n",
        "# print(NUM_TO_LOAD)\n",
        "\n",
        "# reproductivity를 위한 seed 고정\n",
        "lr=2e-5\n",
        "batch_size = 32\n",
        "training_epochs = 3 # Number of training epochs (authors recommend between 2 and 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8YnThWhl6HW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 기본 Text를 BertTokenizer를 이용하여 Tokenizing\n",
        "\n",
        "# # aws s3의 bert-base-multilingual-uncased-vocab.txt 파일을 불러와 tokenization 수행\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "\n",
        "# # Make sure all comment_text values are strings\n",
        "# df_ep_sample['contents'] = df_ep_sample['contents'].astype(str) \n",
        "\n",
        "# # train_df의 \"comment_text\"에서 na를 \"DUMMY_VALUE\"로 채우고, 최대 MAX_SEQUENCE_LENGTH 만큼 잘라냄\n",
        "# sequences = convert_bert_token(df_ep_sample[\"contents\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH, tokenizer)\n",
        "\n",
        "# print(sequences.shape)\n",
        "# print(df_ep_sample['contents'][:2])\n",
        "# display(sequences[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSKjJZl_IzLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the lines to BERT format # do token-convert-to-ids\n",
        "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
        "def convert_bert_token(example, max_seq_length,tokenizer):\n",
        "    max_seq_length -=2\n",
        "    all_tokens = []\n",
        "    longer = 0\n",
        "    for text in tqdm_notebook(example):\n",
        "        # print(\"text :\", text)\n",
        "        tokens_a = tokenizer.tokenize(text)\n",
        "        print(\"tokens_a : \", tokens_a)\n",
        "        if len(tokens_a)>max_seq_length:  #token의 길이가 max_seq_length보다 길면 max_seq_length 뒤로는 잘라내고, longer 변수를 1증가 시킴\n",
        "            tokens_a = tokens_a[:max_seq_length]\n",
        "            longer += 1\n",
        "        # token의 앞 뒤에 [CLS]와 [SEP]을 추가 시키고 남는자리는 zero padding\n",
        "        # print(\"max_seq_length: \", max_seq_length, \"len(tokens_a): \", len(tokens_a), \"max_seq_length - len(tokens_a) : \", max_seq_length - len(tokens_a))\n",
        "\n",
        "        # \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+ [0] * int(max_seq_length - len(tokens_a)) # token을 vocab을 이용하여 id로 convert\n",
        "        all_tokens.append(one_token) # all_tokens에 추가\n",
        "    # print(longer)\n",
        "    return np.array(all_tokens)\n",
        "\n",
        "def create_attention_masks(sequence):\n",
        "# Create attention masks\n",
        "  attention_masks = []\n",
        "\n",
        "  # Create a mask of 1s for each token followed by 0s for padding\n",
        "  for seq in sequence:\n",
        "    # print(\"seq is \", seq)\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "  # print(attention_masks)\n",
        "  return attention_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGQSs3C29ki",
        "colab_type": "code",
        "outputId": "883b8a8a-fc29-4dc4-bdf4-1d6864cb1446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "741d72d3c2bd469c850d9b5a78e93b28",
            "b00d3afbf41d463b8f7ac7a0ce555885",
            "cfa82dd712474e72ac67866dc0f6a6cf",
            "7b5cb9da93dd4a73a5fecc0ce34376c8",
            "2c5632f83e8544239b93d98aad57afef",
            "042642f1a72c4762880c78dc63a52b00",
            "494f2aa8591e46a280937ead9b093e37",
            "2266487323844963ae5d437fe36183a9"
          ]
        }
      },
      "source": [
        "# OKT로 Tokenize 한 데이터를 string으로 붙인 뒤 이를 다시 bert 형태로 tokenizing\n",
        "\n",
        "%%time\n",
        "# aws s3의 bert-base-multilingual-uncased-vocab.txt 파일을 불러와 tokenization 수행\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "# text : 옷 입고 목욕탕 들어가는거 웃기 넼 저 거 명품 일 텐데\n",
        "# tokens_a :  ['ᄋ', '##ᅩ', '##ᆺ', '이', '##ᆸ고', 'ᄆ', '##ᅩᆨ', '##요', '##ᆨ', '##타', '##ᆼ', '들어', '##가는', '##거', 'ᄋ', '##ᅮ', '##ᆺ', '##기', '네', '##ᆿ', 'ᄌ', '##ᅥ', 'ᄀ', '##ᅥ', 'ᄆ', '##ᅧᆼ', '##품', '이', '##ᆯ', 'ᄐ', '##ᅦᆫ', '##데']\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained(os.path.join(cased, vocab), do_lower_case=True, do_basic_tokenize=False)\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True, do_basic_tokenize=False)\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False, do_basic_tokenize=False)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
        "# tokens_a :  ['옷', '입', '##고', '목', '##욕', '##탕', '들어', '##가는', '##거', '웃', '##기', '[UNK]', '저', '거', '명', '##품', '일', '텐', '##데']\n",
        "# tokens_a :  ['[UNK]', '선', '황', '##제', '가', '바', '##람', '은', '못', '[UNK]', '같다', '##는', '생', '##각', '이', '든', '##다', '태', '##후', '가', '황', '##후', '로', '있는', '##한', '감', '##히', '생', '##각', '도', '못', '했', '##을', '##듯']\n",
        "\n",
        "# Make sure all comment_text values are strings\n",
        "# df_ep_sample['kkma_token_str'] = df_ep_sample['kkma_token'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# train_df의 \"comment_text\"에서 na를 \"DUMMY_VALUE\"로 채우고, 최대 MAX_SEQUENCE_LENGTH 만큼 잘라냄\n",
        "sequences = convert_bert_token(df_ep_sample[\"okt_token_str\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH, tokenizer)\n",
        "attention_masks = np.asarray(create_attention_masks(sequences))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp7d6jqgjr\n",
            "100%|██████████| 995526/995526 [00:00<00:00, 5702862.10B/s]\n",
            "INFO:transformers.file_utils:copying /tmp/tmp7d6jqgjr to cache at /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "INFO:transformers.file_utils:removing temp file /tmp/tmp7d6jqgjr\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "741d72d3c2bd469c850d9b5a78e93b28",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tokens_a :  ['옷', '입', '##고', '목', '##욕', '##탕', '들어', '##가는', '##거', '웃', '##기', '[UNK]', '저', '거', '명', '##품', '일', '텐', '##데']\n",
            "tokens_a :  ['[UNK]', '선', '황', '##제', '가', '바', '##람', '은', '못', '[UNK]', '같다', '##는', '생', '##각', '이', '든', '##다', '태', '##후', '가', '황', '##후', '로', '있는', '##한', '감', '##히', '생', '##각', '도', '못', '했', '##을', '##듯']\n",
            "tokens_a :  []\n",
            "tokens_a :  []\n",
            "tokens_a :  ['이', '게', '나', '##라', '냐', '방송', '에서', '이', '##딴', '수', '##위', '가', '나', '##오', '##고', '지', '##랄', '이', '##야', '진', '##짜', '개', '[UNK]', '조선', '시', '##발']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '님', '진', '##짜', '예', '##쁘', '##다']\n",
            "tokens_a :  ['오', '##늘', '은', '이', '거', '다']\n",
            "tokens_a :  ['에', '##쓰', '엠', '드', '##립', '봐']\n",
            "tokens_a :  ['세', '인', '##데', '저', '정도', '는', '나', '##와', '##줘', '##야', '##지', '댓', '##글', '보고', '졸', '##라', '야', '##한', '##줄', '알', '##았', '##더', '##니', '##만', '미', '##드', '세', '보다', '못', '하', '##네', '청', '##소', '##년', '이야기', '도', '아', '##니', '##고', '나', '##이', '든', '성', '인', '##들', '이', '야', '긴', '##데', '저', '##런', '##거', '안', '나', '##오는', '##게', '이상', '##한', '##거', '아', '##님', '그', '##냥', '드라마', '붙', '##히', '##고', '방송', '해', '##줘', '##라', '제', '##발']\n",
            "tokens_a :  ['토', '##나', '##와']\n",
            "tokens_a :  ['페', '##하', '소', '##리', '를', '몇', '번', '하는', '##거', '##지']\n",
            "tokens_a :  ['역시', '막', '##장', '##드라마', '전', '##문', '작', '##가', '는', '미', '##쳤', '##네', '수', '##위', '도', '높', '##아']\n",
            "tokens_a :  ['드라마', '수', '##준', '하고', '##는', '이런', '막', '##장', '##드라마', '진', '##짜', '질', '##린다', '이런', '막', '##장', '아', '##님', '드라마', '못', '쓰', '##나', '봐', '##요', '진', '##짜', '수', '##준', '하고', '##는']\n",
            "tokens_a :  ['아', '##니', '옷', '거', '##품', '목', '[UNK]']\n",
            "tokens_a :  ['미', '##친', '##거', '##아', '##니', '##냐', '이런', '걸', '내', '##보', '##내', '##다', '##니']\n",
            "tokens_a :  ['솔', '##직', '##히', '왕', '이', '가지', '고', '노', '##는', '##거', '##지', '진', '##심', '으로', '사', '##랑', '하', '##면', '[UNK]', '냐']\n",
            "tokens_a :  ['이', '##래', '##서', '이', '##랑', '에서', '밀', '##려', '났', '##구', '##나', '너', '##무', '야', '##하고', '너', '##무', '오', '##글', '##거', '##림', '##에', '연', '##기', '도', '조', '##금', '부', '##자', '##연', '##스', '##럽', '##고', '옛', '##날', '공', '##중', '파', '##였', '##음', '이런', '##거', '못', '했', '##을', '##텐', '##데', '이', '젠', '시', '##청', '##률', '도', '안', '나', '##오', '##고', '그', '##러', '##니', '이', '젠', '아', '##무', '장', '##르', '막', '하는', '##구', '##나']\n",
            "tokens_a :  ['미', '##친']\n",
            "tokens_a :  ['나', '만', '이어', '##폰', '다', '들', '##리', '##나', '##요']\n",
            "tokens_a :  ['맞', '##죠']\n",
            "tokens_a :  ['옷', '입', '##고', '목', '##욕', '##탕', '들어', '##가는', '##거', '웃', '##기', '[UNK]', '저', '거', '명', '##품', '일', '텐', '##데']\n",
            "tokens_a :  ['[UNK]', '선', '황', '##제', '가', '바', '##람', '은', '못', '[UNK]', '같다', '##는', '생', '##각', '이', '든', '##다', '태', '##후', '가', '황', '##후', '로', '있는', '##한', '감', '##히', '생', '##각', '도', '못', '했', '##을', '##듯']\n",
            "tokens_a :  []\n",
            "tokens_a :  []\n",
            "tokens_a :  ['이', '게', '나', '##라', '냐', '방송', '에서', '이', '##딴', '수', '##위', '가', '나', '##오', '##고', '지', '##랄', '이', '##야', '진', '##짜', '개', '[UNK]', '조선', '시', '##발']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '님', '진', '##짜', '예', '##쁘', '##다']\n",
            "tokens_a :  ['오', '##늘', '은', '이', '거', '다']\n",
            "tokens_a :  ['에', '##쓰', '엠', '드', '##립', '봐']\n",
            "tokens_a :  ['세', '인', '##데', '저', '정도', '는', '나', '##와', '##줘', '##야', '##지', '댓', '##글', '보고', '졸', '##라', '야', '##한', '##줄', '알', '##았', '##더', '##니', '##만', '미', '##드', '세', '보다', '못', '하', '##네', '청', '##소', '##년', '이야기', '도', '아', '##니', '##고', '나', '##이', '든', '성', '인', '##들', '이', '야', '긴', '##데', '저', '##런', '##거', '안', '나', '##오는', '##게', '이상', '##한', '##거', '아', '##님', '그', '##냥', '드라마', '붙', '##히', '##고', '방송', '해', '##줘', '##라', '제', '##발']\n",
            "tokens_a :  ['토', '##나', '##와']\n",
            "tokens_a :  ['페', '##하', '소', '##리', '를', '몇', '번', '하는', '##거', '##지']\n",
            "tokens_a :  ['역시', '막', '##장', '##드라마', '전', '##문', '작', '##가', '는', '미', '##쳤', '##네', '수', '##위', '도', '높', '##아']\n",
            "tokens_a :  ['드라마', '수', '##준', '하고', '##는', '이런', '막', '##장', '##드라마', '진', '##짜', '질', '##린다', '이런', '막', '##장', '아', '##님', '드라마', '못', '쓰', '##나', '봐', '##요', '진', '##짜', '수', '##준', '하고', '##는']\n",
            "tokens_a :  ['아', '##니', '옷', '거', '##품', '목', '[UNK]']\n",
            "tokens_a :  ['미', '##친', '##거', '##아', '##니', '##냐', '이런', '걸', '내', '##보', '##내', '##다', '##니']\n",
            "tokens_a :  ['솔', '##직', '##히', '왕', '이', '가지', '고', '노', '##는', '##거', '##지', '진', '##심', '으로', '사', '##랑', '하', '##면', '[UNK]', '냐']\n",
            "tokens_a :  ['이', '##래', '##서', '이', '##랑', '에서', '밀', '##려', '났', '##구', '##나', '너', '##무', '야', '##하고', '너', '##무', '오', '##글', '##거', '##림', '##에', '연', '##기', '도', '조', '##금', '부', '##자', '##연', '##스', '##럽', '##고', '옛', '##날', '공', '##중', '파', '##였', '##음', '이런', '##거', '못', '했', '##을', '##텐', '##데', '이', '젠', '시', '##청', '##률', '도', '안', '나', '##오', '##고', '그', '##러', '##니', '이', '젠', '아', '##무', '장', '##르', '막', '하는', '##구', '##나']\n",
            "tokens_a :  ['미', '##친']\n",
            "tokens_a :  ['나', '만', '이어', '##폰', '다', '들', '##리', '##나', '##요']\n",
            "tokens_a :  ['맞', '##죠']\n",
            "tokens_a :  ['신', '##은', '##경', '의', '사', '##생', '##활', '[UNK]', '에', '거', '##북', '##하', '##긴', '함', '초', '##반', '에', '너', '##무', '야', '##함', '그', '##래', '##서', '채', '##널', '돌', '##렸', '##음']\n",
            "tokens_a :  ['신', '##은', '##경', '이', '가', '연', '##기', '는', '잘', '해']\n",
            "tokens_a :  ['이', '거', '보고', '검', '##색', '했', '##는데', '신', '##성', '##록', '이', '남', '주', '##가', '아니라', '놀', '##랬', '##음']\n",
            "tokens_a :  ['화', '를', '안', '봐', '##서', '그', '##런', '##데', '뭔', '드라마', '에', '##요', '현대', '극', '같은', '##데', '말', '하는', '##거', '##는', '사', '##극', '같', '##고', '뭐', '지', '[UNK]', '만든', '##거', '##지', '이런', '장', '##면', '은', '왜', '있는', '##거', '##지']\n",
            "tokens_a :  ['쓰', '##레', '##기', '드라마', '좀', '내', '##보', '##내', '##지', '##마', '##라', '애', '들', '이', '뭐', '보고', '배우', '##냐']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '법', '##원', '서', '기', '관', '##할', '때', '진', '##짜', '좋', '##앗', '##는데']\n",
            "tokens_a :  ['저', '엄', '##마', '역', '배우', '##는', '장', '##보', '##리', '때', '도', '저', '##렇게', '딸', '한', '##테', '버', '##림', '받', '##더', '##니']\n",
            "tokens_a :  ['미', '##친', '목', '##욕', '##탕', '[UNK]']\n",
            "tokens_a :  ['볼', '##때', '마', '##다', '이', '##태', '##임', '생', '##각', '##나', '##네']\n",
            "tokens_a :  ['[UNK]', '스', '##트', '##레스', '풀', '##린', '[UNK]']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '가', '어머니', '죽', '##임', '아', '##무', '##리', '그', '##래', '##도', '엄', '##마', '를']\n",
            "tokens_a :  ['아', '뭔', '데', '왜', '영', '##상', '지', '##움']\n",
            "tokens_a :  ['앞', '영', '##상', '이', '##랑', '같은', '##데']\n",
            "tokens_a :  ['이', '거', '보고', '채', '##널', '돌', '##림']\n",
            "tokens_a :  ['언', '##니', '가', '살', '##아', '##있다', '작', '##가', '##라', '이', '##렇', '##구', '##나', '무', '##섭']\n",
            "tokens_a :  ['엄', '##마', '랑', '같이', '봤', '##는데', '진', '##짜', '민', '##망', '##해', '죽', '을', '뻔']\n",
            "tokens_a :  ['연', '##기', '##력', '좋은', '배우', '들', '섭', '##외', '해', '두', '곤', '왜', '이런', '드라마', '를']\n",
            "tokens_a :  ['그', '##래', '##서', '신', '##음', '은', '몇', '초', '죠']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '사', '##랑']\n",
            "tokens_a :  ['아', '이', '##엘', '이', '##쁘', '##다']\n",
            "tokens_a :  ['왜', '영', '##상', '바', '꼈', '##지']\n",
            "tokens_a :  ['엘', '##리', '##야', '넘', '##나', '섹', '##시', '##하', '##네']\n",
            "tokens_a :  ['뭐', '야', '앞', '영', '##상', '과', '똑', '##같', '##은', '영', '##상', '이', '[UNK]', '[UNK]']\n",
            "tokens_a :  ['미', '##스', '함', '##무라', '##비', '에서', '속', '기', '사', '역', '##할', '나', '##올', '때', '좋', '##앗', '##는데']\n",
            "tokens_a :  ['공', '##중', '##파', '클', '##라', '##스']\n",
            "tokens_a :  ['[UNK]']\n",
            "tokens_a :  ['신', '##은', '##경', '애', '는', '보', '##나']\n",
            "tokens_a :  ['아들', '빨', '##리', '자', '##라고', '막', '소', '##리', '질', '##렀', '##음', '선', '##정', '적', '이라']\n",
            "tokens_a :  ['헐', '방', '##통', '##위', '뭐', '하', '##냐', '일', '안', '하', '##냐']\n",
            "tokens_a :  ['최', '##진', '##혁', '오', '##직', '최', '배우', '보', '##기', '위해', '기', '##다', '##립', '##니다']\n",
            "tokens_a :  ['정', '##녕', '이', '드라마', '가', '재', '##미', '##나', '나']\n",
            "tokens_a :  ['신', '##은', '##경', '을', '쓰', '##네', '미', '##친', '공', '##중', '##파', '저', '##러', '##니', '처', '욕', '##먹', '##지']\n",
            "tokens_a :  ['신', '##은', '##경', '차', '##암', '나', '년', '동안', '억', '쓴', '년', '나', '##오', '##네']\n",
            "tokens_a :  ['삼', '##촌', '이', '##랑', '보다', '##가', '저', '게', '세', '맞', '##냐', '##고', '몇', '##번', '이', '##나', '질', '##문', '받았다', '무', '##슨', '드라마', '가', '거', '##기', '다', '머', '##리', '에', '총', '맞', '##고', '물', '에', '빠', '##졌', '##는데', '완', '##벽', '##해', '##져', '##서', '나', '##타', '##남', '목', '조', '##르', '##다가', '책', '##임', '##질', '여자', '라', '##니', '진', '##짜', '별']\n",
            "tokens_a :  ['허', '글', '공', '##중', '##파', '에서', '이런', '걸', '시', '##청', '##률', '에', '다', '내', '##놓', '##는', '##구', '##나', '아', '##예', '나', '##체', '로', '[UNK]', '지', '그', '##러', '##셨', '##나', '한', '심', '허', '##다']\n",
            "tokens_a :  ['제', '##목', '이', '##랑', '영', '##상', '이', '##랑', '관련', '##성', '이', '도', '없는', '##데']\n",
            "tokens_a :  ['어', '##디', '##가']\n",
            "tokens_a :  ['[UNK]', '다른', '##거', '나', '##오는', '디', '요']\n",
            "tokens_a :  ['난', '왜', '안', '##보', '이', '##지']\n",
            "tokens_a :  ['[UNK]']\n",
            "tokens_a :  ['여', '##기', '관', '##리', '##자', '채', '##널', '관', '##리', '잘', '좀', '해', '##주', '##세', '##요']\n",
            "tokens_a :  ['뭐', '야', '저', '##만', '전', '영', '##상', '이', '##랑', '똑', '##같', '##이', '보', '##여', '##요']\n",
            "tokens_a :  ['아', '##직', '##도', '전', '영', '##상', '보', '##이는', '##거', '저', '##만', '그', '##런', '가', '##요']\n",
            "tokens_a :  ['욕', '하', '##면서', '##도', '다', '들', '보', '##는', '[UNK]']\n",
            "tokens_a :  ['민', '##망', '##했다']\n",
            "tokens_a :  ['저', '는', '딴', '장', '##면', '보', '##이는', '대', '##여']\n",
            "tokens_a :  ['앗', '다른', '영', '##상', '이', '[UNK]', '훼', '이', '##크', '지', '##리', '네', '이런']\n",
            "tokens_a :  ['욕', '밖', '##에', '안', '나', '##온', '##다', '아', '우', '##리', '들', '을', '얼', '##마', '##나', '우', '##습', '##게', '보', '##면', '아', '열', '받아', '작', '##가', '연', '출', '##진', '진', '##짜', '어', '##후', '아', '##아', '아']\n",
            "tokens_a :  ['와', '저', '##런', '폐', '##륜', '##적', '인', '신', '##은', '##경', '을', '쓰', '##는', '작', '##가', '와', '피', '##디', '는', '몰', '##까']\n",
            "tokens_a :  ['아', '##니', '왜', '뒤', '에', '부', '##분', '이', '잘', '##린', '##거', '##죠']\n",
            "tokens_a :  ['[UNK]', '왜', '전', '꺼', '하고', '같은', '##게', '나', '##오', '##지']\n",
            "tokens_a :  ['저', '##만', '전', '영', '##상', '이', '##랑', '같은', '##게', '나', '##오', '##나', '##요']\n",
            "tokens_a :  ['군', '##산', '지역', '앞', '##바', '##다', '에', '유', '##전', '이', '있다고', '합', '##니다', '유', '##전', '개발', '하', '여', '군', '##산', '경', '##제', '살', '##려', '##요', '청', '##원', '중', '입', '##니다', '한', '표', '부', '##탁', '##드', '##려', '##요', '한', '##번', '의', '관', '##심', '과', '한', '표', '가', '국가', '발', '##전', '에', '##도', '큰', '도', '##움', '[UNK]', '청', '##와', '##대', '청', '##원', '링', '##크', '입', '##니다']\n",
            "tokens_a :  ['음', '보', '##면서', '그', '##리', '기', '##분', '좋', '##진', '않', '##음', '흑', '흑']\n",
            "tokens_a :  ['분', '##위', '##기', '는', '참', '좋은', '##데', '신', '##성', '##록', '도', '좋은', '##데', '스', '##토', '##리', '가', '무', '##슨', '갑', '##자', '##기', '[UNK]']\n",
            "tokens_a :  ['개', '웃', '##긴', '##데']\n",
            "tokens_a :  ['년', '드라마', '라', '##니', '구', '##리', '다']\n",
            "tokens_a :  ['신', '##은', '##경', '나', '##오', '##네', '헐']\n",
            "tokens_a :  ['주', '동', '##민', '은', '꼭', '저', '##런', '선', '##정', '적', '인', '장', '##면', '을', '넣', '##더', '##라', '별', '##로', '필', '##요', '##하지', '##도', '않은', '장', '##면', '같', '##고', '굳', '##이', '저', '##렇게', '연', '##출', '안', '##해', '도', '됐', '##을', '##텐', '##데', '김', '##순', '##옥', '작', '##가', '가', '원래', '막', '##장', '드라마', '쓰', '##긴', '하지만', '이', '##따', '##위', '저', '질', '장', '##면', '을', '넣', '##은', '##건', '피', '##디', '영', '##향', '이', '크', '##지', '[UNK]']\n",
            "tokens_a :  ['가', '##만', '봐', '##도', '[UNK]', '자', '##세', '##히', '봐', '##도', '[UNK]']\n",
            "tokens_a :  ['신', '##은', '##경', '오', '##랜', '만', '에', '나', '##오', '##셔', '##서', '좋', '##아', '##요']\n",
            "tokens_a :  ['지', '##금', '은', '회', '##사', '니', '퇴', '##근', '해', '##서', '봐', '##야', '##지']\n",
            "tokens_a :  ['혹', '##시', '최', '##진', '##혁', '이', '[UNK]', '[UNK]']\n",
            "tokens_a :  ['구', '##성', '도', '엉', '성', '##하고', '스', '##토', '##리', '전', '##개', '##도', '진', '##부', '##하고', '무', '##슨', '드라마', '가', '[UNK]', '야', '출', '##연', '##자', '도', '짜', '##증', '##나', '##겠', '##다']\n",
            "tokens_a :  ['신', '##성', '##록', '물', '에', '빠', '##진', '머', '##리', '스', '##탈', '엽', '##기', '적', '이', '##고', '퇴', '##폐', '적', '이', '##고']\n",
            "tokens_a :  ['젖', '##었', '##겠', '##는데']\n",
            "tokens_a :  ['드라마', '금', '인', '##가', '##요']\n",
            "tokens_a :  ['신', '##은', '##경']\n",
            "tokens_a :  ['[UNK]', '어', '##제', '본', '장', '##면', '이', '##군', '요', '이', '##엘', '##리', '##야', '씨', '너', '##무', '잘', '참', '##던', '##데', '요', '##전', '간', '##지', '##러', '##워', '##서', '못', '참', '##겠', '##던', '##데', '좋은', '연', '##기', '기', '##대', '##할', '##게', '##요']\n",
            "tokens_a :  ['신', '##은', '##경', '이', '나', '##온', '##다고', '미', '##쳤', '##네']\n",
            "tokens_a :  ['[UNK]', '없다']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '목', '빨', '##간', '##거', '봐', '개', '##야', '##해']\n",
            "tokens_a :  ['미', '##친', '목', '에', '키', '##스', '하는', '##거', '가', '##까', '##이', '도', '잡', '##네']\n",
            "tokens_a :  ['난', '이', '거', '[UNK]']\n",
            "tokens_a :  ['이런', '##게', '놀', '##라', '##운', '##게', '아니라', '신', '##은', '##경', '다시', '드라마', '를', '찍', '##고', '있다는', '##게', '놀', '##라', '##운', '##듯']\n",
            "tokens_a :  ['신', '##은', '##경', '나', '##오', '##자', '마', '##자', '다른', '채', '##널', '돌', '##려', '버', '##림']\n",
            "tokens_a :  ['이', '드라마', '볼', '##려', '##고', '했', '##는데', '신', '##은', '##경', '나', '##오', '##더', '##라', '그', '##래', '##서', '안', '본', '##다', '피', '##디', '도', '대', '##단', '##하', '##네', '섭', '##외', '할', '생', '##각', '을', '다', '하고', '기', '##획', '##사', '에서', '[UNK]', '않', '##앗', '##나', '다른', '기', '##획', '##사', '들어', '##간', '##거']\n",
            "tokens_a :  ['아', '##무', '##리', '봐', '##도', '넘', '[UNK]', '해', '##골', '같', '##아']\n",
            "tokens_a :  ['이', '허', '##언', '##증', '환', '##자', '기', '##어', '나', '##왔', '##네']\n",
            "tokens_a :  ['약', '##간', '철', '##구', '닮', '##은', '##듯']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '되', '##게', '섹', '##시', '##하', '##네']\n",
            "tokens_a :  ['제', '##목', '만', '고', '##급', '지', '네', '내', '##용', '은', '허', '술', '하고', '포', '##장', '은', '명', '##품', '처', '##럼', '해', '##놓', '##고', '열', '어', '##보', '니', '[UNK]', '이', '##네', '막', '##장', '의', '품', '##격']\n",
            "tokens_a :  ['수', '##위', '좀', '##더', '[UNK]', '해', '##주', '##세', '##요']\n",
            "tokens_a :  ['잘', '못', '본', '##줄', '신', '##은', '##경', '도', '돈', '문', '##제', '있', '##지', '않', '##나', '어', '##찌', '나', '##오', '##냐']\n",
            "tokens_a :  ['개', '##막', '장', '##이', '네']\n",
            "tokens_a :  ['자', '##식', '버', '##린', '마', '##녀', '신', '##은', '##경', '이', '드라마', '에', '또', '나', '##오', '##네', '어', '##이', '##없', '##다']\n",
            "tokens_a :  ['폐', '##하', '뭐', '야', '궁', '이', '##야']\n",
            "tokens_a :  ['이런', '드라마', '도', '있어', '##나']\n",
            "tokens_a :  ['근', '디', '신', '##은', '##경', '은', '왜', '나', '##오는', '##거', '##에', '##요']\n",
            "tokens_a :  ['느', '##끼', '##한', '##데']\n",
            "tokens_a :  ['너', '##무', '처음', '부', '##터', '잔', '##인', '##함']\n",
            "tokens_a :  ['너', '##무', '야', '##해서', '깜', '##짝', '이', '##야']\n",
            "tokens_a :  ['드라마', '초', '##반', '부', '##터', '막', '##장', '에', '##어', '이', '가', '없', '##네']\n",
            "tokens_a :  ['폐', '##하']\n",
            "tokens_a :  ['연', '##출', '진', '##짜', '어', '##이', '##없', '##네']\n",
            "tokens_a :  ['신', '##은', '##경', '이', '여', '##기', '왜', '나', '##오', '##냐']\n",
            "tokens_a :  ['채', '##널', '몇', '##번', '을', '돌', '##렸다', '자', '##극', '적', '인', '장', '##면', '이', '많', '##음', '그', '##래', '##서', '안', '볼', '##려', '구', '요']\n",
            "tokens_a :  ['퇴', '##폐', '적']\n",
            "tokens_a :  ['머', '야', '신', '##은', '##경', '방송', '금', '##지', '아', '##냐']\n",
            "tokens_a :  ['민', '##망', '##하', '##더', '##라', '저', '##런', '게', '방송', '에', '나', '와', '##도', '됨', '했', '##음']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '되', '##게', '섹', '##시', '##하', '##네']\n",
            "tokens_a :  ['제', '##목', '만', '고', '##급', '지', '네', '내', '##용', '은', '허', '술', '하고', '포', '##장', '은', '명', '##품', '처', '##럼', '해', '##놓', '##고', '열', '어', '##보', '니', '[UNK]', '이', '##네', '막', '##장', '의', '품', '##격']\n",
            "tokens_a :  ['수', '##위', '좀', '##더', '[UNK]', '해', '##주', '##세', '##요']\n",
            "tokens_a :  ['잘', '못', '본', '##줄', '신', '##은', '##경', '도', '돈', '문', '##제', '있', '##지', '않', '##나', '어', '##찌', '나', '##오', '##냐']\n",
            "tokens_a :  ['개', '##막', '장', '##이', '네']\n",
            "tokens_a :  ['자', '##식', '버', '##린', '마', '##녀', '신', '##은', '##경', '이', '드라마', '에', '또', '나', '##오', '##네', '어', '##이', '##없', '##다']\n",
            "tokens_a :  ['폐', '##하', '뭐', '야', '궁', '이', '##야']\n",
            "tokens_a :  ['이런', '드라마', '도', '있어', '##나']\n",
            "tokens_a :  ['근', '디', '신', '##은', '##경', '은', '왜', '나', '##오는', '##거', '##에', '##요']\n",
            "tokens_a :  ['느', '##끼', '##한', '##데']\n",
            "tokens_a :  ['너', '##무', '처음', '부', '##터', '잔', '##인', '##함']\n",
            "tokens_a :  ['너', '##무', '야', '##해서', '깜', '##짝', '이', '##야']\n",
            "tokens_a :  ['드라마', '초', '##반', '부', '##터', '막', '##장', '에', '##어', '이', '가', '없', '##네']\n",
            "tokens_a :  ['폐', '##하']\n",
            "tokens_a :  ['연', '##출', '진', '##짜', '어', '##이', '##없', '##네']\n",
            "tokens_a :  ['신', '##은', '##경', '이', '여', '##기', '왜', '나', '##오', '##냐']\n",
            "tokens_a :  ['채', '##널', '몇', '##번', '을', '돌', '##렸다', '자', '##극', '적', '인', '장', '##면', '이', '많', '##음', '그', '##래', '##서', '안', '볼', '##려', '구', '요']\n",
            "tokens_a :  ['퇴', '##폐', '적']\n",
            "tokens_a :  ['머', '야', '신', '##은', '##경', '방송', '금', '##지', '아', '##냐']\n",
            "tokens_a :  ['민', '##망', '##하', '##더', '##라', '저', '##런', '게', '방송', '에', '나', '와', '##도', '됨', '했', '##음']\n",
            "tokens_a :  ['아', '##주', '야', '##동', '을', '내', '##보', '##내', '##라']\n",
            "tokens_a :  ['임', '##성', '##한', '작', '##가', '는', '쓰', '##면서', '##도', '이', '##건', '아', '##니', '##지', '너', '##무', '심', '##하지', '했', '##지만', '만', '##에', '하', '##나', '상', '##대', '드라마', '가', '마지막', '회', '프', '##리', '##미', '##엄', '을', '얻', '##어', '프로', '라', '##도', '우', '##리', '드라마', '를', '앞', '##서', '##면', '그', '드라마', '는', '결국', '우', '##리', '드라마', '를', '이', '##기', '고', '끝', '##난', '게', '되', '##기', '때', '##문', '에', '그', '##동안', '의', '내', '고', '##생', '과', '노', '##력', '이', '수', '##포', '로', '돌', '##아', '##갈', '수', '밖', '##에', '없', '##어', '눈', '##물', '을', '머', '금', '##고', '에', '##피', '##소', '##드', '를', '동', '##원', '했다', '##고', '밝혔다', '임', '작', '##가', '는', '자', '##신', '의', '이', '같은', '행', '##동', '을', '악', '##행', '이라', '표', '##현', '하', '##기도', '했다', '김', '##순', '##옥', '도', '임', '##성', '##한', '드', '##작', '은', '##퇴', '하고', '털', '##어', '##놓', '##는', '이야기', '시', '##청', '##률', '위해', '독', '##약', '풀', '##라', '고', '방송', '##국', '에서', '전', '##화', '오', '##면', '사', '##람', '죽', '이', '##고', '사', '##고', '##내', '##고', '시', '##청', '##률', '이', '진', '##정', '다', '##냐']\n",
            "tokens_a :  ['아', '진', '##짜', '개', '오', '##글', '##거', '##리고', '진', '##부', '##하고', '이런', '씬', '##좀', '넣', '##지', '마', '##쇼']\n",
            "tokens_a :  ['보', '##는', '청', '##소', '##년', '들', '도', '있고', '뭐', '하는', '거', '##야']\n",
            "tokens_a :  ['아', '##니', '목', '조', '##르', '##다가', '급', '좋', '##아', '##하', '##냐']\n",
            "tokens_a :  ['소', '##리', '가', '잘', '안', '들', '##리는', '##건', '개', '##선', '해', '##주', '##세', '##요']\n",
            "tokens_a :  ['괜', '##찮', '##나']\n",
            "tokens_a :  ['화', '는', '소', '##리', '가', '안', '나', '##와', '##서', '못', '봤', '##어', '##요']\n",
            "tokens_a :  ['근', '##데', '인', '##간', '적', '으로', '이', '##엘', '##리', '##야', '너', '##무', '예', '##쁘', '##다', '이', '드라마', '좀', '뜨', '##면', '새로운', '악', '##녀', '로', '이', '##름', '날', '##릴', '##듯']\n",
            "tokens_a :  ['별', '##거', '아', '##니', '##구', '##만']\n",
            "tokens_a :  ['이어', '##폰', '끼', '고', '듣', '##는데', '나', '만', '한', '쪽', '만', '들', '##리', '##나']\n",
            "tokens_a :  ['아', '##빠', '랑', '보', '##신', '분', '[UNK]', '진', '##짜', '어', '##색', '##했', '##겠', '##다', '난', '이', '##해', '하', '지', '##롱', '예', '##전', '다른', '드라마', '보다', '아', '##빠', '랑', '와', '시', '##간', '이', '빨', '##리', '초', '##가', '분', '##인', '##줄', '그', '##래', '##도', '[UNK]', '황', '##후']\n",
            "tokens_a :  ['버', '##퍼', '##링', '장', '##난', '아', '##님', '접', '##속', '자', '가', '몇', '명', '##인', '거', '야']\n",
            "tokens_a :  ['아', '##빠', '랑', '보다', '##가']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '눈', '##빛', '장', '##난', '아', '##니다']\n",
            "tokens_a :  ['이', '거', '찍', '##고', '컷', '하고', '##나', '##서', '이', '##엘', '##리', '##야', '랑', '신', '##성', '##록', '되', '##게', '어', '##색', '##했', '##을', '##듯']\n",
            "tokens_a :  ['요', '##새', '드라마', '왜', '이', '##렇게', '자', '##극', '적', '이', '##냐', '이', '젠', '진', '##짜', '이런', '##것', '좀', '그', '##만', '보고', '싶', '##네']\n",
            "tokens_a :  ['신', '##은', '##경', '휴', '##대', '##폰', '던', '##진', '##거']\n",
            "tokens_a :  ['와', '##우']\n",
            "tokens_a :  ['뭔', '데', '웃', '##기', '지']\n",
            "tokens_a :  ['와']\n",
            "tokens_a :  ['아', '##니', '왜', '이런', '역', '##할', '로', '##만', '나', '##오', '##냐']\n",
            "tokens_a :  ['분', '##위', '##기', '깜', '##놀']\n",
            "tokens_a :  ['그', '##래', '##서', '오', '##늘', '최', '##진', '##혁', '은', '안', '[UNK]', '낼', '은']\n",
            "tokens_a :  ['[UNK]', '방', '##심', '##하고', '##보다', '가', '남', '##사', '스', '##러', '##웠']\n",
            "tokens_a :  ['어', '##우', '야']\n",
            "tokens_a :  ['에서', '내', '##보', '##냈다', '##니']\n",
            "tokens_a :  ['오', '##우', '##야', '오', '##우', '##야']\n",
            "tokens_a :  ['와']\n",
            "tokens_a :  ['아', '##니', '왜', '이런', '역', '##할', '로', '##만', '나', '##오', '##냐']\n",
            "tokens_a :  ['분', '##위', '##기', '깜', '##놀']\n",
            "tokens_a :  ['그', '##래', '##서', '오', '##늘', '최', '##진', '##혁', '은', '안', '[UNK]', '낼', '은']\n",
            "tokens_a :  ['[UNK]', '방', '##심', '##하고', '##보다', '가', '남', '##사', '스', '##러', '##웠']\n",
            "tokens_a :  ['어', '##우', '야']\n",
            "tokens_a :  ['에서', '내', '##보', '##냈다', '##니']\n",
            "tokens_a :  ['오', '##우', '##야', '오', '##우', '##야']\n",
            "tokens_a :  ['와']\n",
            "tokens_a :  ['아', '##니', '왜', '이런', '역', '##할', '로', '##만', '나', '##오', '##냐']\n",
            "tokens_a :  ['분', '##위', '##기', '깜', '##놀']\n",
            "tokens_a :  ['그', '##래', '##서', '오', '##늘', '최', '##진', '##혁', '은', '안', '[UNK]', '낼', '은']\n",
            "tokens_a :  ['[UNK]', '방', '##심', '##하고', '##보다', '가', '남', '##사', '스', '##러', '##웠']\n",
            "tokens_a :  ['어', '##우', '야']\n",
            "tokens_a :  ['에서', '내', '##보', '##냈다', '##니']\n",
            "tokens_a :  ['오', '##우', '##야', '오', '##우', '##야']\n",
            "tokens_a :  ['황', '##제', '용', '##포', '박', '##제', '합', '##시', '##다', '섹', '##시', '##하다']\n",
            "tokens_a :  ['와', '황', '##제', '멋', '##있다']\n",
            "tokens_a :  ['아들', '이', '엄', '##마', '보다', '주', '##름', '이', '많', '##네']\n",
            "tokens_a :  ['남', '##주', '신', '##성', '##록', '이', '면', '좋', '##겠', '##다']\n",
            "tokens_a :  ['섹', '##시', '##해', '와', '씨', '신', '##성', '##록']\n",
            "tokens_a :  ['황', '##제', '섹', '##시', '##하다', '용', '##포', '입', '##은', '##거', '존', '##멋']\n",
            "tokens_a :  ['무', '##슨', '엄', '##마', '가', '아들', '한', '##테', '꾸', '##벅', '인', '##사', '하고', '##가', '##냐', '헐', '호', '##칭', '도', '페', '##하', '가', '아니라', '황', '##상', '이', '##지', '아', '##무', '##리', '왕', '이라', '##도', '아들', '인', '##데']\n",
            "tokens_a :  ['신', '##성', '##록', '졸', '##라', '잘', '##생', '##김', '용', '##포', '박', '##제', '하', '##자', '그리고', '신', '##성', '##록', '캐', '##릭', '##터', '황', '##제', '불', '##쌍', '##함']\n",
            "tokens_a :  ['신', '##성', '##록', '인', '##생', '캐', '만', '##난', '##듯']\n",
            "tokens_a :  ['딱', '보', '니', '##까', '신', '##성', '##록', '원래', '착', '##한', '캐', '##릭', '인', '##데', '아', '##내', '죽', '##어', '##가지', '##고', '저', '##렇게', '된', '##거', '좀', '지', '##나', '##면', '엄', '##마', '랑', '싸', '##우', '##고', '장', '##나라', '한', '##테', '반', '##하', '##겟', '##지', '하지만', '남', '##주는', '최', '##진', '##혁']\n",
            "tokens_a :  ['와', '나', '웬', '##만', '##하면', '남자', '몸', '보고', '우', '##와', '안', '그', '##러', '##는데', '신', '##성', '##록', '은', '몸', '##매', '진', '##짜', '이', '##쁘', '##다', '여자', '가', '봐', '##도', '부', '##러', '##운', '몸', '우', '오']\n",
            "tokens_a :  ['인', '##정', '합', '##니다']\n",
            "tokens_a :  ['심', '##장', '터', '##질', '뻔']\n",
            "tokens_a :  ['머', '##리', '내', '##린', '##거', '존', '##잘']\n",
            "tokens_a :  ['둘', '##다', '멍', '##청']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '진', '##짜', '이', '##쁘', '##다']\n",
            "tokens_a :  ['토', '##나', '##온', '##다']\n",
            "tokens_a :  []\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '예', '##쁜', '##데']\n",
            "tokens_a :  ['상', '##사', '랑', '비', '##서', '관', '##계', '이', '##딴', '식', '으로', '묘', '##사', '하는', '드라마', '좋', '##다고', '쳐', '보', '##는', '개', '##돼', '##지', '들', '자', '##기', '들', '인', '##권', '추', '##락', '하는', '드라마', '그', '##리', '보고', '##싶', '##냐']\n",
            "tokens_a :  ['저', '질', '류', '드라마']\n",
            "tokens_a :  ['그', '##건', '너', '에', '##게', '달', '##렸', '##지', '오', '글']\n",
            "tokens_a :  ['여성', '비', '##율', '댓', '##글', '창', '더', '##럽', '##네', '그', '분', '##들', '좌', '##표', '[UNK]', '나', '##용']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '한', '##테', '걸', '##어', '##갈', '때', '이', '##야', '멋', '##지', '긴', '##하', '##군']\n",
            "tokens_a :  ['장', '##나라', '랑', '이', '##엘', '##리', '##야', '랑', '각각', '있을', '때', '마', '##다', '넘', '달', '##라', '##지고', '연', '##기', '잘', '하는', '##듯']\n",
            "tokens_a :  ['참', '##한', '드', '##는', '케', '##이', '##블', '은', '발', '##전', '한', '데', '공', '##중', '##파', '는', '계속', '퇴', '보', '##되', '네', '여', '##전', '##히', '시', '##청', '##자', '가', '자', '##극', '적', '인', '것', '좋', '##아', '##하는', '줄', '알', '고', '그', '걸', '로', '시', '##청', '##률', '높', '##이', '##려', '##는', '것', '맨', '##날', '사', '##골', '우', '##리', '듯', '한', '캐', '##릭', '##터', '설', '##정', '스', '##토', '##리', '진', '##심', '제작', '##비', '가', '아', '##깝', '##다', '그', '돈', '으로', '연', '##극', '이', '##나', '다른', '곳', '에', '투', '##자', '하지', '나', '##도', '작', '##가', '지', '##망', '생', '이', '##지만', '여', '##전', '##히', '저', '##런', '것', '보', '##면', '답', '##답', '##함', '대', '##중', '은', '변', '##하는', '##데', '기', '##존', '작', '##가', '는', '여', '##전', '##히', '퇴', '##보', '도', '##전', '하는', '작', '##가', '는', '적', '##은', '것', '같', '##아', '좀', '##더', '다', '##양', '##하게', '시', '##도', '하고', '보', '##여', '##줬', '##으면', '싶', '##음']\n",
            "tokens_a :  ['대', '##사', '가', '류', '인', '##터', '##넷', '소', '##설', '급', '이', '##네']\n",
            "tokens_a :  ['[UNK]', '존', '##나', '오', '##글', '##거', '##리', '##네']\n",
            "tokens_a :  ['물', '고', '빠', '##는', '##것', '##만', '보', '##이는', '##건']\n",
            "tokens_a :  ['오', '이', '##번', '드라마', '에서', '이', '##엘', '##리', '##야', '비', '##중', '크', '##다']\n",
            "tokens_a :  ['와', '나', '웬', '##만', '##하면', '남자', '몸', '보고', '우', '##와', '안', '그', '##러', '##는데', '신', '##성', '##록', '은', '몸', '##매', '진', '##짜', '이', '##쁘', '##다', '여자', '가', '봐', '##도', '부', '##러', '##운', '몸', '우', '오']\n",
            "tokens_a :  ['인', '##정', '합', '##니다']\n",
            "tokens_a :  ['심', '##장', '터', '##질', '뻔']\n",
            "tokens_a :  ['머', '##리', '내', '##린', '##거', '존', '##잘']\n",
            "tokens_a :  ['둘', '##다', '멍', '##청']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '진', '##짜', '이', '##쁘', '##다']\n",
            "tokens_a :  ['토', '##나', '##온', '##다']\n",
            "tokens_a :  []\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '예', '##쁜', '##데']\n",
            "tokens_a :  ['상', '##사', '랑', '비', '##서', '관', '##계', '이', '##딴', '식', '으로', '묘', '##사', '하는', '드라마', '좋', '##다고', '쳐', '보', '##는', '개', '##돼', '##지', '들', '자', '##기', '들', '인', '##권', '추', '##락', '하는', '드라마', '그', '##리', '보고', '##싶', '##냐']\n",
            "tokens_a :  ['저', '질', '류', '드라마']\n",
            "tokens_a :  ['그', '##건', '너', '에', '##게', '달', '##렸', '##지', '오', '글']\n",
            "tokens_a :  ['여성', '비', '##율', '댓', '##글', '창', '더', '##럽', '##네', '그', '분', '##들', '좌', '##표', '[UNK]', '나', '##용']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '한', '##테', '걸', '##어', '##갈', '때', '이', '##야', '멋', '##지', '긴', '##하', '##군']\n",
            "tokens_a :  ['장', '##나라', '랑', '이', '##엘', '##리', '##야', '랑', '각각', '있을', '때', '마', '##다', '넘', '달', '##라', '##지고', '연', '##기', '잘', '하는', '##듯']\n",
            "tokens_a :  ['참', '##한', '드', '##는', '케', '##이', '##블', '은', '발', '##전', '한', '데', '공', '##중', '##파', '는', '계속', '퇴', '보', '##되', '네', '여', '##전', '##히', '시', '##청', '##자', '가', '자', '##극', '적', '인', '것', '좋', '##아', '##하는', '줄', '알', '고', '그', '걸', '로', '시', '##청', '##률', '높', '##이', '##려', '##는', '것', '맨', '##날', '사', '##골', '우', '##리', '듯', '한', '캐', '##릭', '##터', '설', '##정', '스', '##토', '##리', '진', '##심', '제작', '##비', '가', '아', '##깝', '##다', '그', '돈', '으로', '연', '##극', '이', '##나', '다른', '곳', '에', '투', '##자', '하지', '나', '##도', '작', '##가', '지', '##망', '생', '이', '##지만', '여', '##전', '##히', '저', '##런', '것', '보', '##면', '답', '##답', '##함', '대', '##중', '은', '변', '##하는', '##데', '기', '##존', '작', '##가', '는', '여', '##전', '##히', '퇴', '##보', '도', '##전', '하는', '작', '##가', '는', '적', '##은', '것', '같', '##아', '좀', '##더', '다', '##양', '##하게', '시', '##도', '하고', '보', '##여', '##줬', '##으면', '싶', '##음']\n",
            "tokens_a :  ['대', '##사', '가', '류', '인', '##터', '##넷', '소', '##설', '급', '이', '##네']\n",
            "tokens_a :  ['[UNK]', '존', '##나', '오', '##글', '##거', '##리', '##네']\n",
            "tokens_a :  ['물', '고', '빠', '##는', '##것', '##만', '보', '##이는', '##건']\n",
            "tokens_a :  ['오', '이', '##번', '드라마', '에서', '이', '##엘', '##리', '##야', '비', '##중', '크', '##다']\n",
            "tokens_a :  ['목', '##걸', '##이', '랑', '[UNK]', '아', '##우', '껴']\n",
            "tokens_a :  ['둘', '이', '의', '##외', '##로', '어', '##울', '##려']\n",
            "tokens_a :  ['별', '##로', '다', '별', '##로', '야']\n",
            "tokens_a :  ['이', '##태', '##임', '하고', '송', '##지', '##효', '섞', '##은', '얼', '##굴']\n",
            "tokens_a :  ['신', '##성', '##록', '이런', '역', '##할', '엄', '##청', '잘', '하', '##네', '다른', '역', '##할', '도', '해', '##봤', '##으면']\n",
            "tokens_a :  ['근', '##데', '너', '##무', '오', '##글', '##거', '##려', '진', '##심', '으로', '세', '##기', '에', '[UNK]', '폐', '##하']\n",
            "tokens_a :  ['헐', '몸', '봐', '[UNK]']\n",
            "tokens_a :  ['드라마', '심', '##각', '보다', '##가', '어', '##이', '##없', '##어', '##서', '웃', '##음', '만', '나', '##옴', '시대', '에', '뒤', '떨', '##어', '##지고', '진', '##부', '##한', '드라마', '좀', '그', '##만', '찍', '##으면', '안', '##되', '##나', '배우', '들', '도', '이런', '드라마', '좀', '거', '##르', '고']\n",
            "tokens_a :  ['옷', '이', '왜', '저', '##런', '건', '지', '누', '##가', '좀', '설', '##명', '을']\n",
            "tokens_a :  ['아', '진', '##짜', '구', '##리', '다', '시대', '를', '역', '##행', '하', '##네', '웬', '##일', '이', '##냐']\n",
            "tokens_a :  ['야', '##하다', '공', '##중', '##파']\n",
            "tokens_a :  ['너', '##무', '야', '##동', '찍', '##어', '대', '##더', '##라', '포', '노', '##가', '##기', '일', '##보', '직', '##전', '처', '##럼', '애', '들', '보', '##기', '민', '##망', '##함']\n",
            "tokens_a :  ['그', '##런', '목', '##걸', '##이', '는', '도', '##대', '##체', '어', '##디', '##서', '난', '##거', '##냐']\n",
            "tokens_a :  ['진', '##짜', '개', '더', '##럽', '##다', '대', '##사', '##도', '개', '오', '##글', '##거', '##리고']\n",
            "tokens_a :  ['카', '##톡', '개', '넘', '[UNK]']\n",
            "tokens_a :  ['더', '##럽', '##다', '비', '##서', '성', '적', '##대', '상', '화', '##하는', '건', '##가', '생', '##각', '이', '있는', '건', '##지', '없는', '건', '##지']\n",
            "tokens_a :  ['돈', '꽃', '##처럼', '진', '##지', '모', '##드', '로', '가', '##던', '##가', '명', '##랑', '소', '##녀', '성', '공', '##기', '처', '##럼', '캐', '발', '##랄', '로', '가', '##던', '##가', '그', '##것', '도', '아', '##니', '##면', '별', '##그', '##대', '처', '##럼', '완', '급', '조', '##절', '을', '잘', '##해야', '지', '드라마', '이', '거', '뭐', '냐']\n",
            "tokens_a :  ['이', '게', '뭐', '냐', '심', '##의', '에', '안', '걸', '##리는', '##게', '웃', '##기', '다']\n",
            "tokens_a :  ['여', '비', '##서', '에', '대한', '인', '##식', '이', '바', '##뀌', '기', '##는', '개', '##뿔', '아', '##무', '##리', '드라마', '고', '캐', '##릭', '##터', '상', '이라', '##지만', '시대', '가', '어느', '[UNK]', '이런', '걸', '[UNK]', '볼', '거', '라', '생', '##각', '하는', '##거', '##냐', '솔', '##직', '##히', '비', '##서', '지', '##망', '하는', '학', '##생', '으로', '##서', '이런', '드라마', '가', '아', '##직', '##도', '있다는', '##게', '많이', '안', '##타', '##깝', '##다', '도', '로', '##맨', '##틱', '하지', '않', '##음', '더', '##러', '##워', '에', '라', '##이', '[UNK]', '[UNK]']\n",
            "tokens_a :  ['세', '##기', '에', '저', '##런', '대', '##사', '가', '나', '##오', '##다', '##니', '[UNK]']\n",
            "tokens_a :  ['나', '##이', '차', '##나는', '상', '##사', '와', '여', '비', '##서', '가', '또', '이', '##렇게', '쓰', '##여', '##지', '##네', '진', '##짜', '언', '##제', '바', '##뀌', '##냐', '이런', '##게', '대', '##중', '##매', '##체', '에', '자', '##꾸', '보', '##이', '니', '##까', '사', '##람', '들', '이', '여', '비', '##서', '가', '성', '##폭', '##력', '당', '##했다', '##고', '해', '##도', '로', '##맨', '##틱', '이라', '생', '##각', '하지', '위', '##력', '에', '의한', '성', '##폭', '##력', '이', '아니라', '[UNK]']\n",
            "tokens_a :  ['둘', '잘', '어', '##울', '##리는', '##데', '내', '##용', '무', '##엇', '비', '##서', '와', '여자', '래']\n",
            "tokens_a :  ['아', '##니', '연', '##기', '는', '다', '잘', '하는', '##데', '각', '##본', '이', '왜', '이', '##래']\n",
            "tokens_a :  ['상', '##사', '와', '여', '##비', '서', '관', '##계', '아', '##직', '##도', '이', '##렇게', '묘', '##사', '되', '##다', '##니']\n",
            "tokens_a :  ['이', '게', '첫', '방', '##이', '였', '##다고']\n",
            "tokens_a :  ['[UNK]']\n",
            "tokens_a :  ['목', '##걸', '##이', '좀', '하지', '##마']\n",
            "tokens_a :  ['카', '##톡', '개', '좋', '##겠', '##다']\n",
            "tokens_a :  ['진', '##심', '개', '이', '##쁘', '##네']\n",
            "tokens_a :  ['누', '##구', '생', '##각', '##나', '##네', '비', '##서', '로', '여자', '로']\n",
            "tokens_a :  []\n",
            "tokens_a :  ['신', '##성', '##록', '역', '##할', '은', '목', '##소', '##리', '엄', '##청', '낮', '##은', '사', '##람', '이', '하', '##면', '더', '어', '##울', '##렸', '##을', '##듯', '신', '##성', '##록', '은', '너', '##무', '가', '##벼', '움']\n",
            "tokens_a :  ['왜', '항', '##상', '드라마', '는', '빙', '##빙', '저', '##래', '##도', '노']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '이', '##쁜', '##데', '왜', '이런', '역', '##할', '하', '지']\n",
            "tokens_a :  ['무', '##리', '없는', '드라마', '될', '##듯', '대', '##박', '##나', '##길', '더', '로', '##사', '스', '##토', '##어', '팜', '구', '##경', '오', '##세', '##요', '예', '##쁜', '옷', '있어', '##요']\n",
            "tokens_a :  ['임', '##성', '##한', '작', '##가', '는', '쓰', '##면서', '##도', '이', '##건', '아', '##니', '##지', '너', '##무', '심', '##하지', '했', '##지만', '만', '##에', '하', '##나', '상', '##대', '드라마', '가', '마지막', '회', '프', '##리', '##미', '##엄', '을', '얻', '##어', '프로', '라', '##도', '우', '##리', '드라마', '를', '앞', '##서', '##면', '그', '드라마', '는', '결국', '우', '##리', '드라마', '를', '이', '##기', '고', '끝', '##난', '게', '되', '##기', '때', '##문', '에', '그', '##동안', '의', '내', '고', '##생', '과', '노', '##력', '이', '수', '##포', '로', '돌', '##아', '##갈', '수', '밖', '##에', '없', '##어', '눈', '##물', '을', '머', '금', '##고', '에', '##피', '##소', '##드', '를', '동', '##원', '했다', '##고', '밝혔다', '임', '작', '##가', '는', '자', '##신', '의', '이', '같은', '행', '##동', '을', '악', '##행', '이라', '표', '##현', '하', '##기도', '했다', '김', '##순', '##옥', '도', '임', '##성', '##한', '드', '##작', '은', '##퇴', '하고', '털', '##어', '##놓', '##는', '이야기', '시', '##청', '##률', '위해', '독', '##약', '풀', '##라', '고', '방송', '##국', '에서', '전', '##화', '오', '##면', '사', '##람', '죽', '이', '##고', '사', '##고', '##내', '##고', '시', '##청', '##률', '이', '진', '##정', '다', '##냐']\n",
            "tokens_a :  ['저', '여자', '[UNK]']\n",
            "tokens_a :  ['섹', '##시', '##해', '신', '##성', '##록', '오', '##빠', '[UNK]']\n",
            "tokens_a :  ['너', '##무', '마', '##르', '##셔', '##서', '살', '좀', '##더', '찌', '면', '나', '##을', '##거', '같', '##아', '##요']\n",
            "tokens_a :  ['아', '느', '##끼', '##해']\n",
            "tokens_a :  ['김', '##순', '##옥', '진', '##화', '한', '귀', '##여', '##니', '같은', '인', '##간']\n",
            "tokens_a :  ['첫', '방', '부', '##터', '너', '##무', '좀', '그', '##렇', '##다']\n",
            "tokens_a :  ['나', '만', '이어', '##폰', '한', '##쪽', '만', '들', '##림']\n",
            "tokens_a :  ['신', '##성', '##록', '아', '##내', '가', '이', '거', '보', '##면', '속', '뒤', '##집', '##어', '##질', '듯']\n",
            "tokens_a :  ['신', '##성', '##록', '존', '##잘', '어', '##깨', '선', '대', '##박', '그', '##런', '##데', '목', '##걸', '##이', '랑', '목', '폴', '##라', '는', '버', '##려']\n",
            "tokens_a :  ['근', '##데', '무', '##서', '##워']\n",
            "tokens_a :  ['대', '##사', '개', '##오', '글']\n",
            "tokens_a :  ['신', '##성', '##록', '이런', '역', '들', '그', '##만', '좀', '해', '##줬', '##으면', '죽', '##어', '야', '##사', '는', '남자', '같은', '드라마', '때', '는', '너', '##무', '좋', '##았', '##는', '[UNK]', '이미', '##지', '변', '##신', '이', '필', '##요', '##할', '때', '인', '##듯']\n",
            "tokens_a :  ['얘', '는', '왜', '이런', '역', '##할', '만', '해']\n",
            "tokens_a :  ['신', '##성', '##록', '이', '##마', '주', '##름', '이', '##랑', '[UNK]', '[UNK]', '야', '##할', '##듯']\n",
            "tokens_a :  ['으', '##악']\n",
            "tokens_a :  ['혼', '##자', '보', '##길', '망', '##정', '이', '##지', '가', '##족', '들', '같이', '있', '##었', '##음', '민', '##망', '##해서', '죽', '을', '뻔']\n",
            "tokens_a :  ['박', '##력', '과', '분', '##위', '##기', '때', '##문', '에', '계속', '놀', '##람']\n",
            "tokens_a :  ['오', '##우', '##야', '오', '##우', '##야']\n",
            "tokens_a :  ['근', '##데', '포', '##스터', '에', '대한민국', '이라고', '써', '##있', '##음']\n",
            "tokens_a :  ['나', '##라', '님', '연', '##기', '넘', '자', '##연', '##스', '##럽', '자', '나']\n",
            "tokens_a :  ['사', '##랑', '##스', '##럽', '##네', '##요', '진', '##짜', '대', '##단', '##해', '##요']\n",
            "tokens_a :  ['장', '##나라', '정', '##말', '이', '##쁘', '##네', '안', '늙', '##어']\n",
            "tokens_a :  ['써', '##니', '성', '##격', '좋', '##네']\n",
            "tokens_a :  ['장', '##나라', '화', '##이', '##팅']\n",
            "tokens_a :  ['이', '##쁘', '##다', '넘', '넘']\n",
            "tokens_a :  ['동안', '##미', '##녀', '나', '##라', '언', '##니']\n",
            "tokens_a :  ['보', '##기', '만', '##해', '도', '에', '##너', '##지', '[UNK]', '장', '##나라', '넘', '이', '##쁘', '##고', '귀', '##여', '##워']\n",
            "tokens_a :  ['김', '##종', '##구', '배우']\n",
            "tokens_a :  ['너', '##무', '귀', '##여', '##운', '##거', '[UNK]']\n",
            "tokens_a :  ['나', '##라', '##짱']\n",
            "tokens_a :  ['화', '##장', '하는', '##거나', '올', '때', '예', '##뻐', '##서', '놀', '##랐', '##네', '##요', '귀', '##여', '##운', '오', '써', '##니']\n",
            "tokens_a :  ['귀', '##여', '##운', '##거', '바']\n",
            "tokens_a :  ['김', '##종', '##국', '찾', '##기']\n",
            "tokens_a :  ['장', '##나라', '김', '##종', '##구', '배우', '님', '졸', '##귀']\n",
            "tokens_a :  ['김', '##종', '##구', '배우', '님', '이다', '방', '##금', '배', '##니', '##싱', '보고', '##왔', '##는데']\n",
            "tokens_a :  ['김', '##종', '##구', '배우', '님']\n",
            "tokens_a :  ['장', '##나라', '진', '##짜', '변', '##하지', '않는', '외', '##모', '다', '헤', '##어', '##스', '##타', '##일', '귀', '여', '[UNK]']\n",
            "tokens_a :  ['장', '##나라', '미', '##모']\n",
            "tokens_a :  ['근', '##데', '포', '##스터', '에', '대한민국', '이라고', '써', '##있', '##음']\n",
            "tokens_a :  ['나', '##라', '님', '연', '##기', '넘', '자', '##연', '##스', '##럽', '자', '나']\n",
            "tokens_a :  ['사', '##랑', '##스', '##럽', '##네', '##요', '진', '##짜', '대', '##단', '##해', '##요']\n",
            "tokens_a :  ['장', '##나라', '정', '##말', '이', '##쁘', '##네', '안', '늙', '##어']\n",
            "tokens_a :  ['써', '##니', '성', '##격', '좋', '##네']\n",
            "tokens_a :  ['장', '##나라', '화', '##이', '##팅']\n",
            "tokens_a :  ['이', '##쁘', '##다', '넘', '넘']\n",
            "tokens_a :  ['동안', '##미', '##녀', '나', '##라', '언', '##니']\n",
            "tokens_a :  ['보', '##기', '만', '##해', '도', '에', '##너', '##지', '[UNK]', '장', '##나라', '넘', '이', '##쁘', '##고', '귀', '##여', '##워']\n",
            "tokens_a :  ['김', '##종', '##구', '배우']\n",
            "tokens_a :  ['너', '##무', '귀', '##여', '##운', '##거', '[UNK]']\n",
            "tokens_a :  ['나', '##라', '##짱']\n",
            "tokens_a :  ['화', '##장', '하는', '##거나', '올', '때', '예', '##뻐', '##서', '놀', '##랐', '##네', '##요', '귀', '##여', '##운', '오', '써', '##니']\n",
            "tokens_a :  ['귀', '##여', '##운', '##거', '바']\n",
            "tokens_a :  ['김', '##종', '##국', '찾', '##기']\n",
            "tokens_a :  ['장', '##나라', '김', '##종', '##구', '배우', '님', '졸', '##귀']\n",
            "tokens_a :  ['김', '##종', '##구', '배우', '님', '이다', '방', '##금', '배', '##니', '##싱', '보고', '##왔', '##는데']\n",
            "tokens_a :  ['김', '##종', '##구', '배우', '님']\n",
            "tokens_a :  ['장', '##나라', '진', '##짜', '변', '##하지', '않는', '외', '##모', '다', '헤', '##어', '##스', '##타', '##일', '귀', '여', '[UNK]']\n",
            "tokens_a :  ['장', '##나라', '미', '##모']\n",
            "tokens_a :  ['장', '##나라', '쥬', '아']\n",
            "tokens_a :  ['나', '##라', '언', '##니', '귀', '##여', '##워']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '너', '##무', '이', '##쁘', '##고', '연', '##기', '잘', '한다']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '보', '##살', '이', '##네']\n",
            "tokens_a :  ['헤', '##어', '##지', '##면', '되', '##지', '저', '때', '까', '##지', '왜', '만', '냐', '나']\n",
            "tokens_a :  ['아', '##니', '바', '##람', '은', '당', '##연', '##히', '싫', '##지', '그리고', '유', '##라', '년', '저', '##렇게', '끝', '에', '애', '##교', '부', '##리', '##면', '은', '좋은', '줄', '알', '지']\n",
            "tokens_a :  ['어', '##우', '무', '##섭']\n",
            "tokens_a :  ['개', '불', '##쌍', '##하다', '[UNK]', '살', '##빼', '##든', '말', '든', '잘', '##생', '##겨', '##지', '##든', '말', '든', '응', '##원', '하', '##나', '도', '안', '##된다', '[UNK]']\n",
            "tokens_a :  ['[UNK]', '살', '##빼', '##는', '계', '##기', '가', '인', '##성', '과', '관련', '돼', '있어서', '인', '##성', '도', '좀', '변', '##했', '##으면']\n",
            "tokens_a :  ['자', '##기', '구', '해', '##준', '은', '##인', '집', '에', '불', '지', '##르는', '##것', '##도', '극', '##혐']\n",
            "tokens_a :  ['머', '##리', '에', '총', '맞', '##고', '성', '##격', '변', '##하는', '듯']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '진', '##짜', '진', '##심', '싫', '##어', '##해']\n",
            "tokens_a :  ['그', '여자', '가', '그', '##만', '이상', '한', '짓', '좀', '[UNK]']\n",
            "tokens_a :  ['남자', '가', '얼', '##굴', '여', '##부', '를', '떠', '##나', '문', '##제', '가', '많', '##아', '보', '##이는', '##데', '##요', '직', '##장', '앞', '에서']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '대', '##박', '이다']\n",
            "tokens_a :  ['남', '##주', '너', '##무', '스', '##토', '##커', '같', '##음']\n",
            "tokens_a :  ['아', '##니', '애', '##초', '에', '왜', '사', '##귀', '##게', '된', '##건', '가', '##요', '이', '##엘', '##리', '##야', '가', '나', '##쁜', '사', '##람', '##인', '줄', '알', '##았', '##는데', '보', '##살', '이', '##었', '##네']\n",
            "tokens_a :  ['저', '정도', '면', '데', '##이트', '폭', '##력', '아', '##니', '##냐', '저', '##건', '아', '##니다']\n",
            "tokens_a :  ['신', '##은', '##경', '씨', '좋은', '연', '##기', '다시', '볼', '수', '있어서', '감', '##사', '##합', '##니다']\n",
            "tokens_a :  ['외', '##모', '를', '떠', '##나', '##서', '행', '##동', '이', '##나', '이런', '##게', '진', '##짜', '최', '##악', '사', '##람', '들', '많은', '직', '##장', '에서', '큰', '##소', '##리', '치', '##면', '서', '부', '##르고', '집', '##착', '하고']\n",
            "tokens_a :  ['진', '##짜', '열', '라', '싫', '##은', '##거', '같다']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '넘', '예', '##쁘', '##다']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '너', '##무', '이', '##쁘', '##고', '연', '##기', '잘', '한다']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '보', '##살', '이', '##네']\n",
            "tokens_a :  ['헤', '##어', '##지', '##면', '되', '##지', '저', '때', '까', '##지', '왜', '만', '냐', '나']\n",
            "tokens_a :  ['아', '##니', '바', '##람', '은', '당', '##연', '##히', '싫', '##지', '그리고', '유', '##라', '년', '저', '##렇게', '끝', '에', '애', '##교', '부', '##리', '##면', '은', '좋은', '줄', '알', '지']\n",
            "tokens_a :  ['어', '##우', '무', '##섭']\n",
            "tokens_a :  ['개', '불', '##쌍', '##하다', '[UNK]', '살', '##빼', '##든', '말', '든', '잘', '##생', '##겨', '##지', '##든', '말', '든', '응', '##원', '하', '##나', '도', '안', '##된다', '[UNK]']\n",
            "tokens_a :  ['[UNK]', '살', '##빼', '##는', '계', '##기', '가', '인', '##성', '과', '관련', '돼', '있어서', '인', '##성', '도', '좀', '변', '##했', '##으면']\n",
            "tokens_a :  ['자', '##기', '구', '해', '##준', '은', '##인', '집', '에', '불', '지', '##르는', '##것', '##도', '극', '##혐']\n",
            "tokens_a :  ['머', '##리', '에', '총', '맞', '##고', '성', '##격', '변', '##하는', '듯']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '진', '##짜', '진', '##심', '싫', '##어', '##해']\n",
            "tokens_a :  ['그', '여자', '가', '그', '##만', '이상', '한', '짓', '좀', '[UNK]']\n",
            "tokens_a :  ['남자', '가', '얼', '##굴', '여', '##부', '를', '떠', '##나', '문', '##제', '가', '많', '##아', '보', '##이는', '##데', '##요', '직', '##장', '앞', '에서']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '대', '##박', '이다']\n",
            "tokens_a :  ['남', '##주', '너', '##무', '스', '##토', '##커', '같', '##음']\n",
            "tokens_a :  ['아', '##니', '애', '##초', '에', '왜', '사', '##귀', '##게', '된', '##건', '가', '##요', '이', '##엘', '##리', '##야', '가', '나', '##쁜', '사', '##람', '##인', '줄', '알', '##았', '##는데', '보', '##살', '이', '##었', '##네']\n",
            "tokens_a :  ['저', '정도', '면', '데', '##이트', '폭', '##력', '아', '##니', '##냐', '저', '##건', '아', '##니다']\n",
            "tokens_a :  ['신', '##은', '##경', '씨', '좋은', '연', '##기', '다시', '볼', '수', '있어서', '감', '##사', '##합', '##니다']\n",
            "tokens_a :  ['외', '##모', '를', '떠', '##나', '##서', '행', '##동', '이', '##나', '이런', '##게', '진', '##짜', '최', '##악', '사', '##람', '들', '많은', '직', '##장', '에서', '큰', '##소', '##리', '치', '##면', '서', '부', '##르고', '집', '##착', '하고']\n",
            "tokens_a :  ['진', '##짜', '열', '라', '싫', '##은', '##거', '같다']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '넘', '예', '##쁘', '##다']\n",
            "tokens_a :  ['우', '[UNK]', '소', '##름']\n",
            "tokens_a :  ['외', '##모', '를', '떠', '##나', '##서', '나', '##왕', '식', '극', '##혐', '소', '##름', '돋', '##는다', '어느', '미', '##친', '여자', '가', '저', '##런', '폭', '##력', '남', '에', '집', '##착', '남', '을', '좋', '##아', '##하', '##겠', '##냐', '민', '유', '##라', '가', '다', '이', '##해', '될', '지', '##경', '나', '##라', '면', '완', '##전', '공', '##포', '겠', '##다', '눈', '에', '[UNK]', '없는', '##거', '같은', '##데', '여자', '가', '떠', '##나', '##겠', '##다고', '하', '##면', '패', '죽', '##일', '##거', '같은', '으', '극', '##혐']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '미', '##모', '미', '##침', '겁', '##나', '이', '##쁘', '##다', '진', '##짜']\n",
            "tokens_a :  ['무', '##섭', '##네', '외', '##모', '만', '변', '##하면', '다', '##야', '스', '##토', '##리', '왜', '이', '##래']\n",
            "tokens_a :  ['나', '##중', '에', '[UNK]', '목', '##소', '##리', '도', '거', '##칠', '##고', '눈', '##치', '없이', '무', '##식', '##하고', '매', '##너', '없는', '지', '##금', '의', '나', '##왕', '식', '##이', '최', '##진', '##혁', '으로', '바', '##뀐', '들', '팬', '들', '이', '어', '##찌', '호', '##감', '을', '느', '##낄', '##까', '차', '##라', '##리', '최', '##진', '##혁', '을', '뚱', '##뚱', '##하게', '분', '##장', '을', '시', '##키', '##지', '그', '##랬', '##냐', '제작', '##비', '가', '부', '##족', '##했', '##나']\n",
            "tokens_a :  ['아', '##니', '심', '##하', '##노', '살', '##빼', '##고', '최', '##진', '##혁', '실', '##화']\n",
            "tokens_a :  ['헐', '할', '##말', '잃']\n",
            "tokens_a :  ['바', '##람', '피', '##는', '##건', '당', '##연', '##히', '못', '참', '##지', '[UNK]']\n",
            "tokens_a :  ['최', '##진', '##혁', '배우', '님', '만', '기', '##다', '##립', '##니다']\n",
            "tokens_a :  ['아', '##니', '대', '##체', '극', '##본', '왜', '이런']\n",
            "tokens_a :  ['죄', '##송', '##하지', '##만', '외', '##모', '를', '자', '##꾸', '만', '##약', '최', '##진', '##혁', '이', '저', '역', '##할', '햇', '다', '며', '아', '##마', '그', '##냥', '사', '##랑', '꾼', '이', '엿', '게', '죠']\n",
            "tokens_a :  ['남', '##주', '성', '##격', '저', '##건', '거의', '집', '##착', '남', '인', '##데', '절', '##대', '사', '귀', '##지', '말', '아', '야', '##할']\n",
            "tokens_a :  ['솔', '##직', '##히', '저', '##런', '남', '##친', '은', '외', '##모', '가', '문', '##제', '가', '아니라', '집', '##착', '때', '##문', '에', '되', '##게', '위', '##험', '##하고', '싫', '##을', '##거', '같은', '##데']\n",
            "tokens_a :  ['남', '주', '##인', '##공', '최', '##진', '##혁', '으로', '변', '##하', '##든', '말', '든', '성', '##격', '극', '##혐', '##임', '여자', '직', '##장', '앞', '에서', '소', '##리', '[UNK]', '질', '##러', '대', '##고', '행', '##패', '부', '##리고', '여자', '가', '부', '##담', '##스', '##러', '##워', '##하는', '티', '내', '##는데', '##도', '집', '##착', '에', '모', '##가지', '를', '[UNK]', '는', '##다', '##느', '##니', '뭐', '니', '진', '##짜', '싫', '##다', '소', '##름', '##끼', '##쳐']\n",
            "tokens_a :  ['아', '##무', '##리', '[UNK]', '않은', '복', '##권', '이라', '해', '##도', '태', '##항', '##호', '에서', '최', '##진', '##혁', '으로', '바', '##뀌', '##는', '건', '반', '##칙', '아닌', '감']\n",
            "tokens_a :  ['민', '유', '##라', '바', '##람', '합', '##법']\n",
            "tokens_a :  ['메', '##인', '남', '주', '##가', '너', '##무', '문', '##제', '가', '많', '##네', '각', '성', '한다', '##해', '도', '사', '##람', '본', '##질', '이', '변', '##할', '리', '##가']\n",
            "tokens_a :  ['태', '##항', '##호', '호', '##블', '##리', '가', '이', '##딴', '역', '을', '맡', '##다', '##니']\n",
            "tokens_a :  ['외', '##모', '가', '잘', '##생', '##겨', '##져', '##도', '저', '인', '##성', '이', '##면', '문', '##제', '많', '##을', '것', '같은', '##데', '집', '##착', '있어', '보', '##이', '고', '비', '##정', '##상', '적', '인', '사', '##람', '같', '##음']\n",
            "tokens_a :  ['장', '##난', '하', '##나', '역시', '작', '##가', '는', '기', '##대', '를', '저', '버', '##리', '##지', '않', '##네', '역시', '[UNK]', '저', '게', '무', '##슨', '장', '##난', '해']\n",
            "tokens_a :  ['성', '##형', '은', '그', '##렇', '##다', '쳐', '##도', '키', '가', '[UNK]', '페', '##이스', '##오', '##프', '수', '##준', '이', '아닌', '##데']\n",
            "tokens_a :  ['궁', '##금', '##하', '##네', '아', '##냐', '반', '##전', '이', '있을', '##것', '같', '##아', '난', '낼', '도', '본', '##방', '사', '##수', '궁', '##금', '##해']\n",
            "tokens_a :  ['와', '##우', '최', '##진', '##혁', '씨', '빨', '##리', '나', '##와', '##주', '##세', '##요', '용']\n",
            "tokens_a :  ['태', '##항', '##호', '배우', '에서', '최', '##진', '##혁', '배우', '로', '나', '##왕', '식', '##이', '엄', '##마', '사', '##고', '를', '파', '##헤', '##치', '##다', '황', '##제', '오', '##른', '팔', '마', '필', '##주', '##에게', '위', '##협', '을', '받고', '그', '후', '예', '##전', '경', '호', '##원', '##대', '장', '##이', '왕', '##식', '이', '목', '##숨', '구', '하고', '변', '##신', '최', '##진', '##혁', '등', '##장', '입', '##니다']\n",
            "tokens_a :  ['살', '빼', '##고', '성', '##형', '하', '##시', '##고', '최', '##진', '##혁', '으로', '[UNK]', '나', '##오는', '건', '##가', '요', '이런', '뭐', '점', '하', '##나', '찍', '##은', '##것', '보', '##단', '훨', '현', '##실', '적', '이', '##긴', '하지만', '허', '##허', '신', '##선', '##하', '##네', '##요', '장', '##나라', '씨', '와', '최', '##진', '##혁', '씨', '가', '막', '##드', '에', '나', '##온', '##다', '##니']\n",
            "tokens_a :  ['저', '분', '이', '각', '##성', '하고', '살', '##빼', '##서', '최', '##진', '##혁', '이', '되', '##구', '너']\n",
            "tokens_a :  ['최', '##진', '##혁', '이', '되는', '##걸', '떠', '##나', '##서', '문', '##제', '많', '##아', '보', '##이는', '##데']\n",
            "tokens_a :  ['아', '##니', '어', '##케', '저', '사', '##람', '이', '최', '##진', '##혁', '이', '될', '수', '##가', '있', '##지']\n",
            "tokens_a :  ['저', '분', '이', '나', '##중', '에', '살', '##빼', '##서', '최', '##진', '##혁', '이', '[UNK]']\n",
            "tokens_a :  ['극', '##혐', '돈', '벌', '##기', '힘', '##들', '##다']\n",
            "tokens_a :  ['은', '##근', '소', '##름']\n",
            "tokens_a :  ['어', '##우']\n",
            "tokens_a :  ['눈', '##물', '을', '머', '금', '##고', '못', '받', '##았', '##어']\n",
            "tokens_a :  ['받', '##았', '##어', '##야', '##지']\n",
            "tokens_a :  ['나', '같', '##으면', '감', '##사', '##합', '##니다', '##하고', '챙', '##길', '##거', '##야', '어', '##차', '##피', '저', '사', '##람', '들', '다시', '볼', '사', '##람', '들', '도', '아', '##니', '##고', '잠', '##깐', '의', '창', '##피', '##함', '##만', '참', '##으면', '되', '##니', '##까']\n",
            "tokens_a :  ['개', '웃', '##겨']\n",
            "tokens_a :  ['저', '나', '##왕', '식', '이라고', '써', '##있는', '목', '##걸', '##이', '를', '써', '##니', '가', '간', '##직', '하고', '있다', '##가', '살', '##빼', '##고', '멋', '##있', '##어진', '최', '##진', '##혁', '이', '발', '##견', '하', '##게', '되는', '내', '##용', '이', '나', '##올', '것', '같', '##아', '##요']\n",
            "tokens_a :  ['황', '##후', '는', '장', '##나라', '가', '나', '##서', '##줘', '##야', '재', '##미', '질', '##듯', '장', '##나라', '는', '모두', '랑', '케', '##미', '괜', '##찮', '##네']\n",
            "tokens_a :  ['장', '##나라', '태', '##후', '랑', '케', '##미', '있', '##네', '나', '##중', '에', '대', '##결', '기', '##대', '된다']\n",
            "tokens_a :  ['장', '##나라', '연', '기', '스', '##탈', '질', '##려']\n",
            "tokens_a :  ['명', '##랑', '성', '공', '##기', '였', '##나', '그', '##때', '랑', '연', '##기', '##력', '이', '##나', '톤', '이', '##나', '똑', '##같', '##다']\n",
            "tokens_a :  ['나', '##라', '##짱', '넘', '귀', '##여', '##워', '[UNK]', '나', '##라', '##짱', '의', '시', '##간', '만', '거', '##꾸', '##로', '가', '##는', '##듯']\n",
            "tokens_a :  ['돈', '도', '잘', '못', '벌', '##텐', '##데', '챙', '##기', '##지']\n",
            "tokens_a :  ['위', '##로', '금', '을', '누', '##가', '저', '##딴', '식', '으로', '주', '##냐', '치', '##료', '비', '일', '##체', '보', '##상', '이', '기', '##본', '이', '##지', '황', '##제', '테', '##러', '에', '같이', '있던', '민', '##간', '##인', '은', '치', '##료', '만', '해', '##준', '##다', '쳐', '##도', '장', '##나라', '는', '황', '##제', '의', '목', '##숨', '을', '구', '했다', '##고', '언', '##론', '에서', '인', '##터', '##뷰', '하', '##려', '##고', '득', '##달', '같이', '붙', '##는', '장', '##면', '이', '나', '##와', '##야', '되는', '##데', '입', '##헌', '##군', '##주', '##국', '에서', '저', '##렇게', '언', '##론', '을', '덮', '는', '##게', '말', '이', '되', '##나', '파', '##파', '##라', '##치', '가', '기', '##본', '인', '##데']\n",
            "tokens_a :  ['속', '##물', '써', '##니', '귀', '##여', '##워', '나', '같', '##으면', '철', '##판', '깔', '##고', '받', '##을', '##텐', '##데']\n",
            "tokens_a :  ['귀', '##여', '##워', '##라', '얼', '##른', '챙', '##겼', '##어', '##야', '##지', '자', '##존', '##심', '이', '뭐', '##라고', '황', '##실', '은', '애', '##초', '에', '줄', '생', '##각', '이', '없', '##었', '##나', '본', '##데', '저', '##걸', '받', '##았', '##어', '##야', '##지', '태', '##후', '는', '태', '##후', '네', '[UNK]', '에서', '악', '##의', '기', '##운', '이', '느', '##껴', '##진다']\n",
            "tokens_a :  ['저', '라', '두', '그', '##냥', '챙', '##겼', '##을', '##것', '같은', '나', '##라', '언', '##니', '완', '##전', '[UNK]']\n",
            "tokens_a :  ['다', '들', '왜', '눈', '##치', '줘', '큐']\n",
            "tokens_a :  ['어', '##여', '뿐', '나', '##라', '##짱']\n",
            "tokens_a :  ['귀', '##여', '##워']\n",
            "tokens_a :  ['귀', '##엽', '##네']\n",
            "tokens_a :  ['자', '##존', '##심', '이', '뭐', '##라고']\n",
            "tokens_a :  ['눈', '##물', '을', '머', '금', '##고', '못', '받', '##았', '##어']\n",
            "tokens_a :  ['받', '##았', '##어', '##야', '##지']\n",
            "tokens_a :  ['나', '같', '##으면', '감', '##사', '##합', '##니다', '##하고', '챙', '##길', '##거', '##야', '어', '##차', '##피', '저', '사', '##람', '들', '다시', '볼', '사', '##람', '들', '도', '아', '##니', '##고', '잠', '##깐', '의', '창', '##피', '##함', '##만', '참', '##으면', '되', '##니', '##까']\n",
            "tokens_a :  ['개', '웃', '##겨']\n",
            "tokens_a :  ['저', '나', '##왕', '식', '이라고', '써', '##있는', '목', '##걸', '##이', '를', '써', '##니', '가', '간', '##직', '하고', '있다', '##가', '살', '##빼', '##고', '멋', '##있', '##어진', '최', '##진', '##혁', '이', '발', '##견', '하', '##게', '되는', '내', '##용', '이', '나', '##올', '것', '같', '##아', '##요']\n",
            "tokens_a :  ['황', '##후', '는', '장', '##나라', '가', '나', '##서', '##줘', '##야', '재', '##미', '질', '##듯', '장', '##나라', '는', '모두', '랑', '케', '##미', '괜', '##찮', '##네']\n",
            "tokens_a :  ['장', '##나라', '태', '##후', '랑', '케', '##미', '있', '##네', '나', '##중', '에', '대', '##결', '기', '##대', '된다']\n",
            "tokens_a :  ['장', '##나라', '연', '기', '스', '##탈', '질', '##려']\n",
            "tokens_a :  ['명', '##랑', '성', '공', '##기', '였', '##나', '그', '##때', '랑', '연', '##기', '##력', '이', '##나', '톤', '이', '##나', '똑', '##같', '##다']\n",
            "tokens_a :  ['나', '##라', '##짱', '넘', '귀', '##여', '##워', '[UNK]', '나', '##라', '##짱', '의', '시', '##간', '만', '거', '##꾸', '##로', '가', '##는', '##듯']\n",
            "tokens_a :  ['돈', '도', '잘', '못', '벌', '##텐', '##데', '챙', '##기', '##지']\n",
            "tokens_a :  ['위', '##로', '금', '을', '누', '##가', '저', '##딴', '식', '으로', '주', '##냐', '치', '##료', '비', '일', '##체', '보', '##상', '이', '기', '##본', '이', '##지', '황', '##제', '테', '##러', '에', '같이', '있던', '민', '##간', '##인', '은', '치', '##료', '만', '해', '##준', '##다', '쳐', '##도', '장', '##나라', '는', '황', '##제', '의', '목', '##숨', '을', '구', '했다', '##고', '언', '##론', '에서', '인', '##터', '##뷰', '하', '##려', '##고', '득', '##달', '같이', '붙', '##는', '장', '##면', '이', '나', '##와', '##야', '되는', '##데', '입', '##헌', '##군', '##주', '##국', '에서', '저', '##렇게', '언', '##론', '을', '덮', '는', '##게', '말', '이', '되', '##나', '파', '##파', '##라', '##치', '가', '기', '##본', '인', '##데']\n",
            "tokens_a :  ['속', '##물', '써', '##니', '귀', '##여', '##워', '나', '같', '##으면', '철', '##판', '깔', '##고', '받', '##을', '##텐', '##데']\n",
            "tokens_a :  ['귀', '##여', '##워', '##라', '얼', '##른', '챙', '##겼', '##어', '##야', '##지', '자', '##존', '##심', '이', '뭐', '##라고', '황', '##실', '은', '애', '##초', '에', '줄', '생', '##각', '이', '없', '##었', '##나', '본', '##데', '저', '##걸', '받', '##았', '##어', '##야', '##지', '태', '##후', '는', '태', '##후', '네', '[UNK]', '에서', '악', '##의', '기', '##운', '이', '느', '##껴', '##진다']\n",
            "tokens_a :  ['저', '라', '두', '그', '##냥', '챙', '##겼', '##을', '##것', '같은', '나', '##라', '언', '##니', '완', '##전', '[UNK]']\n",
            "tokens_a :  ['다', '들', '왜', '눈', '##치', '줘', '큐']\n",
            "tokens_a :  ['어', '##여', '뿐', '나', '##라', '##짱']\n",
            "tokens_a :  ['귀', '##여', '##워']\n",
            "tokens_a :  ['귀', '##엽', '##네']\n",
            "tokens_a :  ['자', '##존', '##심', '이', '뭐', '##라고']\n",
            "tokens_a :  ['공', '##감']\n",
            "tokens_a :  ['그', '##냥', '가', '##져', '##가지', '##는', '아', '##깝', '##다', '황', '##실', '측', '도', '줄', '마', '##음', '이', '없', '##구', '##만']\n",
            "tokens_a :  ['나', '같', '##으면', '언', '##능', '챙', '##김']\n",
            "tokens_a :  ['[UNK]']\n",
            "tokens_a :  ['와', '이', '##엘', '##리', '##야', '어떤', '연', '##기', '는', '진', '##짜', '잘', '하는', '##데', '어떤', '연', '##기', '는', '너', '##무', '부', '##자', '##연', '스', '##럽', '##게', '하고']\n",
            "tokens_a :  ['이', '엘', '##리', '아', '이', '거', '[UNK]', '신', '##애', '##리', '양', '##달', '희', '아', '##니', '##냐']\n",
            "tokens_a :  ['이', '거', '작', '##가', '가', '임', '##성', '##한', '인', '##가', '##요', '[UNK]', '이', '##네', '[UNK]']\n",
            "tokens_a :  ['나', '##왕', '식', '##이', '민', '##유', '##리', '보고', '자', '##기', '한', '##테', '시', '##집', '오', '##라', '고', '했', '##는데', '나', '동', '##식', '은', '또', '뭐', '##임']\n",
            "tokens_a :  ['후', '에', '소', '##름', '돋', '##는', '최', '##진', '##혁', '님', '복', '##수', '기', '##대', '##해', '##요']\n",
            "tokens_a :  ['처', '##절', '##한', '연', '##기', '너', '##무', '잘', '한다', '이', '##엘', '##리', '##야', '이', '##렇게', '까', '##지', '연', '##기', '잘', '하는', '##줄', '몰', '##랐다']\n",
            "tokens_a :  ['임', '##성', '##한', '작', '##가', '는', '쓰', '##면서', '##도', '이', '##건', '아', '##니', '##지', '너', '##무', '심', '##하지', '했', '##지만', '만', '##에', '하', '##나', '상', '##대', '드라마', '가', '마지막', '회', '프', '##리', '##미', '##엄', '을', '얻', '##어', '프로', '라', '##도', '우', '##리', '드라마', '를', '앞', '##서', '##면', '그', '드라마', '는', '결국', '우', '##리', '드라마', '를', '이', '##기', '고', '끝', '##난', '게', '되', '##기', '때', '##문', '에', '그', '##동안', '의', '내', '고', '##생', '과', '노', '##력', '이', '수', '##포', '로', '돌', '##아', '##갈', '수', '밖', '##에', '없', '##어', '눈', '##물', '을', '머', '금', '##고', '에', '##피', '##소', '##드', '를', '동', '##원', '했다', '##고', '밝혔다', '임', '작', '##가', '는', '자', '##신', '의', '이', '같은', '행', '##동', '을', '악', '##행', '이라', '표', '##현', '하', '##기도', '했다', '김', '##순', '##옥', '도', '임', '##성', '##한', '드', '##작', '은', '##퇴', '하고', '털', '##어', '##놓', '##는', '이야기', '시', '##청', '##률', '위해', '독', '##약', '풀', '##라', '고', '방송', '##국', '에서', '전', '##화', '오', '##면', '사', '##람', '죽', '이', '##고', '사', '##고', '##내', '##고', '시', '##청', '##률', '이', '진', '##정', '다', '##냐']\n",
            "tokens_a :  ['이', '##엘', '##리', '##야', '연', '##기', '잘', '한다', '몰', '##입', '##도', '최고']\n",
            "tokens_a :  ['왕', '##식', '이', '엄', '##니']\n",
            "tokens_a :  ['이야기', '구', '##성', '이', '왜', '저', '##럼', '최', '##진', '##혁', '엄', '##마', '는', '진', '##짜', '이', '##엘', '##리', '##야', '엄', '##마', '처', '##럼', '굴', '##어', '자', '##기', '아들', '생', '##각', '하', '면', '그', '##냥', '자', '##기', '가', '엘', '##리', '##야', '보고', '꺼', '##지', '##라고', '연', '끊', '##고', '아들', '한', '##테', '는', '암', '말', '안', '하지', '않', '##나', '누', '##구', '엄', '##마', '인', '##지']\n",
            "tokens_a :  ['볼', '##수', '##록', '궁', '##금', '##하', '##긴', '##한', '##데', '난', '솔', '##직', '##히', '저', '남', '##편', '이', '부', '##인', '한', '##테', '딴', '놈', '하고', '##만', '나', '##면', '죽', '##인', '##다고', '한', '말', '보고', '는', '그', '##런', '사', '##람', '이', '##랑', '어', '##떻', '##게', '사', '##나', '생', '##각', '했', '##고', '엄', '##마', '라는', '사', '##람', '이', '저', '##런', '광', '##경', '을', '목', '##격', '하고', '성', '##격', '뻔', '##히', '이상', '##한', '남', '##편', '한', '##테', '가', '##서', '##말', '한다', '##는', '거', '에', '##도', '이', '##해', '가', '되', '##질', '않는다', '그', '##렇', '##다고', '바', '##람', '##난', '##게', '잘', '했다', '##고', '하는', '##건', '아', '##니', '##지만', '드라마', '몰', '##입', '해', '##서', '생', '##각', '해', '##보', '##니', '끔', '##찍', '##하', '##네']\n",
            "tokens_a :  ['맥', '##락', '을', '찾', '##을', '##수', '없', '##어', '아', '너', '##무', '##했다', '보통', '첫', '회', '##보', '고', '나', '##중', '을', '볼', '##지', '##말', '##지', '결', '##정', '하는', '##데', '시', '##작', '이', '영', '자', '##극', '적', '인', '장', '##면', '을', '넣', '##는다', '##고', '다', '끌', '##리는', '##건', '아닌', '##데', '스', '##토', '##리', '가', '있어', '##야', '자', '##극', '적', '##인', '##것', '##도', '시', '##청', '##자', '한', '##테', '확', '와', '닿', '##지', '이런', '앞', '##뒤', '없는', '스', '##토', '##리', '는', '처음', '본', '##다']\n",
            "tokens_a :  ['참', '##나', '너', '##무', '큰', '걸', '기', '##대', '##했', '##나', '##보다', '로', '맨', '스', '##릴', '##러', '어', '##디', '##가']\n",
            "tokens_a :  ['더', '봐', '##야', '알', '##겠', '##지만', '회', '부', '##터', '무', '##슨', '스', '##토', '##리', '가', '이', '##러', '##냐', '난', '페', '##하', '라고', '하', '##길', '##래', '음', '대한', '##제', '##국', '컨', '##셉', '인', '##가', '하고', '봤', '##는데', '무', '##슨', '페', '##하', '가', '스', '##마', '##트', '##폰', '쓰', '##고', '앉', '##아', '잇', '[UNK]']\n",
            "tokens_a :  ['어', '##떻', '##게', '최', '##진', '##혁', '이', '된다', '##는', '##거', '##야', '진', '##짜']\n",
            "tokens_a :  ['아', '##무', '리', '##살', '을', '빼', '##면', '잎', '##뻐', '##지', '고', '멋', '##있', '##어진', '##다', '##지만', '나', '##왕', '식', '이', '살', '##빼', '##고', '##나', '##면', '최', '##진', '##혁', '이라', '##니']\n",
            "tokens_a :  ['무', '##슨', '내', '##용', '이']\n",
            "tokens_a :  ['옛', '##날', '궁', '드라마', '생', '##각', '하며', '딸', '이', '##랑', '같이', '보다', '화', '##들', '##짝', '내', '용', '##이', '인', '##질', '도', '##박', '폭', '##력', '내', '##연', '관', '##계', '음', '##독', '살', '##인', '[UNK]']\n",
            "tokens_a :  ['연', '##기', '좋', '##다']\n",
            "tokens_a :  ['헬', '##로', '의', '심', '##정', '이', '이', '##해', '가', '감']\n",
            "tokens_a :  ['이', '##겨', '##서', '돈', '받', '##을', '차', '##례', '아', '##니', '##었', '##나', '돈', '은', '받고', '쳐', '들어', '##가지', '아', '##깝']\n",
            "tokens_a :  ['연', '##기', '가', '좀', '오', '바', '한다', '좀', '자', '##연', '스', '##럽', '##게', '스', '##테', '##파', '##니', '야']\n",
            "tokens_a :  ['본', '##방', '사', '##운드', '도', '이상', '##하', '##더', '##니', '스', '##브', '##스', '왜', '그', '##래', '노', '##답']\n",
            "tokens_a :  ['영', '##상', '소', '##리', '안', '나', '##와', '##요', '올', '##리고', '확인', '좀']\n",
            "tokens_a :  ['힘', '##내']\n",
            "tokens_a :  []\n",
            "\n",
            "CPU times: user 708 ms, sys: 116 ms, total: 824 ms\n",
            "Wall time: 1.22 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QIxGomr3Gi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(sequences.shape)\n",
        "# print(df_ep_sample['okt_token_str'][:2])\n",
        "# display(sequences[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4cuU5Ql8psJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # for neural net\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell # jupyter에서 마지막 값만 출력하는게 아니라 모든 출력값을 매번 연속적으로 출력\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # all, last, last_expr, none (기본값은 'last_expr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zp_3jOZ816T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU 사용 setting\n",
        "device=torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uaF3knUfY_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BertTokenization 한 Embedding + Target 변수 (Series를 numpy로 변환하면 (x,) 형태의 출력이기 때문에 열 추가를 위해 reshape 해줌)\n",
        "# all_lines = np.hstack((sequences, df_ep_sample['target'].to_numpy().reshape(-1,1)))\n",
        "# all_lines.shape\n",
        "\n",
        "X = sequences\n",
        "Y = df_ep_sample['target'].to_numpy()\n",
        "# Y = df_ep_sample['target'].to_numpy().reshape(-1,1)\n",
        "\n",
        "# Train & Test Set 분리\n",
        "train_temp_x, test_x, train_temp_y, test_y = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_masks, X, random_state=42, test_size=0.15)\n",
        "# print(train_temp_x[:1])\n",
        "# print(train_masks[:1])\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_temp_x, train_temp_y, test_size=0.15, random_state=42)\n",
        "train_masks, val_masks, _, _ = train_test_split(train_masks, train_temp_x, random_state=42, test_size=0.15)\n",
        "# print(train_x[:1])\n",
        "# print(train_masks[:1])\n",
        "\n",
        "# print(train_x.shape, train_y.shape, val_x.shape, val_y.shape, test_x.shape, test_y.shape)\n",
        "# print(train_masks.shape, val_masks.shape, test_masks.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRRkZ_rGECBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.unique(train_x)\n",
        "# np.unique(train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DZFwNyQ1Q0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset을 상속한 TensorDataset은 train data x와 레이블 y를 묶어놓은 컨테이너로 tensor만 전달 가능함\n",
        "# X는 torch.long 형태의 텐서로, y는 torch.float 타입의 텐서로 입력하여 pytorch에서 연산할 수 있는 기본 구조로 변경하여 train_dataset으로 할당\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_x,dtype=torch.long), torch.tensor(train_masks,dtype=torch.long), torch.tensor(train_y,dtype=torch.float))\n",
        "val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_x,dtype=torch.long), torch.tensor(val_masks,dtype=torch.long), torch.tensor(val_y,dtype=torch.float))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_x,dtype=torch.long), torch.tensor(test_masks,dtype=torch.long), torch.tensor(test_y,dtype=torch.float))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2--Gr8kLFyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# 평가 헬퍼 함수\n",
        "def bert_eval_net(net, data_loader, device=\"cuda\"):\n",
        "  # Dropout 및 BatchNorm 무효화\n",
        "  net.eval()\n",
        "  ys = []\n",
        "  ypreds = []\n",
        "\n",
        "  eval_losse = 0\n",
        "  eval_acc = 0\n",
        "\n",
        "  running_loss = 0.\n",
        "  running_acc = 0.\n",
        "\n",
        "  for x, m, y in data_loader:\n",
        "    x=x.to(device)\n",
        "    m=m.to(device)\n",
        "    y=y.to(device)\n",
        "    with torch.no_grad():\n",
        "      y_pred = net(x, token_type_ids=None, attention_mask=m, labels=None)  # forward\n",
        "    y_pred = (y_pred >0.5).float() # bool tensor로 loss 계산이 안되어 float로 변경\n",
        "    ys.append(y)\n",
        "    ypreds.append(y_pred)\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(y_pred.view_as(y),y) \n",
        "    running_loss += loss.item() # loss calculate\n",
        "\n",
        "    running_acc += (y_pred.view_as(y)== y.byte()).sum().item()/ len(x) # 예측 결과 확인(y는 FloatTensor이므로 Byte Tensor로 변환한 후에 비교)\n",
        "\n",
        "    eval_losses = running_loss/len(data_loader)\n",
        "    eval_acc = running_acc/len(data_loader)\n",
        "\n",
        "  # 미니 배치 단위로 예측 결과 등을 하나로 모음\n",
        "  ys = torch.cat(ys)\n",
        "  ypreds = torch.cat(ypreds)\n",
        "\n",
        "  return ys, ypreds, eval_losses, eval_acc\n",
        "\n",
        "# Train 함수\n",
        "def bert_train_net(net, train_loader, val_loader, optimizer, device=\"cuda\"):\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "  val_losses = []\n",
        "  val_acc = []\n",
        "\n",
        "  tq = tqdm_notebook(range(training_epochs))\n",
        "\n",
        "  for epoch in tq:\n",
        "    running_loss = 0.\n",
        "    running_acc = 0.\n",
        "\n",
        "    # 신경망을 훈련 모드로 설정\n",
        "    net.train()\n",
        "    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
        "    \n",
        "    print(\"Before loop in bert_train_net.................\")\n",
        "    # iteration 1회에 train_loader의 batch_size (여기서는 128)만큼씩 읽어와 한꺼번에 batch처리 batch_size * i (여기서는 i = 482) 가 전체 train data set의 크기가 될때까지 loop\n",
        "    for i,(x, m, y) in tk0:\n",
        "      x=x.to(device) # len(x)는 batch_size\n",
        "      m=m.to(device)\n",
        "      y=y.to(device)\n",
        "\n",
        "      loss = net(x, token_type_ids=None, attention_mask=m, labels=y)  # forward\n",
        "\n",
        "      optimizer.zero_grad() # step과 zero_grad는 쌍을 이루는 것이라고 생각하면 됨 # optimizer의 gradient를 0으로 초기화\n",
        "      loss.backward() # backpropagation\n",
        "      optimizer.step() # update gradients\n",
        "      running_loss += loss.item() # loss calculate\n",
        "\n",
        "    train_losses.append(running_loss/len(train_loader))\n",
        "    train_acc.append(running_acc/len(train_loader))\n",
        "\n",
        "    torch.save(model.state_dict(), OUTPUT_MODEL_FILE)\n",
        "    \n",
        "    eval_ys, eval_ypreds, eval_losses, eval_acc = bert_eval_net(net, val_loader, device)\n",
        "    # val_ypreds = val_ypreds > 0.5\n",
        "    \n",
        "    # print(\" type(val_ypreds) is \", type(val_ypreds) , \"| type(val_ys)  is \", type(val_ys))\n",
        "    # val_acc.append((val_ypreds.view_as(val_ys) == val_ys.to(device).byte()).sum().item()/len(val_ys))\n",
        "    val_losses.append(eval_losses)\n",
        "    val_acc.append(eval_acc)\n",
        "    print(\"epoch: {}/{} | avg_train_loss: {:.4f} | avg_train_acc: {:.4f}  |  avg_val_loss: {:.4f} |  avg_val_acc: {:.4f}\".format(epoch, training_epochs, train_losses[-1], train_acc[-1], val_losses[-1], val_acc[-1]))\n",
        "\n",
        "  loss_acc_graph(train_losses, val_losses, train_acc, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMkB5nm7GoA9",
        "colab_type": "code",
        "outputId": "e449a3e4-0e09-4a04-e5c5-c22045be9913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# BertForSequenceClassification is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. \n",
        "# As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "# load pre-trained BERT model's weight in ../\n",
        "# model = BertForSequenceClassification.from_pretrained(BERT_MODEL_PATH + 'pytorch_model.bin', cache_dir=None, num_labels=len(np.unique(Y)))\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', cache_dir=None, num_labels=len(np.unique(Y)))\n",
        "\n",
        "# num_training_steps = 100\n",
        "# num_warmup_steps = 10\n",
        "# warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "# optimizer = AdamW(optimizer_grouped_parameters, lr=lr, warmup=.1)\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\n",
        "model.to(device) # GPU 연산을 위해 cuda로 전송\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "bert_train_net(model, train_loader, val_loader, optimizer, device)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.83b0fa3d7f1ac0e113ad300189a938c6f14d0588a4200f30eef109d0a047c484\n",
            "INFO:transformers.configuration_utils:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-a19de4ed4289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# GPU 연산을 위해 cuda로 전송\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 11.17 GiB total capacity; 10.69 GiB already allocated; 70.81 MiB free; 104.40 MiB cached)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWY-U3lCOJYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "ys, ypreds,_,_ = bert_eval_net(net, test_loader, loss_fn ,device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QMVH-Ei_Yl0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}