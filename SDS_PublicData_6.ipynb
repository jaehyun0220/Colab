{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDS_PublicData#6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaehyun0220/Colab/blob/master/SDS_PublicData_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFW4i1H_63WG",
        "colab_type": "text"
      },
      "source": [
        "## 3조. 건강검진 데이터를 활용한 치아우식증 발생 예측\n",
        "#### # Ver 6. Keras 활용 MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUuUBLq6o5n",
        "colab_type": "code",
        "outputId": "90a237b5-2410-495c-d49a-c924adc48425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Auth 인증 및 Google Drive 활용 Data load\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-RQ4wm563Cr",
        "colab_type": "text"
      },
      "source": [
        "#### #2. 작업환경 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjHIIUYEXh6n",
        "colab_type": "code",
        "outputId": "337a7829-3092-4372-81d5-c51893119108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "# woe package install\n",
        "! pip install woe"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting woe\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/32/ba4d592dfef45338ee04ca90c37b4f3aa345bdeafcad4dcbf654ad0b14c2/woe-0.1.4-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from woe) (1.17.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from woe) (1.3.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from woe) (3.1.1)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.6/dist-packages (from woe) (0.25.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->woe) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->woe) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->woe) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->woe) (2.4.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.2->woe) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->woe) (41.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0.0->woe) (1.12.0)\n",
            "Installing collected packages: woe\n",
            "Successfully installed woe-0.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0KCYMzm7Iv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "00349e27-b6b7-4b4f-c81a-e8eeae817200"
      },
      "source": [
        "# 기본 라이브러리 로드\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "#데이터 전처리 관련 라이브러리 로드\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "#모델 알고리즘 로드\n",
        "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "\n",
        "#차원축소 알고리즘 로드\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Calculate IV Setting\n",
        "import pandas.core.algorithms as algos\n",
        "from pandas import Series\n",
        "import scipy.stats.stats as stats\n",
        "import traceback\n",
        "import string\n",
        "import woe\n",
        "from woe.eval import plot_ks\n",
        "\n",
        "# Deep Learning Model 로드\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from keras import optimizers\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "# Pytorch 로드\n",
        "import torch\n",
        "import torch.nn as nn # for neural net # nn: Deep learning model에 필요한 모듈이 모아져 있는 패키지 ex) nn.Linear(128, 128), nn.ReLU()\n",
        "import torch.nn.functional as F # F: nn과 같은 모듈이 모아져 있지만 함수의 input으로 반드시 연산이 되어야 하는 값을 받습니다. ex) F.linear(X, 128, 128), R.relu(X)\n",
        "import torch.optim as optim # 학습에 관련된 optimizing method가 있는 패키지\n",
        "import torch.utils.data as data_utils # batch generator 등 학습 데이터에 관련된 패키지\n",
        "\n",
        "#HyperParameter Tuning을 위한 라이브러리 로드\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#모델 평가를 위한 라이브러리 로드\n",
        "from sklearn import metrics, model_selection\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
        "\n",
        "#수학 & 통계 관련 라이브러리 로드\n",
        "import scipy.stats as st\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl  # 기본 설정 만지는 용도\n",
        "import matplotlib.pyplot as plt  # 그래프 그리는 용도\n",
        "import matplotlib.font_manager as fm  # 폰트 관련 용도\n",
        "\n",
        "\n",
        "#Configure Visualization Defaults\n",
        "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
        "%matplotlib inline\n",
        "mpl.style.use('ggplot')\n",
        "sns.set_style('white')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUDEfenP7Rdl",
        "colab_type": "text"
      },
      "source": [
        "#### #3.사용할 사용자 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igQsIjbj7ON5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def auc_graph(roc_auc, fpr, tpr):\n",
        "  plt.title('Receiver Operating Characteristic')\n",
        "  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "  plt.legend(loc = 'lower right')\n",
        "  plt.plot([0, 1], [0, 1],'r--')\n",
        "  plt.xlim([0, 1])\n",
        "  plt.ylim([0, 1])\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.show()\n",
        "\n",
        "# Define a binning function for continous independent variables\n",
        "max_bin = 20\n",
        "force_bin = 3\n",
        "\n",
        "def mono_bin(Y, X, n = max_bin):\n",
        "  df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "  justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "  notmiss = df1[['X','Y']][df1.X.notnull()]\n",
        "  r = 0\n",
        "  while np.abs(r) < 1:\n",
        "    try:\n",
        "      global d1\n",
        "      global d2\n",
        "      d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
        "      d2 = d1.groupby('Bucket', as_index=True)\n",
        "      r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
        "      n = n - 1 \n",
        "    except Exception as e:\n",
        "      n = n - 1\n",
        "      break\n",
        "      \n",
        "  if len(d2) == 1:\n",
        "    n = force_bin         \n",
        "    bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
        "    if len(np.unique(bins)) == 2:\n",
        "        bins = np.insert(bins, 0, 1)\n",
        "        bins[1] = bins[1]-(bins[1]/2)\n",
        "    d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
        "    d2 = d1.groupby('Bucket', as_index=True)\n",
        "\n",
        "  d3 = pd.DataFrame({},index=[])\n",
        "  d3[\"MIN_VALUE\"] = d2.min().X\n",
        "  d3[\"MAX_VALUE\"] = d2.max().X\n",
        "  d3[\"COUNT\"] = d2.count().Y\n",
        "  d3[\"EVENT\"] = d2.sum().Y\n",
        "  d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
        "  d3=d3.reset_index(drop=True)\n",
        "\n",
        "  if len(justmiss.index) > 0:\n",
        "      d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "      d4[\"MAX_VALUE\"] = np.nan\n",
        "      d4[\"COUNT\"] = justmiss.count().Y\n",
        "      d4[\"EVENT\"] = justmiss.sum().Y\n",
        "      d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "      d3 = d3.append(d4,ignore_index=True)\n",
        "\n",
        "  d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "  d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "  d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "  d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "  d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "  d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "  d3[\"VAR_NAME\"] = \"VAR\"\n",
        "  d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
        "  d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "  d3.IV = d3.IV.sum()\n",
        "\n",
        "  return(d3)\n",
        "  \n",
        "# Define a binning function for categorical independent variables\n",
        "def char_bin(Y, X):\n",
        "  df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "  justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "  notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
        "  df2 = notmiss.groupby('X',as_index=True)\n",
        "\n",
        "  d3 = pd.DataFrame({},index=[])\n",
        "  d3[\"COUNT\"] = df2.count().Y\n",
        "  d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
        "  d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
        "  d3[\"EVENT\"] = df2.sum().Y\n",
        "  d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
        "\n",
        "  if len(justmiss.index) > 0:\n",
        "    d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "    d4[\"MAX_VALUE\"] = np.nan\n",
        "    d4[\"COUNT\"] = justmiss.count().Y\n",
        "    d4[\"EVENT\"] = justmiss.sum().Y\n",
        "    d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "    d3 = d3.append(d4,ignore_index=True)\n",
        "\n",
        "  d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "  d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "  d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "  d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "  d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "  d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "  d3[\"VAR_NAME\"] = \"VAR\"\n",
        "  d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
        "  d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "  d3.IV = d3.IV.sum()\n",
        "  d3 = d3.reset_index(drop=True)\n",
        "\n",
        "  return(d3)\n",
        "\n",
        "# Calculate Information Values\n",
        "def calc_iv_all(df1, target):\n",
        "    \n",
        "  stack = traceback.extract_stack()\n",
        "  filename, lineno, function_name, code = stack[-2]\n",
        "  vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
        "  final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
        "\n",
        "  x = df1.dtypes.index\n",
        "  count = -1\n",
        "\n",
        "  for i in tqdm_notebook(x):\n",
        "    if i.upper() not in (final.upper()):\n",
        "      if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
        "        conv = mono_bin(target, df1[i])\n",
        "        conv[\"VAR_NAME\"] = i\n",
        "        count = count + 1\n",
        "      else:\n",
        "        conv = char_bin(target, df1[i])\n",
        "        conv[\"VAR_NAME\"] = i            \n",
        "        count = count + 1\n",
        "\n",
        "      if count == 0:\n",
        "        iv_df = conv\n",
        "      else:\n",
        "        iv_df = iv_df.append(conv,ignore_index=True)\n",
        "\n",
        "  iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
        "  iv = iv.reset_index()\n",
        "  return(iv_df,iv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa1S7ugaqFtt",
        "colab_type": "text"
      },
      "source": [
        "##### #3-1. Keras를 위한 평가 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN-h8mLoqEuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep Learning 평가를 위한 평가 함수 정의\n",
        "from keras import backend as K\n",
        "\n",
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kde9-tsZ7vhS",
        "colab_type": "text"
      },
      "source": [
        "#### #4.원천 데이터 load 및 seed 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69HLpZvl7v0v",
        "colab_type": "code",
        "outputId": "865429ec-ca7d-40b6-d87d-103315e5030e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "set_random_seed = 2580 # seed 지정\n",
        "target_nm = 'dental_carries' # 타겟 변수 지정\n",
        "\n",
        "#### 2013년 기준 #########################################################################################################################################\n",
        "df_raw_2013 = pd.read_csv('../gdrive/My Drive/sds/data/NHIS_OPEN_GJ_2013_eng.csv', encoding = 'euc-kr')\n",
        "\n",
        "# 컬럼명 내 불필요한 공백 및 특수문자 제거\n",
        "df_raw_2013.rename(columns=lambda x: re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》 ]', '', x), inplace=True)\n",
        "\n",
        "# 구강검진 결과가 있는 데이터셋만 load\n",
        "df_data = df_raw_2013[df_raw_2013['examine_mouth']==1]\n",
        "\n",
        "# 분석과 관련 없는 변수 제거\n",
        "del_cols = ['baseyear', 'id', 'data_open_date','examine_mouth']\n",
        "df_data.drop(columns=del_cols, axis=1, inplace=True)\n",
        "\n",
        "# 치아우식증 결과가 있는 데이터만 끌고 옴\n",
        "df_data = df_data[~(df_data['dental_carries'].isnull()) & (df_data['dental_carries'] != 2)]\n",
        "\n",
        "# null 포함 데이터 모두 삭제 \n",
        "df_data.dropna(how='any', inplace=True)\n",
        "display(df_data.info())\n",
        "print(len(df_data))\n",
        "\n",
        "print(df_data[target_nm].value_counts())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 380657 entries, 3 to 999998\n",
            "Data columns (total 30 columns):\n",
            "sex                      380657 non-null int64\n",
            "ageband                  380657 non-null int64\n",
            "province                 380657 non-null int64\n",
            "height                   380657 non-null int64\n",
            "weight                   380657 non-null int64\n",
            "waist                    380657 non-null int64\n",
            "sight_l                  380657 non-null float64\n",
            "sight_r                  380657 non-null float64\n",
            "hearing_l                380657 non-null float64\n",
            "hearing_r                380657 non-null float64\n",
            "bp_systolic              380657 non-null int64\n",
            "bp_diastolic             380657 non-null int64\n",
            "bs_before                380657 non-null int64\n",
            "tot_cholesterol          380657 non-null int64\n",
            "triglycerides            380657 non-null int64\n",
            "HDL_cholesterol          380657 non-null int64\n",
            "LDL_cholesterol          380657 non-null float64\n",
            "hemoglobin               380657 non-null float64\n",
            "piu                      380657 non-null float64\n",
            "serum_creatinine         380657 non-null float64\n",
            "AST                      380657 non-null int64\n",
            "ALT                      380657 non-null int64\n",
            "GammaGTP                 380657 non-null int64\n",
            "smoking                  380657 non-null float64\n",
            "drinking                 380657 non-null float64\n",
            "dental_carries           380657 non-null float64\n",
            "missing_tooth            380657 non-null float64\n",
            "dental_abrasion          380657 non-null float64\n",
            "wisdom_teeth_abnormal    380657 non-null float64\n",
            "plaque                   380657 non-null float64\n",
            "dtypes: float64(15), int64(15)\n",
            "memory usage: 90.0 MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "380657\n",
            "0.0    288017\n",
            "1.0     92640\n",
            "Name: dental_carries, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH0vKa9Y-frQ",
        "colab_type": "text"
      },
      "source": [
        "#### #5. 데이터샘플링 및 Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvLVC4de8VL3",
        "colab_type": "code",
        "outputId": "143a4b2b-2410-40f7-8e67-fce952677450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# 치아우식 0과 1의 값을 9만건씩 총 18만건 추출\n",
        "df_sample = df_data.groupby(target_nm).apply(lambda x: x.sample(n=50000, random_state=set_random_seed)).copy()\n",
        "df_sample.reset_index(drop=True, inplace=True)\n",
        "print(df_sample[target_nm].value_counts())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0    50000\n",
            "0.0    50000\n",
            "Name: dental_carries, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uj02jC6EECx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 명목형 변수 변경\n",
        "category_features = ['sex', 'ageband', 'height', 'weight','province', 'hearing_l', 'hearing_r', 'smoking','drinking', 'piu']\n",
        "\n",
        "for col in df_data.columns:\n",
        "  if col in category_features:\n",
        "    df_sample[col] = df_sample[col].astype(object)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlz7eUbdETWb",
        "colab_type": "text"
      },
      "source": [
        "##### #5-1.내부 명목형 변수 묶기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzmle99OEYot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 명목형 변수의 범주화\n",
        "# 성별코드 변환\n",
        "df_sample[\"C_sex\"] = df_sample[\"sex\"].apply(lambda x:  'Male' if x == 1 else 'Female')\n",
        "\n",
        "# 연령대코드 5세 단위 변환\n",
        "min_age_code = df_sample[\"ageband\"].min()\n",
        "\n",
        "df_sample[\"C_ageband\"] = df_sample[\"ageband\"].apply(lambda x:  (x-1)*5 + 20 if min_age_code == 1 else (x-1)*5).astype(object)\n",
        "\n",
        "# 시도코드 변환\n",
        "df_sample[\"C_province\"] = np.where(df_sample['province'] == 11, 'Seoul', \n",
        "                            np.where(df_sample['province']== 26, 'Busan', \n",
        "                              np.where(df_sample['province'] == 27, 'Daegu', \n",
        "                                np.where(df_sample['province'] == 28, 'Incheon', \n",
        "                                  np.where(df_sample['province'] == 29, 'Kwangju', \n",
        "                                    np.where(df_sample['province'] == 30, 'Daejeon', \n",
        "                                      np.where(df_sample['province'] == 31, 'Ulsan', \n",
        "                                        np.where(df_sample['province'] == 36, 'Sejong', \n",
        "                                          np.where(df_sample['province'] == 41, 'Gyeonggi', \n",
        "                                            np.where(df_sample['province'] == 42, 'Gangwon', \n",
        "                                              np.where(df_sample['province'] == 43, 'Chungbuk', \n",
        "                                                np.where(df_sample['province'] == 44, 'Chungnam', \n",
        "                                                  np.where(df_sample['province'] == 45, 'Jeonbuk', \n",
        "                                                    np.where(df_sample['province'] == 46, 'Jeonnam', \n",
        "                                                      np.where(df_sample['province'] == 47, 'Gyungbuk', \n",
        "                                                        np.where(df_sample['province'] == 48, 'Gyungnam', \n",
        "                                                          np.where(df_sample['province'] == 49, 'Jeju', 'Err')\n",
        "                                      ))))))))))))))))\n",
        "\n",
        "# 청력좌  변환\n",
        "df_sample[\"C_hearing_l\"] = df_sample[\"hearing_l\"].apply(lambda x:  'Normal' if x == 1 else 'Abnormal')\n",
        "\n",
        "# 청력우  변환\n",
        "df_sample[\"C_hearing_r\"] = df_sample[\"hearing_r\"].apply(lambda x:  'Normal' if x == 1 else 'Abnormal')\n",
        "\n",
        "# 요단백  변환\n",
        "df_sample[\"C_piu\"] = df_sample[\"piu\"].apply(lambda x:  'Negative' if x == 1 else 'Positive')\n",
        "\n",
        "# 흡연상태  변환\n",
        "df_sample[\"C_smoking\"] = df_sample[\"smoking\"].apply(lambda x:  'NonSmoking' if x == 1 else ('StopSmoking' if x==2 else 'Smoking'))\n",
        "\n",
        "# 음주여부  변환\n",
        "df_sample[\"C_drinking\"] = df_sample[\"drinking\"].apply(lambda x:  'NonDrinking' if x == 0 else 'Drinking')\n",
        "\n",
        "# 변환 명목형 원 컬럼 삭제\n",
        "del_obj_trans_cols = ['sex', 'ageband', 'province', 'hearing_l', 'hearing_r', 'piu', 'smoking', 'drinking']\n",
        "df_sample.drop(columns=del_obj_trans_cols, axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N3rwa_DE5Py",
        "colab_type": "text"
      },
      "source": [
        "##### #5-2. 내부 수치형 변수 구간화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox2naYoPE5mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 수치형 변수 명목화\n",
        "# BMI 변수 추가\n",
        "df_sample[\"N_BMI\"] = (df_sample[\"weight\"]/(df_sample[\"height\"]*df_sample[\"height\"])).astype(float)\n",
        "\n",
        "# 공복 혈당 \n",
        "# 100 이하는 정상/ 100 ~ 125는 '공복혈당장애 - 당뇨 전단계'/ 125 이상은 당뇨\n",
        "# 당뇨병의 중간인 100~125 mg/dL가 나오거나 \n",
        "\n",
        "df_sample[\"N_diabetes\"] = np.where(df_sample['bs_before'].between(0, 100, inclusive=True), 'Normal', \n",
        "                               np.where(df_sample['bs_before'].between(100, 124.9, inclusive=True), 'Prediabetes', \n",
        "                                        'Diabetes')\n",
        "                              )\n",
        "\n",
        "# HDL 콜레스테롤\n",
        "# HDL 콜레스테롤 수치가 남자에서 40 mg/dL (1.0 mmol/L) 이하 또는 여자에서 50 mg/dL (1.3 mmol/L) 이하일 경우에는 다른 위험인자와 독립적으로 심장 질환의 위험도가 증가\n",
        "# HDL 콜레스테롤 수치가 남자에서 40-50 mg/dL (1.0-1.3 mmol/L) 그리고 여자에서 50-59 mg/dL (1.3-1.5 mmol/L)인 경우에는 심장 질환의 평균위험도와 연관\n",
        "# 보통 HDL 콜레스테롤 수치가 60 mg/dL (1.55 mmol/L) 또는 그 이상일 경우에는 심장 질환 평균 위험도보다 낮음\n",
        "# National Cholesterol Education Panel Adult Treatment Guidelines에 따르면 HDL 콜레스테롤 수치가 60 mg/dL 이상일 경우 심장 질환에서 보호되고 음성 위험 인자로서 치료되어야 함\n",
        "df_sample[\"N_HDL\"] = np.where(\n",
        "                    (df_sample['C_sex'] == \"Female\") & (df_sample['HDL_cholesterol']<= 50), 'HighRisk', \n",
        "                       np.where(\n",
        "                         (df_sample['C_sex']== \"Female\") & (df_sample['HDL_cholesterol'].between(50, 59.9, inclusive=True)), 'MediumRisk', \n",
        "                           np.where(\n",
        "                               (df_sample['C_sex']== \"Male\") & (df_sample['HDL_cholesterol']<= 40), 'HighRisk', \n",
        "                                      np.where(\n",
        "                                          (df_sample['C_sex']== \"Male\") & (df_sample['HDL_cholesterol'].between(40, 49.9, inclusive=True)), 'MediumRisk', 'Normal')\n",
        "                          )))\n",
        "# LDL 콜레스테롤(혈중 모든 콜레스테롤 중 LDL 콜레스테롤이 심장질환에 대한 위험도를 확인하는데 가장 중요한 지표)\n",
        "# 대부분의 치료 결정이 LDL 수치를 토대로 이루어지기 때문에 이 검사를 통해 식이요법 또는 운동처방의 효과를 감시하거나 지질감소 약물을 처방하는 것이 유용한지에 대한 평가\n",
        "# 100 mg/dL (2.59mmol/L) 미만 – 최적\n",
        "# 100-129 mg/dL (2.59-3.34 mmol/L) – 최적에 인접\n",
        "# 130-159 mg/dL (3.37-4.12 mmol/L) – 상한 경계성\n",
        "# 160-189 mg/dL (4.15-4.90 mmol/L) – 높음\n",
        "# 190 mg/dL (4.90 mmol) 이상 – 매우 높음\n",
        "df_sample[\"N_LDL\"] = np.where(df_sample['LDL_cholesterol']<100, 'Good', \n",
        "                    np.where(df_sample['LDL_cholesterol'].between(100, 122.9, inclusive=True), 'NearGood', \n",
        "                             np.where(df_sample['LDL_cholesterol'].between(130, 159.9, inclusive=True), 'Upperbound', \n",
        "                                      np.where(df_sample['LDL_cholesterol'].between(160, 189.9, inclusive=True), 'High', 'VeryHigh')\n",
        "                              )))\n",
        "\n",
        "# 트리글리세라이드\n",
        "# 지방의 한 형태로서 몸의 주요 에너지원, 트리글리세라이드가 증가하는 것은, 이유가 분명하지 않으나 심혈관 질환으로 진행될 위험의 증가와 관련\n",
        "# 일부 인자들 즉, 운동 부족, 과체중, 흡연, 과음 및 당뇨와 신질환 등의 질병 상태가 고트리글리세라이드혈증 및 심혈관 질환 위험도 증가에 기여할 수 있음\n",
        "# 성인에서는 트리글리세라이드 결과가 아래와 같이 나뉘어진다.\n",
        "# 150 mg/dL (1.7 mmol/L) 미만: 바람직\n",
        "# 150-199 mg/dL (1.7-2.2 mmol/L): 경계성증가\n",
        "# 200-499 mg/dL (2.3-5.6 mmol/L): 증가: \n",
        "# 500 mg/dL (5.6 mmol/L) 이상: 매우 증가\n",
        "# 이 수치는 공복시 트리글리세라이드 수치에 기준합니다.\n",
        "df_sample[\"N_TRI\"] = np.where(df_sample['triglycerides']<150, 'Good', \n",
        "                    np.where(df_sample['triglycerides'].between(150, 199.9, inclusive=True), 'Check', \n",
        "                             np.where(df_sample['triglycerides'].between(200, 499.9, inclusive=True), 'Increased', 'VeryIncreased')\n",
        "                              ))\n",
        "\n",
        "# 혈색소\n",
        "# 성인의 데시리터(100밀리리터) 당 12그램에서 18그램 정도: 정상치\n",
        "# 18그램 이상: 폐질환 등 기타 이상\n",
        "# 12그램 미만: 빈혈\n",
        "df_sample[\"N_HEMO\"] = np.where(df_sample['hemoglobin'].between(12, 17.9, inclusive=True), 'Normal', \n",
        "                    np.where(df_sample['hemoglobin']>=18, 'Abnormal', 'Anemia'))\n",
        "\n",
        "# 요단백: 소변으로 빠져나가는 잉여의 단백질을 검출하기 위해, 신장 기증을 평가하고 모니터하는 것을 돕기 위해,  그리고 신장 손상을 검출하기 위해 검사\n",
        "# 요단백은 보통 소변에서 검출 되지 않음\n",
        "\n",
        "# 혈중 크레아티닌 농도의 \n",
        "# 정상범위는 0.50~1.4 mg/dL 입니다\n",
        "# 근육량에 비례하는 검사결과이므로 여성보다는 남성에게서 약간 높은 수치가 나타나고, 식사나 운동이 결과에 영향이 거의 미치지 않습니다. 지속적으로 많은 양의 육식을 섭취한 경우에는 크레아티닌 농도가 높게 측정됩니다.\n",
        "df_sample[\"N_CRE\"] = np.where(df_sample['serum_creatinine'].between(0.5, 1.4, inclusive=True), 'Normal', 'Abnormal')\n",
        "\n",
        "# 혈청지오티AST:\n",
        "#  간기능을 평가하는 기초검사항목으로서 알코올성 간장애나 만성 간질환에서 주로 증가한다.\n",
        " \n",
        "\n",
        "# 혈청지오티ALT:\n",
        "# 간기능을 평가하는 기초검사항목으로서 급성 간염 시 주로 증가한다.\n",
        "# 증가: 간질환, 심근경색, 지방간, 비만\n",
        "\n",
        "\n",
        "# 감마지티피\n",
        "# 간장세포나 담낭세포가 파괴되면 감마지티피가 혈액속으로 누출되어 수치가 높아짐\n",
        "# 남자는 50IU 이하, 여자는 32IU이하가 정상\n",
        "# 100이하면 음주 조절을 통해 조정 가능하나 100 이상이면 지방간이 진행되고 있을 가능성이 높음\n",
        "# 200 이상이면 담석이나 담도암등으로 담도가 막혀있을 가능성이 높음\n",
        "# 500 이상이면 황달\n",
        "df_sample[\"N_GTP\"] = np.where((df_sample['GammaGTP'] >= 500), 'Jaundice', \n",
        "                       np.where(df_sample['GammaGTP']>=200, 'Abnormal', \n",
        "                           np.where(df_sample['GammaGTP']>= 100, 'PossibleAbnormal', \n",
        "                             np.where((df_sample['C_sex']== \"Male\") & (df_sample['GammaGTP']<=50), 'Normal',\n",
        "                              np.where((df_sample['C_sex']== \"Female\") & (df_sample['GammaGTP']<=32), 'Normal', 'Check')\n",
        "                          ))))\n",
        "\n",
        "# 변환 수치형 원 컬럼 삭제\n",
        "del_num_trans_cols = ['bs_before', 'HDL_cholesterol', 'LDL_cholesterol', 'triglycerides', 'hemoglobin', 'serum_creatinine', 'GammaGTP']\n",
        "df_sample.drop(columns=del_num_trans_cols, axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZjaLDYILxSx",
        "colab_type": "text"
      },
      "source": [
        "##### #5-3. 내부 수치형 변수 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwlAxb6ZKdgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 수치형 변수들 중 분포 이슈로 log 값을 취할 필요 있는 변수들 추출\n",
        "num_ln_target_features = ['sight_l', 'sight_r', 'AST','ALT']\n",
        "\n",
        "# Feature Engineering의 일환으로 Log 값 취한 뒤 Normalization을 하여 변수명 + LN (lognorm)으로 열 추가 후 그래프 다시 그림\n",
        "num_ln_cols = list(map(lambda x: \"LN_\"+str(x), num_ln_target_features))\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('log_scaler', FunctionTransformer(np.log1p, validate=True)), # pipeline 내 log transformation을 위해 Function Transfomer 사용\n",
        "        ('normalizer', MinMaxScaler()),\n",
        "    ])\n",
        "\n",
        "piped_np = num_pipeline.fit_transform(df_sample[num_ln_target_features])\n",
        "piped_df = pd.DataFrame(piped_np, columns=num_ln_cols)\n",
        "\n",
        "piped_df.head()\n",
        "\n",
        "df_sample.drop(columns=num_ln_target_features, axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx43qNb6L9e3",
        "colab_type": "code",
        "outputId": "9e948e9c-badd-419e-9080-b032d6a49ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "print(\"Before Re-indexing: Data count is \", len(df_sample))\n",
        "df_sample.drop_duplicates()\n",
        "df_sample.reset_index(inplace=True, drop=True)\n",
        "print(\"After Re-indexing: Data count is \", len(df_sample))\n",
        "\n",
        "df_fe = pd.concat([df_sample, piped_df], axis=1)\n",
        "df_fe.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Re-indexing: Data count is  100000\n",
            "After Re-indexing: Data count is  100000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waist</th>\n",
              "      <th>bp_systolic</th>\n",
              "      <th>bp_diastolic</th>\n",
              "      <th>tot_cholesterol</th>\n",
              "      <th>dental_carries</th>\n",
              "      <th>missing_tooth</th>\n",
              "      <th>dental_abrasion</th>\n",
              "      <th>wisdom_teeth_abnormal</th>\n",
              "      <th>plaque</th>\n",
              "      <th>C_sex</th>\n",
              "      <th>C_ageband</th>\n",
              "      <th>C_province</th>\n",
              "      <th>C_hearing_l</th>\n",
              "      <th>C_hearing_r</th>\n",
              "      <th>C_piu</th>\n",
              "      <th>C_smoking</th>\n",
              "      <th>C_drinking</th>\n",
              "      <th>N_BMI</th>\n",
              "      <th>N_diabetes</th>\n",
              "      <th>N_HDL</th>\n",
              "      <th>N_LDL</th>\n",
              "      <th>N_TRI</th>\n",
              "      <th>N_HEMO</th>\n",
              "      <th>N_CRE</th>\n",
              "      <th>N_GTP</th>\n",
              "      <th>LN_sight_l</th>\n",
              "      <th>LN_sight_r</th>\n",
              "      <th>LN_AST</th>\n",
              "      <th>LN_ALT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>155</td>\n",
              "      <td>55</td>\n",
              "      <td>74</td>\n",
              "      <td>170</td>\n",
              "      <td>110</td>\n",
              "      <td>208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002289</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Good</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.135235</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.374972</td>\n",
              "      <td>0.318926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>155</td>\n",
              "      <td>45</td>\n",
              "      <td>67</td>\n",
              "      <td>95</td>\n",
              "      <td>65</td>\n",
              "      <td>162</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>20</td>\n",
              "      <td>Chungnam</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Good</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.357967</td>\n",
              "      <td>0.357967</td>\n",
              "      <td>0.365967</td>\n",
              "      <td>0.279401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>165</td>\n",
              "      <td>70</td>\n",
              "      <td>91</td>\n",
              "      <td>155</td>\n",
              "      <td>105</td>\n",
              "      <td>215</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>30</td>\n",
              "      <td>Incheon</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002571</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>NearGood</td>\n",
              "      <td>Increased</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Abnormal</td>\n",
              "      <td>0.302229</td>\n",
              "      <td>0.238306</td>\n",
              "      <td>0.511011</td>\n",
              "      <td>0.340812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>160</td>\n",
              "      <td>60</td>\n",
              "      <td>77</td>\n",
              "      <td>110</td>\n",
              "      <td>70</td>\n",
              "      <td>223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>50</td>\n",
              "      <td>Gyeonggi</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Smoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>Normal</td>\n",
              "      <td>MediumRisk</td>\n",
              "      <td>Upperbound</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Check</td>\n",
              "      <td>0.214731</td>\n",
              "      <td>0.214731</td>\n",
              "      <td>0.555008</td>\n",
              "      <td>0.600442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>160</td>\n",
              "      <td>55</td>\n",
              "      <td>69</td>\n",
              "      <td>129</td>\n",
              "      <td>71</td>\n",
              "      <td>140</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Gyeonggi</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Positive</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>0.002148</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>HighRisk</td>\n",
              "      <td>Good</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.238306</td>\n",
              "      <td>0.238306</td>\n",
              "      <td>0.383516</td>\n",
              "      <td>0.318926</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  height weight  waist  bp_systolic  ...  LN_sight_l  LN_sight_r    LN_AST    LN_ALT\n",
              "0    155     55     74          170  ...    0.135235    0.260671  0.374972  0.318926\n",
              "1    155     45     67           95  ...    0.357967    0.357967  0.365967  0.279401\n",
              "2    165     70     91          155  ...    0.302229    0.238306  0.511011  0.340812\n",
              "3    160     60     77          110  ...    0.214731    0.214731  0.555008  0.600442\n",
              "4    160     55     69          129  ...    0.238306    0.238306  0.383516  0.318926\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caU-WGrLNtak",
        "colab_type": "text"
      },
      "source": [
        "#### #6. 외부변수 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl_X8cosNt9D",
        "colab_type": "code",
        "outputId": "00ee21ad-87a1-4792-8799-1b94d62194e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_add1 = pd.read_csv('../gdrive/My Drive/sds/data/DentalExamineResult_2014_PortionbyTotInspector.csv', encoding = 'euc-kr')\n",
        "df_add2 = pd.read_csv('../gdrive/My Drive/sds/data/KOSIS_AgeSex_AverageDentalVisitCnt_2012.csv', encoding = 'euc-kr')\n",
        "df_add3 = pd.read_csv('../gdrive/My Drive/sds/data/KOSIS_DentalPrevalenceTrend_2012.csv', encoding = 'euc-kr')\n",
        "df_add4 = pd.read_csv('../gdrive/My Drive/sds/data/chs_12_final_from_python.csv', encoding = 'euc-kr')\n",
        "\n",
        "# Multiple Key를 사용하여 Join\n",
        "# left_on=['column_name1','column_name2'], right_on = ['column_name3','column_name4']\n",
        "print(len(df_fe))\n",
        "\n",
        "df_new = pd.merge(left=df_fe, right=df_add1, how='outer', left_on=['C_province','C_sex'], right_on = ['Province','Sex'], sort=False)\n",
        "df_new.drop(columns=['Province','Sex'], axis=1, inplace=True)\n",
        "display(df_new.head())\n",
        "print(len(df_new))\n",
        "\n",
        "df_new = pd.merge(left=df_new, right=df_add2, how='outer', left_on=['C_ageband','C_sex'], right_on = ['Ageband','Sex'], sort=False)\n",
        "df_new.drop(columns=['Ageband','Sex'], axis=1, inplace=True)\n",
        "display(df_new.head())\n",
        "print(len(df_new))\n",
        "\n",
        "df_new = pd.merge(left=df_new, right=df_add3, how='outer', left_on=['C_ageband','C_sex'], right_on = ['Ageband','Sex'], sort=False)\n",
        "df_new.drop(columns=['Ageband','Sex'], axis=1, inplace=True)\n",
        "display(df_new.head())\n",
        "print(len(df_new))\n",
        "\n",
        "df_new = pd.merge(left=df_new, right=df_add4, how='left', left_on=['C_ageband','C_sex','C_province', 'weight', 'height'], right_on = ['R_ageband','R_sex','R_province','R_weight','R_height'], sort=False)\n",
        "df_new.drop(columns=['R_ageband','R_sex','R_province','R_weight','R_height'], axis=1, inplace=True)\n",
        "display(df_new.head())\n",
        "print(len(df_new))\n",
        "\n",
        "display(df_new.info())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waist</th>\n",
              "      <th>bp_systolic</th>\n",
              "      <th>bp_diastolic</th>\n",
              "      <th>tot_cholesterol</th>\n",
              "      <th>dental_carries</th>\n",
              "      <th>missing_tooth</th>\n",
              "      <th>dental_abrasion</th>\n",
              "      <th>wisdom_teeth_abnormal</th>\n",
              "      <th>plaque</th>\n",
              "      <th>C_sex</th>\n",
              "      <th>C_ageband</th>\n",
              "      <th>C_province</th>\n",
              "      <th>C_hearing_l</th>\n",
              "      <th>C_hearing_r</th>\n",
              "      <th>C_piu</th>\n",
              "      <th>C_smoking</th>\n",
              "      <th>C_drinking</th>\n",
              "      <th>N_BMI</th>\n",
              "      <th>N_diabetes</th>\n",
              "      <th>N_HDL</th>\n",
              "      <th>N_LDL</th>\n",
              "      <th>N_TRI</th>\n",
              "      <th>N_HEMO</th>\n",
              "      <th>N_CRE</th>\n",
              "      <th>N_GTP</th>\n",
              "      <th>LN_sight_l</th>\n",
              "      <th>LN_sight_r</th>\n",
              "      <th>LN_AST</th>\n",
              "      <th>LN_ALT</th>\n",
              "      <th>A_NormalA_Result</th>\n",
              "      <th>A_NormalB_Result</th>\n",
              "      <th>A_Caution_Result</th>\n",
              "      <th>A_NeedCare_Result</th>\n",
              "      <th>A_Nutrition_Edu</th>\n",
              "      <th>A_Hygine_Edu</th>\n",
              "      <th>A_Fluoride_Edu</th>\n",
              "      <th>A_Examine_Rec</th>\n",
              "      <th>A_Care_Rec</th>\n",
              "      <th>A_Carries_Rec</th>\n",
              "      <th>A_Cure_Rec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>155</td>\n",
              "      <td>55</td>\n",
              "      <td>74</td>\n",
              "      <td>170</td>\n",
              "      <td>110</td>\n",
              "      <td>208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002289</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Good</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.135235</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.374972</td>\n",
              "      <td>0.318926</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>155</td>\n",
              "      <td>55</td>\n",
              "      <td>76</td>\n",
              "      <td>123</td>\n",
              "      <td>76</td>\n",
              "      <td>200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>60</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Abnormal</td>\n",
              "      <td>Abnormal</td>\n",
              "      <td>Positive</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002289</td>\n",
              "      <td>Normal</td>\n",
              "      <td>HighRisk</td>\n",
              "      <td>Upperbound</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.214731</td>\n",
              "      <td>0.189809</td>\n",
              "      <td>0.383516</td>\n",
              "      <td>0.279401</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>155</td>\n",
              "      <td>55</td>\n",
              "      <td>77</td>\n",
              "      <td>108</td>\n",
              "      <td>68</td>\n",
              "      <td>198</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>60</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002289</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>NearGood</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.072840</td>\n",
              "      <td>0.072840</td>\n",
              "      <td>0.399390</td>\n",
              "      <td>0.385380</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>150</td>\n",
              "      <td>60</td>\n",
              "      <td>88</td>\n",
              "      <td>110</td>\n",
              "      <td>70</td>\n",
              "      <td>270</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>55</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002667</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>Normal</td>\n",
              "      <td>High</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.238306</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.383516</td>\n",
              "      <td>0.330234</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>165</td>\n",
              "      <td>60</td>\n",
              "      <td>74</td>\n",
              "      <td>110</td>\n",
              "      <td>70</td>\n",
              "      <td>225</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>55</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002204</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Upperbound</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.357967</td>\n",
              "      <td>0.357967</td>\n",
              "      <td>0.439558</td>\n",
              "      <td>0.400290</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  height weight  waist  ...  A_Care_Rec  A_Carries_Rec  A_Cure_Rec\n",
              "0    155     55     74  ...        8.17           2.68        0.57\n",
              "1    155     55     76  ...        8.17           2.68        0.57\n",
              "2    155     55     77  ...        8.17           2.68        0.57\n",
              "3    150     60     88  ...        8.17           2.68        0.57\n",
              "4    165     60     74  ...        8.17           2.68        0.57\n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waist</th>\n",
              "      <th>bp_systolic</th>\n",
              "      <th>bp_diastolic</th>\n",
              "      <th>tot_cholesterol</th>\n",
              "      <th>dental_carries</th>\n",
              "      <th>missing_tooth</th>\n",
              "      <th>dental_abrasion</th>\n",
              "      <th>wisdom_teeth_abnormal</th>\n",
              "      <th>plaque</th>\n",
              "      <th>C_sex</th>\n",
              "      <th>C_ageband</th>\n",
              "      <th>C_province</th>\n",
              "      <th>C_hearing_l</th>\n",
              "      <th>C_hearing_r</th>\n",
              "      <th>C_piu</th>\n",
              "      <th>C_smoking</th>\n",
              "      <th>C_drinking</th>\n",
              "      <th>N_BMI</th>\n",
              "      <th>N_diabetes</th>\n",
              "      <th>N_HDL</th>\n",
              "      <th>N_LDL</th>\n",
              "      <th>N_TRI</th>\n",
              "      <th>N_HEMO</th>\n",
              "      <th>N_CRE</th>\n",
              "      <th>N_GTP</th>\n",
              "      <th>LN_sight_l</th>\n",
              "      <th>LN_sight_r</th>\n",
              "      <th>LN_AST</th>\n",
              "      <th>LN_ALT</th>\n",
              "      <th>A_NormalA_Result</th>\n",
              "      <th>A_NormalB_Result</th>\n",
              "      <th>A_Caution_Result</th>\n",
              "      <th>A_NeedCare_Result</th>\n",
              "      <th>A_Nutrition_Edu</th>\n",
              "      <th>A_Hygine_Edu</th>\n",
              "      <th>A_Fluoride_Edu</th>\n",
              "      <th>A_Examine_Rec</th>\n",
              "      <th>A_Care_Rec</th>\n",
              "      <th>A_Carries_Rec</th>\n",
              "      <th>A_Cure_Rec</th>\n",
              "      <th>A_AverageDentalHospitalVisitCnt</th>\n",
              "      <th>A_AverageDentalClinicVisitCnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>155</td>\n",
              "      <td>55</td>\n",
              "      <td>74</td>\n",
              "      <td>170</td>\n",
              "      <td>110</td>\n",
              "      <td>208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002289</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Good</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.135235</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.374972</td>\n",
              "      <td>0.318926</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>150</td>\n",
              "      <td>70</td>\n",
              "      <td>89</td>\n",
              "      <td>134</td>\n",
              "      <td>82</td>\n",
              "      <td>194</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.003111</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>MediumRisk</td>\n",
              "      <td>VeryHigh</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.365967</td>\n",
              "      <td>0.368977</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>155</td>\n",
              "      <td>50</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>81</td>\n",
              "      <td>196</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>0.002081</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>Normal</td>\n",
              "      <td>NearGood</td>\n",
              "      <td>Good</td>\n",
              "      <td>Anemia</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.238306</td>\n",
              "      <td>0.456511</td>\n",
              "      <td>0.368977</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>160</td>\n",
              "      <td>55</td>\n",
              "      <td>75</td>\n",
              "      <td>120</td>\n",
              "      <td>70</td>\n",
              "      <td>209</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>0.002148</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>MediumRisk</td>\n",
              "      <td>Upperbound</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.105152</td>\n",
              "      <td>0.135235</td>\n",
              "      <td>0.391642</td>\n",
              "      <td>0.330234</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>150</td>\n",
              "      <td>50</td>\n",
              "      <td>75</td>\n",
              "      <td>120</td>\n",
              "      <td>60</td>\n",
              "      <td>204</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002222</td>\n",
              "      <td>Normal</td>\n",
              "      <td>HighRisk</td>\n",
              "      <td>Upperbound</td>\n",
              "      <td>Good</td>\n",
              "      <td>Anemia</td>\n",
              "      <td>Abnormal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.420682</td>\n",
              "      <td>0.318926</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  height weight  ...  A_AverageDentalHospitalVisitCnt  A_AverageDentalClinicVisitCnt\n",
              "0    155     55  ...                             48.1                        1061.92\n",
              "1    150     70  ...                             48.1                        1061.92\n",
              "2    155     50  ...                             48.1                        1061.92\n",
              "3    160     55  ...                             48.1                        1061.92\n",
              "4    150     50  ...                             48.1                        1061.92\n",
              "\n",
              "[5 rows x 44 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waist</th>\n",
              "      <th>bp_systolic</th>\n",
              "      <th>bp_diastolic</th>\n",
              "      <th>tot_cholesterol</th>\n",
              "      <th>dental_carries</th>\n",
              "      <th>missing_tooth</th>\n",
              "      <th>dental_abrasion</th>\n",
              "      <th>wisdom_teeth_abnormal</th>\n",
              "      <th>plaque</th>\n",
              "      <th>C_sex</th>\n",
              "      <th>C_ageband</th>\n",
              "      <th>C_province</th>\n",
              "      <th>C_hearing_l</th>\n",
              "      <th>C_hearing_r</th>\n",
              "      <th>C_piu</th>\n",
              "      <th>C_smoking</th>\n",
              "      <th>C_drinking</th>\n",
              "      <th>N_BMI</th>\n",
              "      <th>N_diabetes</th>\n",
              "      <th>N_HDL</th>\n",
              "      <th>N_LDL</th>\n",
              "      <th>N_TRI</th>\n",
              "      <th>N_HEMO</th>\n",
              "      <th>N_CRE</th>\n",
              "      <th>N_GTP</th>\n",
              "      <th>LN_sight_l</th>\n",
              "      <th>LN_sight_r</th>\n",
              "      <th>LN_AST</th>\n",
              "      <th>LN_ALT</th>\n",
              "      <th>A_NormalA_Result</th>\n",
              "      <th>A_NormalB_Result</th>\n",
              "      <th>A_Caution_Result</th>\n",
              "      <th>A_NeedCare_Result</th>\n",
              "      <th>A_Nutrition_Edu</th>\n",
              "      <th>A_Hygine_Edu</th>\n",
              "      <th>A_Fluoride_Edu</th>\n",
              "      <th>A_Examine_Rec</th>\n",
              "      <th>A_Care_Rec</th>\n",
              "      <th>A_Carries_Rec</th>\n",
              "      <th>A_Cure_Rec</th>\n",
              "      <th>A_AverageDentalHospitalVisitCnt</th>\n",
              "      <th>A_AverageDentalClinicVisitCnt</th>\n",
              "      <th>A_DentalPrevalenceTrend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>155</td>\n",
              "      <td>55</td>\n",
              "      <td>74</td>\n",
              "      <td>170</td>\n",
              "      <td>110</td>\n",
              "      <td>208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002289</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Good</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.135235</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.374972</td>\n",
              "      <td>0.318926</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "      <td>558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>150</td>\n",
              "      <td>70</td>\n",
              "      <td>89</td>\n",
              "      <td>134</td>\n",
              "      <td>82</td>\n",
              "      <td>194</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.003111</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>MediumRisk</td>\n",
              "      <td>VeryHigh</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.365967</td>\n",
              "      <td>0.368977</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "      <td>558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>155</td>\n",
              "      <td>50</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>81</td>\n",
              "      <td>196</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>0.002081</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>Normal</td>\n",
              "      <td>NearGood</td>\n",
              "      <td>Good</td>\n",
              "      <td>Anemia</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.238306</td>\n",
              "      <td>0.456511</td>\n",
              "      <td>0.368977</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "      <td>558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>160</td>\n",
              "      <td>55</td>\n",
              "      <td>75</td>\n",
              "      <td>120</td>\n",
              "      <td>70</td>\n",
              "      <td>209</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>0.002148</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>MediumRisk</td>\n",
              "      <td>Upperbound</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.105152</td>\n",
              "      <td>0.135235</td>\n",
              "      <td>0.391642</td>\n",
              "      <td>0.330234</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "      <td>558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>150</td>\n",
              "      <td>50</td>\n",
              "      <td>75</td>\n",
              "      <td>120</td>\n",
              "      <td>60</td>\n",
              "      <td>204</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002222</td>\n",
              "      <td>Normal</td>\n",
              "      <td>HighRisk</td>\n",
              "      <td>Upperbound</td>\n",
              "      <td>Good</td>\n",
              "      <td>Anemia</td>\n",
              "      <td>Abnormal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.420682</td>\n",
              "      <td>0.318926</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>48.1</td>\n",
              "      <td>1061.92</td>\n",
              "      <td>558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  height weight  ...  A_AverageDentalClinicVisitCnt  A_DentalPrevalenceTrend\n",
              "0    155     55  ...                        1061.92                      558\n",
              "1    150     70  ...                        1061.92                      558\n",
              "2    155     50  ...                        1061.92                      558\n",
              "3    160     55  ...                        1061.92                      558\n",
              "4    150     50  ...                        1061.92                      558\n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waist</th>\n",
              "      <th>bp_systolic</th>\n",
              "      <th>bp_diastolic</th>\n",
              "      <th>tot_cholesterol</th>\n",
              "      <th>dental_carries</th>\n",
              "      <th>missing_tooth</th>\n",
              "      <th>dental_abrasion</th>\n",
              "      <th>wisdom_teeth_abnormal</th>\n",
              "      <th>plaque</th>\n",
              "      <th>C_sex</th>\n",
              "      <th>C_ageband</th>\n",
              "      <th>C_province</th>\n",
              "      <th>C_hearing_l</th>\n",
              "      <th>C_hearing_r</th>\n",
              "      <th>C_piu</th>\n",
              "      <th>C_smoking</th>\n",
              "      <th>C_drinking</th>\n",
              "      <th>N_BMI</th>\n",
              "      <th>N_diabetes</th>\n",
              "      <th>N_HDL</th>\n",
              "      <th>N_LDL</th>\n",
              "      <th>N_TRI</th>\n",
              "      <th>N_HEMO</th>\n",
              "      <th>N_CRE</th>\n",
              "      <th>N_GTP</th>\n",
              "      <th>LN_sight_l</th>\n",
              "      <th>LN_sight_r</th>\n",
              "      <th>LN_AST</th>\n",
              "      <th>LN_ALT</th>\n",
              "      <th>A_NormalA_Result</th>\n",
              "      <th>A_NormalB_Result</th>\n",
              "      <th>A_Caution_Result</th>\n",
              "      <th>A_NeedCare_Result</th>\n",
              "      <th>A_Nutrition_Edu</th>\n",
              "      <th>A_Hygine_Edu</th>\n",
              "      <th>A_Fluoride_Edu</th>\n",
              "      <th>A_Examine_Rec</th>\n",
              "      <th>A_Care_Rec</th>\n",
              "      <th>...</th>\n",
              "      <th>R_income</th>\n",
              "      <th>R_AnemiaDiag</th>\n",
              "      <th>R_AnginaPectorisDiag</th>\n",
              "      <th>R_ArthritisDiag</th>\n",
              "      <th>R_Asthma_Diag</th>\n",
              "      <th>R_MasticationLesion</th>\n",
              "      <th>R_BHepatitisDiag</th>\n",
              "      <th>R_CHepatitisDiag</th>\n",
              "      <th>R_HemorrhoidsDiag</th>\n",
              "      <th>R_HealthInstExp</th>\n",
              "      <th>R_HBP_Diag</th>\n",
              "      <th>R_PronounceLesion</th>\n",
              "      <th>R_DentureUse</th>\n",
              "      <th>R_SubjHealthLevel</th>\n",
              "      <th>R_EQVAS</th>\n",
              "      <th>R_FinEduGrade</th>\n",
              "      <th>R_DentDidNotExp</th>\n",
              "      <th>R_EQ5DNormLife</th>\n",
              "      <th>R_CPRRecognition</th>\n",
              "      <th>R_DrinkStartAge</th>\n",
              "      <th>R_FamilyCnt</th>\n",
              "      <th>R_AveSleepTime</th>\n",
              "      <th>R_AIDSRecognition</th>\n",
              "      <th>R_BPCheckinYear</th>\n",
              "      <th>R_WalkingDay</th>\n",
              "      <th>R_WalkingMinutes</th>\n",
              "      <th>R_SmokingStartAge</th>\n",
              "      <th>R_NutriChk</th>\n",
              "      <th>R_DrinkFreq</th>\n",
              "      <th>R_NearGYM</th>\n",
              "      <th>R_EQ5DPain</th>\n",
              "      <th>R_EQ5DAthleticAbility</th>\n",
              "      <th>R_BPCheckinYear.1</th>\n",
              "      <th>R_StressIndex</th>\n",
              "      <th>R_ExerciseMidHour</th>\n",
              "      <th>R_DrinkPerOnce</th>\n",
              "      <th>R_BreakfastperWeek</th>\n",
              "      <th>R_EQ5DSelfManage</th>\n",
              "      <th>R_ExerciseHighHour</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>155</td>\n",
              "      <td>55</td>\n",
              "      <td>74</td>\n",
              "      <td>170</td>\n",
              "      <td>110</td>\n",
              "      <td>208</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002289</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Good</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.135235</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.374972</td>\n",
              "      <td>0.318926</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>...</td>\n",
              "      <td>4862.242991</td>\n",
              "      <td>0.102804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065421</td>\n",
              "      <td>0.009346</td>\n",
              "      <td>0.149533</td>\n",
              "      <td>0.037383</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.093458</td>\n",
              "      <td>0.196262</td>\n",
              "      <td>0.158879</td>\n",
              "      <td>0.018692</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018692</td>\n",
              "      <td>74.700935</td>\n",
              "      <td>5.504673</td>\n",
              "      <td>0.364486</td>\n",
              "      <td>0.056075</td>\n",
              "      <td>0.869159</td>\n",
              "      <td>20.560748</td>\n",
              "      <td>3.551402</td>\n",
              "      <td>6.607477</td>\n",
              "      <td>0.934579</td>\n",
              "      <td>2.149533</td>\n",
              "      <td>4.906542</td>\n",
              "      <td>17.383178</td>\n",
              "      <td>1.177570</td>\n",
              "      <td>0.579439</td>\n",
              "      <td>0.261682</td>\n",
              "      <td>0.850467</td>\n",
              "      <td>0.327103</td>\n",
              "      <td>0.065421</td>\n",
              "      <td>0.915888</td>\n",
              "      <td>0.925234</td>\n",
              "      <td>0.523364</td>\n",
              "      <td>0.766355</td>\n",
              "      <td>5.514019</td>\n",
              "      <td>0.018692</td>\n",
              "      <td>0.495327</td>\n",
              "      <td>0.289720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>150</td>\n",
              "      <td>70</td>\n",
              "      <td>89</td>\n",
              "      <td>134</td>\n",
              "      <td>82</td>\n",
              "      <td>194</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.003111</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>MediumRisk</td>\n",
              "      <td>VeryHigh</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.365967</td>\n",
              "      <td>0.368977</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>...</td>\n",
              "      <td>5100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>155</td>\n",
              "      <td>50</td>\n",
              "      <td>73</td>\n",
              "      <td>129</td>\n",
              "      <td>81</td>\n",
              "      <td>196</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>0.002081</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>Normal</td>\n",
              "      <td>NearGood</td>\n",
              "      <td>Good</td>\n",
              "      <td>Anemia</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.238306</td>\n",
              "      <td>0.456511</td>\n",
              "      <td>0.368977</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>...</td>\n",
              "      <td>4910.857143</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.030075</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.150376</td>\n",
              "      <td>0.045113</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.112782</td>\n",
              "      <td>0.180451</td>\n",
              "      <td>0.067669</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022556</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>5.819549</td>\n",
              "      <td>0.270677</td>\n",
              "      <td>0.067669</td>\n",
              "      <td>0.887218</td>\n",
              "      <td>17.556391</td>\n",
              "      <td>3.518797</td>\n",
              "      <td>6.593985</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.744361</td>\n",
              "      <td>4.661654</td>\n",
              "      <td>17.827068</td>\n",
              "      <td>0.939850</td>\n",
              "      <td>0.586466</td>\n",
              "      <td>0.278195</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.203008</td>\n",
              "      <td>0.037594</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.827068</td>\n",
              "      <td>0.165414</td>\n",
              "      <td>0.691729</td>\n",
              "      <td>5.827068</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.270677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>160</td>\n",
              "      <td>55</td>\n",
              "      <td>75</td>\n",
              "      <td>120</td>\n",
              "      <td>70</td>\n",
              "      <td>209</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>Drinking</td>\n",
              "      <td>0.002148</td>\n",
              "      <td>Prediabetes</td>\n",
              "      <td>MediumRisk</td>\n",
              "      <td>Upperbound</td>\n",
              "      <td>Good</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.105152</td>\n",
              "      <td>0.135235</td>\n",
              "      <td>0.391642</td>\n",
              "      <td>0.330234</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>...</td>\n",
              "      <td>5512.561983</td>\n",
              "      <td>0.123967</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066116</td>\n",
              "      <td>0.008264</td>\n",
              "      <td>0.082645</td>\n",
              "      <td>0.033058</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.123967</td>\n",
              "      <td>0.148760</td>\n",
              "      <td>0.049587</td>\n",
              "      <td>0.008264</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033058</td>\n",
              "      <td>77.512397</td>\n",
              "      <td>5.851240</td>\n",
              "      <td>0.223140</td>\n",
              "      <td>0.033058</td>\n",
              "      <td>0.892562</td>\n",
              "      <td>18.487603</td>\n",
              "      <td>3.512397</td>\n",
              "      <td>6.553719</td>\n",
              "      <td>0.950413</td>\n",
              "      <td>1.867769</td>\n",
              "      <td>4.677686</td>\n",
              "      <td>16.528926</td>\n",
              "      <td>1.099174</td>\n",
              "      <td>0.553719</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.859504</td>\n",
              "      <td>0.280992</td>\n",
              "      <td>0.057851</td>\n",
              "      <td>0.710744</td>\n",
              "      <td>0.892562</td>\n",
              "      <td>0.314050</td>\n",
              "      <td>0.702479</td>\n",
              "      <td>5.942149</td>\n",
              "      <td>0.016529</td>\n",
              "      <td>0.859504</td>\n",
              "      <td>0.223140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>150</td>\n",
              "      <td>50</td>\n",
              "      <td>75</td>\n",
              "      <td>120</td>\n",
              "      <td>60</td>\n",
              "      <td>204</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>45</td>\n",
              "      <td>Seoul</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Negative</td>\n",
              "      <td>NonSmoking</td>\n",
              "      <td>NonDrinking</td>\n",
              "      <td>0.002222</td>\n",
              "      <td>Normal</td>\n",
              "      <td>HighRisk</td>\n",
              "      <td>Upperbound</td>\n",
              "      <td>Good</td>\n",
              "      <td>Anemia</td>\n",
              "      <td>Abnormal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.260671</td>\n",
              "      <td>0.420682</td>\n",
              "      <td>0.318926</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.99</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.21</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.49</td>\n",
              "      <td>8.17</td>\n",
              "      <td>...</td>\n",
              "      <td>5272.500000</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.109375</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>0.218750</td>\n",
              "      <td>0.109375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>74.984375</td>\n",
              "      <td>5.406250</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>21.406250</td>\n",
              "      <td>3.593750</td>\n",
              "      <td>6.437500</td>\n",
              "      <td>0.984375</td>\n",
              "      <td>2.406250</td>\n",
              "      <td>4.875000</td>\n",
              "      <td>17.500000</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.421875</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>1.218750</td>\n",
              "      <td>0.890625</td>\n",
              "      <td>0.421875</td>\n",
              "      <td>0.796875</td>\n",
              "      <td>5.703125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.343750</td>\n",
              "      <td>0.234375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 85 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  height weight  waist  ...  R_EQ5DSelfManage  R_ExerciseHighHour    target\n",
              "0    155     55     74  ...          0.018692            0.495327  0.289720\n",
              "1    150     70     89  ...          0.000000            0.000000  1.000000\n",
              "2    155     50     73  ...          0.015038            0.578947  0.270677\n",
              "3    160     55     75  ...          0.016529            0.859504  0.223140\n",
              "4    150     50     75  ...          0.000000            0.343750  0.234375\n",
              "\n",
              "[5 rows x 85 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100000 entries, 0 to 99999\n",
            "Data columns (total 85 columns):\n",
            "height                             100000 non-null object\n",
            "weight                             100000 non-null object\n",
            "waist                              100000 non-null int64\n",
            "bp_systolic                        100000 non-null int64\n",
            "bp_diastolic                       100000 non-null int64\n",
            "tot_cholesterol                    100000 non-null int64\n",
            "dental_carries                     100000 non-null float64\n",
            "missing_tooth                      100000 non-null float64\n",
            "dental_abrasion                    100000 non-null float64\n",
            "wisdom_teeth_abnormal              100000 non-null float64\n",
            "plaque                             100000 non-null float64\n",
            "C_sex                              100000 non-null object\n",
            "C_ageband                          100000 non-null object\n",
            "C_province                         100000 non-null object\n",
            "C_hearing_l                        100000 non-null object\n",
            "C_hearing_r                        100000 non-null object\n",
            "C_piu                              100000 non-null object\n",
            "C_smoking                          100000 non-null object\n",
            "C_drinking                         100000 non-null object\n",
            "N_BMI                              100000 non-null float64\n",
            "N_diabetes                         100000 non-null object\n",
            "N_HDL                              100000 non-null object\n",
            "N_LDL                              100000 non-null object\n",
            "N_TRI                              100000 non-null object\n",
            "N_HEMO                             100000 non-null object\n",
            "N_CRE                              100000 non-null object\n",
            "N_GTP                              100000 non-null object\n",
            "LN_sight_l                         100000 non-null float64\n",
            "LN_sight_r                         100000 non-null float64\n",
            "LN_AST                             100000 non-null float64\n",
            "LN_ALT                             100000 non-null float64\n",
            "A_NormalA_Result                   100000 non-null float64\n",
            "A_NormalB_Result                   100000 non-null float64\n",
            "A_Caution_Result                   100000 non-null float64\n",
            "A_NeedCare_Result                  100000 non-null float64\n",
            "A_Nutrition_Edu                    100000 non-null float64\n",
            "A_Hygine_Edu                       100000 non-null float64\n",
            "A_Fluoride_Edu                     100000 non-null float64\n",
            "A_Examine_Rec                      100000 non-null float64\n",
            "A_Care_Rec                         100000 non-null float64\n",
            "A_Carries_Rec                      100000 non-null float64\n",
            "A_Cure_Rec                         100000 non-null float64\n",
            "A_AverageDentalHospitalVisitCnt    100000 non-null float64\n",
            "A_AverageDentalClinicVisitCnt      100000 non-null float64\n",
            "A_DentalPrevalenceTrend            100000 non-null int64\n",
            "R_income                           96559 non-null float64\n",
            "R_AnemiaDiag                       96559 non-null float64\n",
            "R_AnginaPectorisDiag               96559 non-null float64\n",
            "R_ArthritisDiag                    96559 non-null float64\n",
            "R_Asthma_Diag                      96559 non-null float64\n",
            "R_MasticationLesion                96559 non-null float64\n",
            "R_BHepatitisDiag                   96559 non-null float64\n",
            "R_CHepatitisDiag                   96559 non-null float64\n",
            "R_HemorrhoidsDiag                  96559 non-null float64\n",
            "R_HealthInstExp                    96559 non-null float64\n",
            "R_HBP_Diag                         96559 non-null float64\n",
            "R_PronounceLesion                  96559 non-null float64\n",
            "R_DentureUse                       96559 non-null float64\n",
            "R_SubjHealthLevel                  96559 non-null float64\n",
            "R_EQVAS                            96559 non-null float64\n",
            "R_FinEduGrade                      96559 non-null float64\n",
            "R_DentDidNotExp                    96559 non-null float64\n",
            "R_EQ5DNormLife                     96559 non-null float64\n",
            "R_CPRRecognition                   96559 non-null float64\n",
            "R_DrinkStartAge                    96559 non-null float64\n",
            "R_FamilyCnt                        96559 non-null float64\n",
            "R_AveSleepTime                     96559 non-null float64\n",
            "R_AIDSRecognition                  96559 non-null float64\n",
            "R_BPCheckinYear                    96559 non-null float64\n",
            "R_WalkingDay                       96559 non-null float64\n",
            "R_WalkingMinutes                   96559 non-null float64\n",
            "R_SmokingStartAge                  96559 non-null float64\n",
            "R_NutriChk                         96559 non-null float64\n",
            "R_DrinkFreq                        96559 non-null float64\n",
            "R_NearGYM                          96559 non-null float64\n",
            "R_EQ5DPain                         96559 non-null float64\n",
            "R_EQ5DAthleticAbility              96559 non-null float64\n",
            "R_BPCheckinYear.1                  96559 non-null float64\n",
            "R_StressIndex                      96559 non-null float64\n",
            "R_ExerciseMidHour                  96559 non-null float64\n",
            "R_DrinkPerOnce                     96559 non-null float64\n",
            "R_BreakfastperWeek                 96559 non-null float64\n",
            "R_EQ5DSelfManage                   96559 non-null float64\n",
            "R_ExerciseHighHour                 96559 non-null float64\n",
            "target                             96559 non-null float64\n",
            "dtypes: float64(63), int64(5), object(17)\n",
            "memory usage: 65.6+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya7dG_AROYbe",
        "colab_type": "code",
        "outputId": "b98f292b-496f-44c8-e94d-2f790af8d273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# 외부변수와 매칭이 되지 않는 항목은 버림\n",
        "df_new.dropna(how='any', inplace=True)\n",
        "print(df_new[target_nm].value_counts())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0    48427\n",
            "1.0    48132\n",
            "Name: dental_carries, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRBtVWUWX6G9",
        "colab_type": "text"
      },
      "source": [
        "#### #7. IV 계산을 통한 변수 선택"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rL7QbTXX2ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # IV 산출\n",
        "# iv_df, IV_first = calc_iv_all(df_new,df_new[target_nm])\n",
        "\n",
        "# # IV 값 내림차순 정렬\n",
        "# IV_first.sort_values('IV',ascending=False)\n",
        "\n",
        "# IV_select_col = list(IV_first[IV_first['IV']>=0.015]['VAR_NAME'])\n",
        "# IV_select_col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RwwMyDLED72",
        "colab_type": "code",
        "outputId": "50f70fdd-ae2a-4730-f1f9-2f538911d711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# 독립변수와 종속변수를 나눠 줌\n",
        "data_x = df_new[df_new.columns.difference([target_nm])]\n",
        "# data_x = df_new[IV_select_col]\n",
        "data_y = df_new[target_nm].astype('float64')\n",
        "\n",
        "# 데이터를 나눈 뒤 속성별로 컬럼을 분류\n",
        "num_attribs = [col for col in data_x.columns if data_x[col].dtype in ['int64','float64']]\n",
        "cat_attribs = [col for col in data_x.columns if data_x[col].dtype not in ['int64','float64']]\n",
        "\n",
        "num_attribs = list(set(num_attribs) - set([target_nm]))\n",
        "\n",
        "print(\"num_attribs: \", num_attribs)\n",
        "print(\"cat_attribs: \", cat_attribs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_attribs:  ['waist', 'dental_abrasion', 'R_NearGYM', 'R_FamilyCnt', 'R_HealthInstExp', 'A_Nutrition_Edu', 'LN_sight_l', 'R_Asthma_Diag', 'A_Carries_Rec', 'R_ExerciseMidHour', 'R_DentureUse', 'R_HBP_Diag', 'R_SubjHealthLevel', 'R_income', 'R_BreakfastperWeek', 'R_EQ5DAthleticAbility', 'R_ExerciseHighHour', 'LN_AST', 'R_CHepatitisDiag', 'R_BPCheckinYear', 'R_DentDidNotExp', 'LN_ALT', 'A_Fluoride_Edu', 'R_SmokingStartAge', 'LN_sight_r', 'R_EQ5DNormLife', 'wisdom_teeth_abnormal', 'R_EQ5DPain', 'A_AverageDentalHospitalVisitCnt', 'R_AnginaPectorisDiag', 'A_AverageDentalClinicVisitCnt', 'A_NormalA_Result', 'A_NormalB_Result', 'A_Caution_Result', 'R_HemorrhoidsDiag', 'R_BPCheckinYear.1', 'tot_cholesterol', 'R_ArthritisDiag', 'R_MasticationLesion', 'R_NutriChk', 'target', 'N_BMI', 'bp_systolic', 'A_Cure_Rec', 'R_WalkingMinutes', 'A_NeedCare_Result', 'R_StressIndex', 'A_DentalPrevalenceTrend', 'plaque', 'R_CPRRecognition', 'R_DrinkFreq', 'R_AnemiaDiag', 'missing_tooth', 'A_Examine_Rec', 'R_EQ5DSelfManage', 'R_FinEduGrade', 'R_WalkingDay', 'R_AveSleepTime', 'R_PronounceLesion', 'bp_diastolic', 'R_AIDSRecognition', 'R_BHepatitisDiag', 'R_DrinkStartAge', 'A_Care_Rec', 'A_Hygine_Edu', 'R_DrinkPerOnce', 'R_EQVAS']\n",
            "cat_attribs:  ['C_ageband', 'C_drinking', 'C_hearing_l', 'C_hearing_r', 'C_piu', 'C_province', 'C_sex', 'C_smoking', 'N_CRE', 'N_GTP', 'N_HDL', 'N_HEMO', 'N_LDL', 'N_TRI', 'N_diabetes', 'height', 'weight']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT20XJ5SICI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label = LabelEncoder()\n",
        "# for col in data_x[cat_attribs].columns:   \n",
        "#   data_x[col] = label.fit_transform(data_x[col])\n",
        "#   data_x = pd.get_dummies(data_x, columns =[col], prefix=col+\"_lb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZGJ5-n5rQUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 수치형 변수 정규화 \n",
        "num_pipeline = Pipeline([\n",
        "        ('min_max_scaler', MinMaxScaler()),\n",
        "    ])\n",
        "\n",
        "# numpy 형식으로 전체 변경\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num_pipeline\", num_pipeline, num_attribs),\n",
        "        (\"cat_encoder\", OneHotEncoder(sparse=False), cat_attribs),\n",
        "    ])\n",
        "\n",
        "data_x_piped = full_pipeline.fit_transform(data_x)\n",
        "\n",
        "data_y_piped = data_y.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7JDk-RXvZO0",
        "colab_type": "code",
        "outputId": "40c6ce6f-f194-4843-e346-d92415d3272c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data_x_piped.shape[1]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "166"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpnjUNhR-xpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_x, test_x, train_y, test_y = train_test_split(data_x_piped, data_y_piped, test_size = 0.2, random_state = set_random_seed)\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = 0.2, random_state = set_random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCF6bpKmwcQK",
        "colab_type": "code",
        "outputId": "aa00d68d-b4ad-43be-f4f5-2b3f870a63bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Tensorboard 사용준비\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-21 01:36:39--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.5.84.255, 34.204.156.91, 52.73.84.118, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.5.84.255|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  18.0MB/s    in 0.7s    \n",
            "\n",
            "2019-11-21 01:36:40 (18.0 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "https://0c87c0ca.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RA9GCcWtIaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = \"softmax\"\n",
        "optimizer = keras.optimizers.SGD()\n",
        "input_dim = train_x.shape[1]\n",
        "batch_size = 128\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CThA-GFqtILK",
        "colab_type": "code",
        "outputId": "4df9b77c-b082-4908-b94c-1afd4ec10a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "K.clear_session() # Tensorboard Callback error 방지를 위해 tensorboard의 session을 clear 해줌\n",
        "with tf.Session(graph=tf.Graph()) as sess:\n",
        "  model = Sequential()\n",
        "  # 첫 번째 Layer (Input layer)\n",
        "  model.add(Dense(input_dim=input_dim, init='glorot_uniform', activation=activation, output_dim=256))\n",
        "  # # 두 번째 Layer (Hidden layer 1)\n",
        "  model.add(Dense(output_dim=256, activation=activation))\n",
        "  # Dense Layer (Output layer)\n",
        "  model.add(Dense(output_dim=1))\n",
        "  model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "  # Cost function 및 Optimizer 설정 # binary class 분류이므로 binary_crossentropy 사용 # Adam optimizer 사용\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=True),  metrics=['accuracy', precision, recall, f1score])\n",
        "\n",
        "  tbCallBack = TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "                          write_graph=True,\n",
        "                          write_grads=True,\n",
        "                          batch_size=batch_size,\n",
        "                          write_images=True)\n",
        "  hist = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(val_x, val_y), callbacks=[tbCallBack])\n",
        "  \n",
        "  score = model.evaluate(test_x, test_y, verbose=0)\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 61797 samples, validate on 15450 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1068: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1112: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/10\n",
            "61797/61797 [==============================] - 3s 50us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/10\n",
            "61797/61797 [==============================] - 3s 48us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0010 - recall: 0.0021 - f1score: 0.0014 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 3/10\n",
            "61797/61797 [==============================] - 3s 46us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 4/10\n",
            "61797/61797 [==============================] - 3s 47us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 5/10\n",
            "61797/61797 [==============================] - 3s 48us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 6/10\n",
            "61797/61797 [==============================] - 3s 47us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 7/10\n",
            "61797/61797 [==============================] - 3s 43us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 8/10\n",
            "61797/61797 [==============================] - 3s 43us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 9/10\n",
            "61797/61797 [==============================] - 3s 43us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 10/10\n",
            "61797/61797 [==============================] - 3s 42us/step - loss: 0.6931 - acc: 0.5023 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 0.6931 - val_acc: 0.5019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Test loss: 0.693153073991145\n",
            "Test accuracy: 0.4987572493786247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99sLXV448bBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = \"relu\"\n",
        "optimizer = keras.optimizers.Adam()\n",
        "input_dim = train_x.shape[1]\n",
        "batch_size = 128\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKzLLmgqBFc",
        "colab_type": "code",
        "outputId": "560dee68-789c-4cb9-997f-636743af2a35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "K.clear_session() # Tensorboard Callback error 방지를 위해 tensorboard의 session을 clear 해줌\n",
        "with tf.Session(graph=tf.Graph()) as sess:\n",
        "  model = Sequential()\n",
        "  # 첫 번째 Layer (Input layer)\n",
        "  model.add(Dense(input_dim=input_dim, init='glorot_uniform', activation=activation, output_dim=256))\n",
        "  model.add(Dropout(0.3)) # 30% 정도를 Drop \n",
        "\n",
        "  # # 두 번째 Layer (Hidden layer 1)\n",
        "  model.add(Dense(output_dim=256, activation=activation))\n",
        "  model.add(Dropout(0.3)) # 30% 정도를 Drop \n",
        "\n",
        "  # # 세 번째 Layer (Hidden layer 2)\n",
        "  model.add(Dense(output_dim=256, activation=activation))\n",
        "  model.add(Dropout(0.3)) # 30% 정도를 Drop \n",
        "\n",
        "  # Dense Layer (Output layer)\n",
        "  model.add(Dense(output_dim=1))\n",
        "  model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "  # Cost function 및 Optimizer 설정 # binary class 분류이므로 binary_crossentropy 사용 # Adam optimizer 사용\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=True),  metrics=['accuracy', precision, recall, f1score])\n",
        "\n",
        "  tbCallBack = TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "                          write_graph=True,\n",
        "                          write_grads=True,\n",
        "                          batch_size=batch_size,\n",
        "                          write_images=True)\n",
        "  hist = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(val_x, val_y), callbacks=[tbCallBack])\n",
        "  \n",
        "  score = model.evaluate(test_x, test_y, verbose=0)\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 61797 samples, validate on 15450 samples\n",
            "Epoch 1/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6922 - acc: 0.5265 - precision: 0.5251 - recall: 0.5326 - f1score: 0.5254 - val_loss: 0.6793 - val_acc: 0.5975 - val_precision: 0.5985 - val_recall: 0.5820 - val_f1score: 0.5884\n",
            "Epoch 2/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6827 - acc: 0.5598 - precision: 0.5598 - recall: 0.5409 - f1score: 0.5476 - val_loss: 0.6675 - val_acc: 0.6126 - val_precision: 0.6035 - val_recall: 0.6476 - val_f1score: 0.6231\n",
            "Epoch 3/100\n",
            "61797/61797 [==============================] - 4s 69us/step - loss: 0.6732 - acc: 0.5833 - precision: 0.5827 - recall: 0.5752 - f1score: 0.5765 - val_loss: 0.6554 - val_acc: 0.6220 - val_precision: 0.6205 - val_recall: 0.6220 - val_f1score: 0.6195\n",
            "Epoch 4/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6653 - acc: 0.5973 - precision: 0.5972 - recall: 0.5868 - f1score: 0.5900 - val_loss: 0.6473 - val_acc: 0.6279 - val_precision: 0.6350 - val_recall: 0.5960 - val_f1score: 0.6131\n",
            "Epoch 5/100\n",
            "61797/61797 [==============================] - 5s 74us/step - loss: 0.6600 - acc: 0.6045 - precision: 0.6048 - recall: 0.5929 - f1score: 0.5967 - val_loss: 0.6424 - val_acc: 0.6297 - val_precision: 0.6391 - val_recall: 0.5903 - val_f1score: 0.6118\n",
            "Epoch 6/100\n",
            "61797/61797 [==============================] - 5s 74us/step - loss: 0.6544 - acc: 0.6152 - precision: 0.6158 - recall: 0.6028 - f1score: 0.6073 - val_loss: 0.6396 - val_acc: 0.6324 - val_precision: 0.6319 - val_recall: 0.6285 - val_f1score: 0.6284\n",
            "Epoch 7/100\n",
            "61797/61797 [==============================] - 4s 69us/step - loss: 0.6525 - acc: 0.6179 - precision: 0.6184 - recall: 0.6078 - f1score: 0.6110 - val_loss: 0.6384 - val_acc: 0.6345 - val_precision: 0.6351 - val_recall: 0.6269 - val_f1score: 0.6291\n",
            "Epoch 8/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6488 - acc: 0.6246 - precision: 0.6259 - recall: 0.6111 - f1score: 0.6165 - val_loss: 0.6373 - val_acc: 0.6349 - val_precision: 0.6396 - val_recall: 0.6125 - val_f1score: 0.6239\n",
            "Epoch 9/100\n",
            "61797/61797 [==============================] - 4s 67us/step - loss: 0.6477 - acc: 0.6240 - precision: 0.6264 - recall: 0.6060 - f1score: 0.6140 - val_loss: 0.6368 - val_acc: 0.6354 - val_precision: 0.6378 - val_recall: 0.6206 - val_f1score: 0.6272\n",
            "Epoch 10/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6458 - acc: 0.6267 - precision: 0.6298 - recall: 0.6085 - f1score: 0.6167 - val_loss: 0.6363 - val_acc: 0.6348 - val_precision: 0.6437 - val_recall: 0.5974 - val_f1score: 0.6178\n",
            "Epoch 11/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6461 - acc: 0.6265 - precision: 0.6297 - recall: 0.6072 - f1score: 0.6162 - val_loss: 0.6358 - val_acc: 0.6351 - val_precision: 0.6416 - val_recall: 0.6067 - val_f1score: 0.6216\n",
            "Epoch 12/100\n",
            "61797/61797 [==============================] - 5s 80us/step - loss: 0.6442 - acc: 0.6306 - precision: 0.6344 - recall: 0.6070 - f1score: 0.6184 - val_loss: 0.6356 - val_acc: 0.6350 - val_precision: 0.6448 - val_recall: 0.5939 - val_f1score: 0.6164\n",
            "Epoch 13/100\n",
            "61797/61797 [==============================] - 4s 71us/step - loss: 0.6435 - acc: 0.6312 - precision: 0.6355 - recall: 0.6080 - f1score: 0.6192 - val_loss: 0.6355 - val_acc: 0.6351 - val_precision: 0.6463 - val_recall: 0.5898 - val_f1score: 0.6149\n",
            "Epoch 14/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6431 - acc: 0.6317 - precision: 0.6366 - recall: 0.6066 - f1score: 0.6194 - val_loss: 0.6351 - val_acc: 0.6370 - val_precision: 0.6414 - val_recall: 0.6146 - val_f1score: 0.6257\n",
            "Epoch 15/100\n",
            "61797/61797 [==============================] - 4s 72us/step - loss: 0.6415 - acc: 0.6321 - precision: 0.6357 - recall: 0.6091 - f1score: 0.6202 - val_loss: 0.6350 - val_acc: 0.6376 - val_precision: 0.6410 - val_recall: 0.6186 - val_f1score: 0.6277\n",
            "Epoch 16/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6418 - acc: 0.6314 - precision: 0.6347 - recall: 0.6108 - f1score: 0.6206 - val_loss: 0.6350 - val_acc: 0.6351 - val_precision: 0.6433 - val_recall: 0.5986 - val_f1score: 0.6182\n",
            "Epoch 17/100\n",
            "61797/61797 [==============================] - 4s 73us/step - loss: 0.6419 - acc: 0.6321 - precision: 0.6362 - recall: 0.6099 - f1score: 0.6206 - val_loss: 0.6348 - val_acc: 0.6365 - val_precision: 0.6411 - val_recall: 0.6131 - val_f1score: 0.6248\n",
            "Epoch 18/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6400 - acc: 0.6340 - precision: 0.6386 - recall: 0.6119 - f1score: 0.6229 - val_loss: 0.6346 - val_acc: 0.6361 - val_precision: 0.6429 - val_recall: 0.6042 - val_f1score: 0.6210\n",
            "Epoch 19/100\n",
            "61797/61797 [==============================] - 4s 71us/step - loss: 0.6402 - acc: 0.6347 - precision: 0.6388 - recall: 0.6140 - f1score: 0.6241 - val_loss: 0.6346 - val_acc: 0.6374 - val_precision: 0.6434 - val_recall: 0.6095 - val_f1score: 0.6241\n",
            "Epoch 20/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6402 - acc: 0.6342 - precision: 0.6380 - recall: 0.6141 - f1score: 0.6235 - val_loss: 0.6347 - val_acc: 0.6359 - val_precision: 0.6451 - val_recall: 0.5967 - val_f1score: 0.6180\n",
            "Epoch 21/100\n",
            "61797/61797 [==============================] - 4s 72us/step - loss: 0.6390 - acc: 0.6359 - precision: 0.6395 - recall: 0.6136 - f1score: 0.6244 - val_loss: 0.6346 - val_acc: 0.6358 - val_precision: 0.6472 - val_recall: 0.5895 - val_f1score: 0.6151\n",
            "Epoch 22/100\n",
            "61797/61797 [==============================] - 5s 78us/step - loss: 0.6391 - acc: 0.6355 - precision: 0.6407 - recall: 0.6109 - f1score: 0.6232 - val_loss: 0.6343 - val_acc: 0.6373 - val_precision: 0.6422 - val_recall: 0.6129 - val_f1score: 0.6253\n",
            "Epoch 23/100\n",
            "61797/61797 [==============================] - 4s 73us/step - loss: 0.6395 - acc: 0.6349 - precision: 0.6396 - recall: 0.6104 - f1score: 0.6225 - val_loss: 0.6343 - val_acc: 0.6367 - val_precision: 0.6431 - val_recall: 0.6066 - val_f1score: 0.6224\n",
            "Epoch 24/100\n",
            "61797/61797 [==============================] - 5s 78us/step - loss: 0.6383 - acc: 0.6371 - precision: 0.6416 - recall: 0.6139 - f1score: 0.6254 - val_loss: 0.6343 - val_acc: 0.6372 - val_precision: 0.6486 - val_recall: 0.5918 - val_f1score: 0.6170\n",
            "Epoch 25/100\n",
            "61797/61797 [==============================] - 5s 73us/step - loss: 0.6380 - acc: 0.6373 - precision: 0.6415 - recall: 0.6146 - f1score: 0.6254 - val_loss: 0.6344 - val_acc: 0.6379 - val_precision: 0.6520 - val_recall: 0.5848 - val_f1score: 0.6145\n",
            "Epoch 26/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6380 - acc: 0.6365 - precision: 0.6406 - recall: 0.6124 - f1score: 0.6243 - val_loss: 0.6341 - val_acc: 0.6366 - val_precision: 0.6442 - val_recall: 0.6031 - val_f1score: 0.6210\n",
            "Epoch 27/100\n",
            "61797/61797 [==============================] - 4s 67us/step - loss: 0.6376 - acc: 0.6358 - precision: 0.6400 - recall: 0.6128 - f1score: 0.6242 - val_loss: 0.6341 - val_acc: 0.6364 - val_precision: 0.6433 - val_recall: 0.6056 - val_f1score: 0.6218\n",
            "Epoch 28/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6370 - acc: 0.6364 - precision: 0.6410 - recall: 0.6127 - f1score: 0.6247 - val_loss: 0.6340 - val_acc: 0.6363 - val_precision: 0.6437 - val_recall: 0.6039 - val_f1score: 0.6210\n",
            "Epoch 29/100\n",
            "61797/61797 [==============================] - 4s 69us/step - loss: 0.6376 - acc: 0.6371 - precision: 0.6419 - recall: 0.6149 - f1score: 0.6258 - val_loss: 0.6340 - val_acc: 0.6368 - val_precision: 0.6425 - val_recall: 0.6104 - val_f1score: 0.6239\n",
            "Epoch 30/100\n",
            "61797/61797 [==============================] - 5s 75us/step - loss: 0.6367 - acc: 0.6381 - precision: 0.6421 - recall: 0.6141 - f1score: 0.6259 - val_loss: 0.6341 - val_acc: 0.6382 - val_precision: 0.6414 - val_recall: 0.6204 - val_f1score: 0.6288\n",
            "Epoch 31/100\n",
            "61797/61797 [==============================] - 4s 69us/step - loss: 0.6362 - acc: 0.6379 - precision: 0.6418 - recall: 0.6168 - f1score: 0.6270 - val_loss: 0.6340 - val_acc: 0.6384 - val_precision: 0.6436 - val_recall: 0.6138 - val_f1score: 0.6263\n",
            "Epoch 32/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6370 - acc: 0.6389 - precision: 0.6443 - recall: 0.6134 - f1score: 0.6265 - val_loss: 0.6340 - val_acc: 0.6390 - val_precision: 0.6387 - val_recall: 0.6334 - val_f1score: 0.6343\n",
            "Epoch 33/100\n",
            "61797/61797 [==============================] - 4s 71us/step - loss: 0.6363 - acc: 0.6382 - precision: 0.6419 - recall: 0.6165 - f1score: 0.6271 - val_loss: 0.6337 - val_acc: 0.6391 - val_precision: 0.6431 - val_recall: 0.6197 - val_f1score: 0.6292\n",
            "Epoch 34/100\n",
            "61797/61797 [==============================] - 5s 78us/step - loss: 0.6360 - acc: 0.6394 - precision: 0.6427 - recall: 0.6184 - f1score: 0.6282 - val_loss: 0.6340 - val_acc: 0.6390 - val_precision: 0.6482 - val_recall: 0.6010 - val_f1score: 0.6217\n",
            "Epoch 35/100\n",
            "61797/61797 [==============================] - 4s 71us/step - loss: 0.6359 - acc: 0.6384 - precision: 0.6415 - recall: 0.6179 - f1score: 0.6277 - val_loss: 0.6340 - val_acc: 0.6392 - val_precision: 0.6493 - val_recall: 0.5989 - val_f1score: 0.6210\n",
            "Epoch 36/100\n",
            "61797/61797 [==============================] - 5s 78us/step - loss: 0.6356 - acc: 0.6394 - precision: 0.6431 - recall: 0.6194 - f1score: 0.6291 - val_loss: 0.6338 - val_acc: 0.6392 - val_precision: 0.6461 - val_recall: 0.6097 - val_f1score: 0.6253\n",
            "Epoch 37/100\n",
            "61797/61797 [==============================] - 4s 68us/step - loss: 0.6348 - acc: 0.6408 - precision: 0.6456 - recall: 0.6172 - f1score: 0.6291 - val_loss: 0.6338 - val_acc: 0.6388 - val_precision: 0.6427 - val_recall: 0.6190 - val_f1score: 0.6287\n",
            "Epoch 38/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6350 - acc: 0.6394 - precision: 0.6437 - recall: 0.6186 - f1score: 0.6289 - val_loss: 0.6338 - val_acc: 0.6398 - val_precision: 0.6480 - val_recall: 0.6066 - val_f1score: 0.6245\n",
            "Epoch 39/100\n",
            "61797/61797 [==============================] - 4s 69us/step - loss: 0.6356 - acc: 0.6400 - precision: 0.6438 - recall: 0.6194 - f1score: 0.6294 - val_loss: 0.6341 - val_acc: 0.6381 - val_precision: 0.6513 - val_recall: 0.5876 - val_f1score: 0.6157\n",
            "Epoch 40/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6350 - acc: 0.6394 - precision: 0.6440 - recall: 0.6173 - f1score: 0.6283 - val_loss: 0.6339 - val_acc: 0.6399 - val_precision: 0.6463 - val_recall: 0.6125 - val_f1score: 0.6269\n",
            "Epoch 41/100\n",
            "61797/61797 [==============================] - 4s 67us/step - loss: 0.6348 - acc: 0.6396 - precision: 0.6429 - recall: 0.6195 - f1score: 0.6289 - val_loss: 0.6345 - val_acc: 0.6382 - val_precision: 0.6540 - val_recall: 0.5801 - val_f1score: 0.6126\n",
            "Epoch 42/100\n",
            "61797/61797 [==============================] - 5s 81us/step - loss: 0.6348 - acc: 0.6395 - precision: 0.6438 - recall: 0.6155 - f1score: 0.6274 - val_loss: 0.6339 - val_acc: 0.6389 - val_precision: 0.6480 - val_recall: 0.6026 - val_f1score: 0.6223\n",
            "Epoch 43/100\n",
            "61797/61797 [==============================] - 4s 73us/step - loss: 0.6342 - acc: 0.6397 - precision: 0.6438 - recall: 0.6179 - f1score: 0.6287 - val_loss: 0.6338 - val_acc: 0.6403 - val_precision: 0.6495 - val_recall: 0.6047 - val_f1score: 0.6241\n",
            "Epoch 44/100\n",
            "61797/61797 [==============================] - 5s 75us/step - loss: 0.6356 - acc: 0.6393 - precision: 0.6434 - recall: 0.6179 - f1score: 0.6284 - val_loss: 0.6337 - val_acc: 0.6399 - val_precision: 0.6460 - val_recall: 0.6144 - val_f1score: 0.6277\n",
            "Epoch 45/100\n",
            "61797/61797 [==============================] - 4s 71us/step - loss: 0.6345 - acc: 0.6394 - precision: 0.6430 - recall: 0.6169 - f1score: 0.6279 - val_loss: 0.6337 - val_acc: 0.6397 - val_precision: 0.6431 - val_recall: 0.6223 - val_f1score: 0.6305\n",
            "Epoch 46/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6347 - acc: 0.6399 - precision: 0.6454 - recall: 0.6152 - f1score: 0.6280 - val_loss: 0.6337 - val_acc: 0.6399 - val_precision: 0.6405 - val_recall: 0.6324 - val_f1score: 0.6344\n",
            "Epoch 47/100\n",
            "61797/61797 [==============================] - 4s 69us/step - loss: 0.6340 - acc: 0.6417 - precision: 0.6461 - recall: 0.6212 - f1score: 0.6314 - val_loss: 0.6337 - val_acc: 0.6403 - val_precision: 0.6423 - val_recall: 0.6280 - val_f1score: 0.6330\n",
            "Epoch 48/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6340 - acc: 0.6399 - precision: 0.6441 - recall: 0.6189 - f1score: 0.6293 - val_loss: 0.6336 - val_acc: 0.6401 - val_precision: 0.6465 - val_recall: 0.6130 - val_f1score: 0.6272\n",
            "Epoch 49/100\n",
            "61797/61797 [==============================] - 4s 69us/step - loss: 0.6333 - acc: 0.6410 - precision: 0.6455 - recall: 0.6203 - f1score: 0.6307 - val_loss: 0.6336 - val_acc: 0.6402 - val_precision: 0.6412 - val_recall: 0.6311 - val_f1score: 0.6342\n",
            "Epoch 50/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6341 - acc: 0.6400 - precision: 0.6433 - recall: 0.6216 - f1score: 0.6303 - val_loss: 0.6335 - val_acc: 0.6403 - val_precision: 0.6471 - val_recall: 0.6116 - val_f1score: 0.6267\n",
            "Epoch 51/100\n",
            "61797/61797 [==============================] - 4s 69us/step - loss: 0.6325 - acc: 0.6414 - precision: 0.6447 - recall: 0.6219 - f1score: 0.6312 - val_loss: 0.6336 - val_acc: 0.6405 - val_precision: 0.6491 - val_recall: 0.6068 - val_f1score: 0.6250\n",
            "Epoch 52/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6339 - acc: 0.6396 - precision: 0.6431 - recall: 0.6194 - f1score: 0.6292 - val_loss: 0.6336 - val_acc: 0.6403 - val_precision: 0.6447 - val_recall: 0.6202 - val_f1score: 0.6302\n",
            "Epoch 53/100\n",
            "61797/61797 [==============================] - 4s 67us/step - loss: 0.6333 - acc: 0.6412 - precision: 0.6451 - recall: 0.6205 - f1score: 0.6307 - val_loss: 0.6336 - val_acc: 0.6403 - val_precision: 0.6455 - val_recall: 0.6166 - val_f1score: 0.6287\n",
            "Epoch 54/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6331 - acc: 0.6421 - precision: 0.6456 - recall: 0.6223 - f1score: 0.6320 - val_loss: 0.6335 - val_acc: 0.6403 - val_precision: 0.6413 - val_recall: 0.6309 - val_f1score: 0.6341\n",
            "Epoch 55/100\n",
            "61797/61797 [==============================] - 4s 68us/step - loss: 0.6333 - acc: 0.6408 - precision: 0.6435 - recall: 0.6232 - f1score: 0.6312 - val_loss: 0.6336 - val_acc: 0.6403 - val_precision: 0.6463 - val_recall: 0.6141 - val_f1score: 0.6278\n",
            "Epoch 56/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6333 - acc: 0.6407 - precision: 0.6443 - recall: 0.6203 - f1score: 0.6302 - val_loss: 0.6337 - val_acc: 0.6409 - val_precision: 0.6509 - val_recall: 0.6017 - val_f1score: 0.6233\n",
            "Epoch 57/100\n",
            "61797/61797 [==============================] - 4s 70us/step - loss: 0.6327 - acc: 0.6422 - precision: 0.6458 - recall: 0.6233 - f1score: 0.6324 - val_loss: 0.6342 - val_acc: 0.6398 - val_precision: 0.6549 - val_recall: 0.5843 - val_f1score: 0.6153\n",
            "Epoch 58/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6324 - acc: 0.6428 - precision: 0.6465 - recall: 0.6220 - f1score: 0.6321 - val_loss: 0.6335 - val_acc: 0.6407 - val_precision: 0.6433 - val_recall: 0.6268 - val_f1score: 0.6329\n",
            "Epoch 59/100\n",
            "61797/61797 [==============================] - 4s 72us/step - loss: 0.6329 - acc: 0.6425 - precision: 0.6458 - recall: 0.6240 - f1score: 0.6329 - val_loss: 0.6341 - val_acc: 0.6403 - val_precision: 0.6553 - val_recall: 0.5844 - val_f1score: 0.6156\n",
            "Epoch 60/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6330 - acc: 0.6402 - precision: 0.6444 - recall: 0.6193 - f1score: 0.6296 - val_loss: 0.6335 - val_acc: 0.6410 - val_precision: 0.6471 - val_recall: 0.6147 - val_f1score: 0.6284\n",
            "Epoch 61/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6324 - acc: 0.6423 - precision: 0.6471 - recall: 0.6204 - f1score: 0.6313 - val_loss: 0.6335 - val_acc: 0.6408 - val_precision: 0.6452 - val_recall: 0.6205 - val_f1score: 0.6306\n",
            "Epoch 62/100\n",
            "61797/61797 [==============================] - 5s 86us/step - loss: 0.6329 - acc: 0.6414 - precision: 0.6463 - recall: 0.6180 - f1score: 0.6296 - val_loss: 0.6335 - val_acc: 0.6397 - val_precision: 0.6398 - val_recall: 0.6336 - val_f1score: 0.6347\n",
            "Epoch 63/100\n",
            "61797/61797 [==============================] - 5s 78us/step - loss: 0.6325 - acc: 0.6417 - precision: 0.6455 - recall: 0.6215 - f1score: 0.6315 - val_loss: 0.6336 - val_acc: 0.6421 - val_precision: 0.6488 - val_recall: 0.6140 - val_f1score: 0.6288\n",
            "Epoch 64/100\n",
            "61797/61797 [==============================] - 5s 74us/step - loss: 0.6317 - acc: 0.6433 - precision: 0.6472 - recall: 0.6238 - f1score: 0.6332 - val_loss: 0.6336 - val_acc: 0.6411 - val_precision: 0.6481 - val_recall: 0.6121 - val_f1score: 0.6274\n",
            "Epoch 65/100\n",
            "61797/61797 [==============================] - 5s 86us/step - loss: 0.6317 - acc: 0.6437 - precision: 0.6484 - recall: 0.6215 - f1score: 0.6325 - val_loss: 0.6336 - val_acc: 0.6409 - val_precision: 0.6381 - val_recall: 0.6448 - val_f1score: 0.6396\n",
            "Epoch 66/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6317 - acc: 0.6432 - precision: 0.6467 - recall: 0.6256 - f1score: 0.6340 - val_loss: 0.6335 - val_acc: 0.6406 - val_precision: 0.6448 - val_recall: 0.6207 - val_f1score: 0.6305\n",
            "Epoch 67/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6319 - acc: 0.6411 - precision: 0.6448 - recall: 0.6206 - f1score: 0.6307 - val_loss: 0.6335 - val_acc: 0.6416 - val_precision: 0.6472 - val_recall: 0.6164 - val_f1score: 0.6294\n",
            "Epoch 68/100\n",
            "61797/61797 [==============================] - 5s 83us/step - loss: 0.6319 - acc: 0.6434 - precision: 0.6476 - recall: 0.6233 - f1score: 0.6331 - val_loss: 0.6336 - val_acc: 0.6393 - val_precision: 0.6374 - val_recall: 0.6397 - val_f1score: 0.6367\n",
            "Epoch 69/100\n",
            "61797/61797 [==============================] - 5s 83us/step - loss: 0.6311 - acc: 0.6438 - precision: 0.6468 - recall: 0.6271 - f1score: 0.6349 - val_loss: 0.6336 - val_acc: 0.6419 - val_precision: 0.6499 - val_recall: 0.6091 - val_f1score: 0.6268\n",
            "Epoch 70/100\n",
            "61797/61797 [==============================] - 5s 82us/step - loss: 0.6308 - acc: 0.6439 - precision: 0.6479 - recall: 0.6229 - f1score: 0.6333 - val_loss: 0.6334 - val_acc: 0.6412 - val_precision: 0.6446 - val_recall: 0.6231 - val_f1score: 0.6317\n",
            "Epoch 71/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6312 - acc: 0.6434 - precision: 0.6463 - recall: 0.6266 - f1score: 0.6345 - val_loss: 0.6335 - val_acc: 0.6395 - val_precision: 0.6376 - val_recall: 0.6405 - val_f1score: 0.6371\n",
            "Epoch 72/100\n",
            "61797/61797 [==============================] - 5s 89us/step - loss: 0.6308 - acc: 0.6434 - precision: 0.6466 - recall: 0.6246 - f1score: 0.6334 - val_loss: 0.6335 - val_acc: 0.6398 - val_precision: 0.6395 - val_recall: 0.6349 - val_f1score: 0.6352\n",
            "Epoch 73/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6309 - acc: 0.6426 - precision: 0.6449 - recall: 0.6269 - f1score: 0.6339 - val_loss: 0.6335 - val_acc: 0.6394 - val_precision: 0.6413 - val_recall: 0.6263 - val_f1score: 0.6317\n",
            "Epoch 74/100\n",
            "61797/61797 [==============================] - 5s 76us/step - loss: 0.6310 - acc: 0.6447 - precision: 0.6476 - recall: 0.6283 - f1score: 0.6358 - val_loss: 0.6335 - val_acc: 0.6394 - val_precision: 0.6419 - val_recall: 0.6247 - val_f1score: 0.6312\n",
            "Epoch 75/100\n",
            "61797/61797 [==============================] - 4s 72us/step - loss: 0.6309 - acc: 0.6441 - precision: 0.6463 - recall: 0.6296 - f1score: 0.6359 - val_loss: 0.6344 - val_acc: 0.6391 - val_precision: 0.6572 - val_recall: 0.5746 - val_f1score: 0.6109\n",
            "Epoch 76/100\n",
            "61797/61797 [==============================] - 5s 81us/step - loss: 0.6312 - acc: 0.6433 - precision: 0.6472 - recall: 0.6214 - f1score: 0.6320 - val_loss: 0.6335 - val_acc: 0.6410 - val_precision: 0.6460 - val_recall: 0.6175 - val_f1score: 0.6294\n",
            "Epoch 77/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6303 - acc: 0.6441 - precision: 0.6470 - recall: 0.6264 - f1score: 0.6346 - val_loss: 0.6334 - val_acc: 0.6410 - val_precision: 0.6451 - val_recall: 0.6217 - val_f1score: 0.6312\n",
            "Epoch 78/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6306 - acc: 0.6448 - precision: 0.6482 - recall: 0.6269 - f1score: 0.6354 - val_loss: 0.6335 - val_acc: 0.6414 - val_precision: 0.6487 - val_recall: 0.6115 - val_f1score: 0.6274\n",
            "Epoch 79/100\n",
            "61797/61797 [==============================] - 5s 80us/step - loss: 0.6306 - acc: 0.6455 - precision: 0.6499 - recall: 0.6261 - f1score: 0.6357 - val_loss: 0.6336 - val_acc: 0.6424 - val_precision: 0.6540 - val_recall: 0.5991 - val_f1score: 0.6231\n",
            "Epoch 80/100\n",
            "61797/61797 [==============================] - 5s 75us/step - loss: 0.6299 - acc: 0.6439 - precision: 0.6486 - recall: 0.6218 - f1score: 0.6328 - val_loss: 0.6334 - val_acc: 0.6401 - val_precision: 0.6417 - val_recall: 0.6285 - val_f1score: 0.6331\n",
            "Epoch 81/100\n",
            "61797/61797 [==============================] - 5s 83us/step - loss: 0.6297 - acc: 0.6456 - precision: 0.6494 - recall: 0.6261 - f1score: 0.6356 - val_loss: 0.6334 - val_acc: 0.6408 - val_precision: 0.6462 - val_recall: 0.6168 - val_f1score: 0.6291\n",
            "Epoch 82/100\n",
            "61797/61797 [==============================] - 5s 78us/step - loss: 0.6300 - acc: 0.6450 - precision: 0.6489 - recall: 0.6268 - f1score: 0.6356 - val_loss: 0.6335 - val_acc: 0.6414 - val_precision: 0.6486 - val_recall: 0.6110 - val_f1score: 0.6272\n",
            "Epoch 83/100\n",
            "61797/61797 [==============================] - 5s 83us/step - loss: 0.6309 - acc: 0.6434 - precision: 0.6470 - recall: 0.6245 - f1score: 0.6337 - val_loss: 0.6338 - val_acc: 0.6414 - val_precision: 0.6534 - val_recall: 0.5960 - val_f1score: 0.6212\n",
            "Epoch 84/100\n",
            "61797/61797 [==============================] - 5s 80us/step - loss: 0.6300 - acc: 0.6444 - precision: 0.6483 - recall: 0.6235 - f1score: 0.6337 - val_loss: 0.6334 - val_acc: 0.6388 - val_precision: 0.6399 - val_recall: 0.6282 - val_f1score: 0.6321\n",
            "Epoch 85/100\n",
            "61797/61797 [==============================] - 5s 81us/step - loss: 0.6287 - acc: 0.6459 - precision: 0.6485 - recall: 0.6288 - f1score: 0.6367 - val_loss: 0.6334 - val_acc: 0.6406 - val_precision: 0.6450 - val_recall: 0.6194 - val_f1score: 0.6299\n",
            "Epoch 86/100\n",
            "61797/61797 [==============================] - 5s 80us/step - loss: 0.6302 - acc: 0.6446 - precision: 0.6485 - recall: 0.6246 - f1score: 0.6343 - val_loss: 0.6334 - val_acc: 0.6410 - val_precision: 0.6455 - val_recall: 0.6188 - val_f1score: 0.6299\n",
            "Epoch 87/100\n",
            "61797/61797 [==============================] - 5s 80us/step - loss: 0.6298 - acc: 0.6450 - precision: 0.6489 - recall: 0.6269 - f1score: 0.6356 - val_loss: 0.6335 - val_acc: 0.6410 - val_precision: 0.6490 - val_recall: 0.6083 - val_f1score: 0.6259\n",
            "Epoch 88/100\n",
            "61797/61797 [==============================] - 5s 82us/step - loss: 0.6304 - acc: 0.6454 - precision: 0.6489 - recall: 0.6270 - f1score: 0.6357 - val_loss: 0.6333 - val_acc: 0.6407 - val_precision: 0.6451 - val_recall: 0.6191 - val_f1score: 0.6298\n",
            "Epoch 89/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6297 - acc: 0.6444 - precision: 0.6474 - recall: 0.6273 - f1score: 0.6350 - val_loss: 0.6340 - val_acc: 0.6417 - val_precision: 0.6577 - val_recall: 0.5849 - val_f1score: 0.6169\n",
            "Epoch 90/100\n",
            "61797/61797 [==============================] - 5s 84us/step - loss: 0.6298 - acc: 0.6427 - precision: 0.6466 - recall: 0.6212 - f1score: 0.6317 - val_loss: 0.6335 - val_acc: 0.6401 - val_precision: 0.6385 - val_recall: 0.6396 - val_f1score: 0.6372\n",
            "Epoch 91/100\n",
            "61797/61797 [==============================] - 5s 81us/step - loss: 0.6295 - acc: 0.6456 - precision: 0.6489 - recall: 0.6268 - f1score: 0.6359 - val_loss: 0.6334 - val_acc: 0.6399 - val_precision: 0.6398 - val_recall: 0.6341 - val_f1score: 0.6349\n",
            "Epoch 92/100\n",
            "61797/61797 [==============================] - 5s 77us/step - loss: 0.6293 - acc: 0.6456 - precision: 0.6491 - recall: 0.6278 - f1score: 0.6362 - val_loss: 0.6335 - val_acc: 0.6411 - val_precision: 0.6500 - val_recall: 0.6052 - val_f1score: 0.6247\n",
            "Epoch 93/100\n",
            "61797/61797 [==============================] - 5s 82us/step - loss: 0.6294 - acc: 0.6465 - precision: 0.6513 - recall: 0.6245 - f1score: 0.6356 - val_loss: 0.6334 - val_acc: 0.6401 - val_precision: 0.6432 - val_recall: 0.6235 - val_f1score: 0.6312\n",
            "Epoch 94/100\n",
            "61797/61797 [==============================] - 5s 78us/step - loss: 0.6292 - acc: 0.6465 - precision: 0.6503 - recall: 0.6262 - f1score: 0.6362 - val_loss: 0.6335 - val_acc: 0.6398 - val_precision: 0.6461 - val_recall: 0.6125 - val_f1score: 0.6268\n",
            "Epoch 95/100\n",
            "61797/61797 [==============================] - 5s 86us/step - loss: 0.6286 - acc: 0.6455 - precision: 0.6492 - recall: 0.6260 - f1score: 0.6355 - val_loss: 0.6335 - val_acc: 0.6408 - val_precision: 0.6485 - val_recall: 0.6088 - val_f1score: 0.6260\n",
            "Epoch 96/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6286 - acc: 0.6474 - precision: 0.6511 - recall: 0.6289 - f1score: 0.6380 - val_loss: 0.6334 - val_acc: 0.6418 - val_precision: 0.6493 - val_recall: 0.6111 - val_f1score: 0.6275\n",
            "Epoch 97/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6286 - acc: 0.6471 - precision: 0.6514 - recall: 0.6275 - f1score: 0.6372 - val_loss: 0.6334 - val_acc: 0.6407 - val_precision: 0.6443 - val_recall: 0.6217 - val_f1score: 0.6308\n",
            "Epoch 98/100\n",
            "61797/61797 [==============================] - 5s 83us/step - loss: 0.6283 - acc: 0.6466 - precision: 0.6505 - recall: 0.6259 - f1score: 0.6364 - val_loss: 0.6336 - val_acc: 0.6407 - val_precision: 0.6479 - val_recall: 0.6112 - val_f1score: 0.6269\n",
            "Epoch 99/100\n",
            "61797/61797 [==============================] - 5s 79us/step - loss: 0.6291 - acc: 0.6461 - precision: 0.6494 - recall: 0.6287 - f1score: 0.6368 - val_loss: 0.6336 - val_acc: 0.6413 - val_precision: 0.6494 - val_recall: 0.6089 - val_f1score: 0.6263\n",
            "Epoch 100/100\n",
            "61797/61797 [==============================] - 5s 85us/step - loss: 0.6282 - acc: 0.6450 - precision: 0.6486 - recall: 0.6260 - f1score: 0.6352 - val_loss: 0.6336 - val_acc: 0.6386 - val_precision: 0.6410 - val_recall: 0.6238 - val_f1score: 0.6302\n",
            "Test loss: 0.635267674280971\n",
            "Test accuracy: 0.6407932891466446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDJNtM_n6iwd",
        "colab_type": "code",
        "outputId": "d8b9f579-be04-4bb4-aadd-6f39db3f131d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "loss_ax.legend(loc='upper left')\n",
        "\n",
        "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "acc_ax.legend(loc='lowere left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAAEJCAYAAAAJnlldAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3RUxdvA8e9mU0kPJEsSQkINvSgg\nAgEJPQEjHQVFEBEBCyJgeUXlh6hYUbBgQQFFUEMzCAoiAZUSAgaQGrJJIL0XUrbM+8clS0LaQgpt\nPufkLHvv3LmT1bNPZu7MMyohhECSJEmSbkMWN7oBkiRJklRXZJCTJEmSblsyyEmSJEm3LRnkJEmS\npNuWDHKSJEnSbcvyRjegLvj7+9/oJkiSJN2STp8+faObUKtuyyAHt99/KEmSpLp2O3YQ5HClJEmS\ndNuSQU6SJEm6bckgJ0mSJN22bttnclcTQpCRkYHRaLzRTbllWVhY4ObmhkqlutFNkSRJMssdE+Qy\nMjKwt7fH1tb2RjflllVYWEhGRgYNGza80U2RJEkyyx0zXGk0GmWAqyFbW1vZE5Yk6ZZyxwQ5SZIk\n6c5zxwxXmkuny0CtdsLCQn40kiTdOELAd9+BXg9dukC7dmBtfaNbdeuRPblShDBQWHgevT691uvO\nycnhu+++u65rH3/8cXJycswu//HHH/PVV19d170kSbrxhIBnnoGHH4YpU6BrV3BwgJ49Yf582LoV\nsrJudCtvDTLIlWEBWGA0Ftd6zTk5Oaxbt67Cc3q9vsprv/jiC5ycnGq9TZIkVcxoVAJNXTh3DmbO\nBBcXsLMDe3twc4OnnoKEBOW+zz0HH3+svJ4+DT/8AHPmgJUVLFsG998PDRsqQe+VV+D48bpp6+1A\njsmVolKpUKmsEKL2g9x7771HXFwcISEh9OrVi/vuu49ly5bh5ORETEwMO3bsYObMmSQlJVFUVMQj\njzzC+PHjAQgMDOSnn37i0qVLPP7449x9990cOXIEjUbDJ598UuWEmpMnT/Lqq69SUFBA06ZNWbJk\nCc7OzqxevZoffvgBtVpNy5Yt+eCDDzh48CBvvPGG6bNYu3YtDg4Otf5ZSFJdyMyEkyeVL36LUn++\nFxfDunXg6wt9+oBlNd96UVEwZowSeFavhtatKy5XWKgEwwYNKq8rLQ0+/RRyc5Vhx+hopRdmZQXj\nxoGXFxgMkJgIn30GX34JvXrBH38oPbl33wWVSmnD5a8DCgrg4EGlzO+/w5tvwpIl8Nhj8L//gUZz\nbZ/bbU/chlq3bl3uWEpKiunfiYnfisjIfhX+RER0FxER3Ss9X9lPYuK3VbYpPj5eBAcHm97v379f\ndO7cWcTFxZmOZWZmCiGEKCgoEMHBwSIjI0MIIUT//v1Fenq6iI+PF23bthX//fefEEKIp59+Wmza\ntKncvT766CPx5ZdfCiGEGD58uDhw4IAQQogPP/xQLF68WAghRO/evUVRUZEQQojs7GwhhBBPPPGE\niIiIEEIIkZeXJ3Q6XZWfoyTdaIWFQmzYIERIiBBWVkKAEPffL0ROjnI+O1uIgQOV4yBEo0ZCTJ0q\nxLlzFde3bp0QDRoI0bixEK6uQtjZCfHJJ0Lk5grx559CvPWWEOPHC9G2rRAWFkLY2wvxwgtCpKaW\nr8toFGL4cOW+trZCODoK4e0txEsvCZGYWL58dLQQU6YIoVYL8dRTyvXmSEsTYs4cISwtlXusX2/e\ndRWp6LvzVldvw5Xh4eEMGTKEQYMGsXLlygrLbNu2jaCgIIKDg5k7d67p+DvvvMPw4cMZPnw427Zt\nq+OWWiDqapziKh07dsTHx8f0fs2aNdx///2MGzeOxMREYmNjy13TpEkT2rZtC0D79u25ePFipfXn\n5uaSm5tLjx49ABg5ciQRERGAkoj1+eefZ/PmzajVagDuuusu3nrrLVavXk1ubi6W1f3JK0m1yGCA\nSZPAz08Zutu9W+n9VOTsWZg3D5o0UXpEBw/C7NlKT+aXX6B3b/jnH+jbF/78E1auhB9/hMGDYcMG\n6N4ddu26Ul9mpnLPBx9Unn9FRsKxYxAQoAwtOjrCfffBCy/AgQNKz+rll2HECHj7baXNr72m/A4l\nNmxQ2vL++0rvKycHLlyAN96Axo3L/07Nm8PXXyvP2j76SOnBmaNhQ+UeJ07A0KFw5ox5190x6iOS\n6vV6MWDAABEXFyeKiorEiBEjxNmzZ8uUiYmJESEhISIrK0sIIURaWpoQQojdu3eLRx99VOh0OpGf\nny9GjRolcnNzq7xfdT25qhQUxIucnAhhNPfPKDNV1JObPn16mfcTJkwQly5dEkIIMWnSJLF//34h\nRNmeXOk6vvzyS/HRRx+Vu1dJTy4nJ0f069fPdDw2NlY88MADQgjlv8k///wjlixZIoYOHWrqtZ06\ndUp8/vnn4r777hPnKvhzV/bk7hyXLgnx999CfPedEP/9V3XPwmgU4uxZpSc0b54QkyYJERQkRK9e\nQkyerNRR1f86RqMQTzyh9Hr69FF6UCCERiPEyy8LERsrhE4nxMaNQgwapJxTq4UYNUqI7duF0Ouv\n1LV9uxDOzkoZBwchduwoe6/oaCHat1euX7ZM6Z25uAihUgnx9NNCXB7gMLVr1SohXn1ViLCwints\n//0nxLhxyv0efFCI4mIh0tOF8PAQolu3sm272d2OPbl6+VM9KioKX19fU68lODiYXbt20bJlS1OZ\nDRs2MHHiRJydnQFMWTXOnTtHt27dsLS0xNLSEn9/f8LDwwkKCqqTtlpYWAECIfSoVFa1Vq+9vT35\n+fmVns/NzcXZ2Rk7Ozuio6M5evRoje/p6OiIk5MTERERdOvWjc2bN9O9e3eMRiOJiYn07NmTu+++\nm7CwMC5dukRWVhb+/v74+/tz/PhxYmJiaNGiRY3bId1atm9XeilRUWV7Uu7u0L8/fPgheHpeOX7u\nHAwYAHFxynsbG+VZk5sbODkpvZlvv1XOOTsrz8ssLKBjR5g1C0JCYPFi+PxzWLAA3noL8vOVdnz7\nrfLM6c03oVEjSEkBHx+lxzZ1qnKfqw0ZAvv3K2XmzoW77ip7vnlz+PtvmDhRee4FEBysPNfq1Kls\nWZUKHn206s+rbVtYv17pAb74otJjc3GB9HT47Te4PFAi3SD1EuSSk5NpXKp/rtFoiIqKKlNGq9UC\nMGHCBIxGI7Nnz6Zv3760adOG5cuXM3XqVAoKCjhw4ECZ4Fhi/fr1rF+/vsZtVamUhShC6IDaC3Ku\nrq7cddddDB8+nICAAO67774y5/v27csPP/zAsGHDaNasGV26dKmV+7799tumiSc+Pj68+eabGAwG\n5s2bR15eHkIIHnnkEZycnFi2bBkHDhxApVLRqlUr+vbtWyttkOqOXq98of/6q/KFWlSkTLBo2lQZ\nxuvXr2x5g0EpU9FkCSGUGX1z5oC/vzJVvXt3aNYMIiJg715lyO/sWQgPV6a05+bCAw9AXp4ycaJH\nD+jQQZlYUfqekZHKJInUVOW9TqcEsbFjleCZmqoErTffVK6xt4fRo5Wf2FhluPHMGWU4Mzi44skj\neqOeY8nH6NK4C23aqKhqxY6TE2zapEz0aNdOGZasqRdeUIL4rFnKZ/nCC9C5c83rlWqoPrqLv/76\nq3jppZdM7zdu3Chef/31MmWmT58uZs6cKYqLi0VcXJzo27evaULEJ598Iu6//37x6KOPiueee06s\nWrWqyvvVZLhSp8sVOTmHhE6XaVb5O40criyvoECIzFr830WnE2L1aiH69RNizZrKy/37rxDNml0Z\nuuvTR5l00bmzMgHB1laIQ4fKtnPAAGVixVNPCXH+/JVzm/7dKVq+MEHQ5B/xwAPKRIuKbNum3GvY\nMGVYbuRIZQLG778r541Go/j00Kfi17O/CoPRUOXvqdcLsWWLUtfkycrvfT2MRqPYfGqzaLO8jeA1\nxJt737y+imogvzhfxGfHCyGUiR8PPqgM99aE0WgUhxMOi/RL6bXQQvPcjsOV9RLkIiMjxdSpU03v\nP/vsM/HZZ5+VKfPKK6+In376yfT+kUceEf/++2+5up577jnx559/Vnm/mgQ5g6FQ5OQcEkVF8su8\nIjLIlZWQoMy0a9hQiMOHzbsmIkKI+fPLB0ajUQluLVsqgcvFRXmdNEmZJVja5s3KzD5vb+VL9eq6\nUlKE8PVVzickKAFl9GilvuBgZSaihYUQd98thGurU4IXnASvIXgNEfzdcPF33N8ipzCnwvZ/+Gm2\noM+bwubZDoIWO8T7718599mhz0z1tFjWQizdt1RkF2ZXWM+1yi3KFdmF2SK7MFtcyL4gdpzbId77\n+z3Rd1VfwWsI/4/9xeA1g4XqNZXYdLL8rOO8ojwRdiZMPL/jeTFl0xQx4acJYsyGMWJf7L4q76s3\n6EVeUV6FQVtv0IsvDn8hPN/1FNb/sxYbjm+otB6j0Wj2Z3HgwgER8HWA4DWE29tuYsXBFUJnKP9X\nQGp+qlh9dLU4cOGAWfVWx5wgt2fPHjF48GAxcOBA8fnnn1dYJiwsTAwbNkwEBQWJ5557rsy53Nxc\nERAQUK6jU1fqZbiyY8eOaLVa4uPj0Wg0hIWF8d5775UpM3DgQMLCwhg9ejQZGRlotVp8fHwwGAzk\n5OTg6urKqVOnOH36NL17966ztpY8h6uLtXJS3dm+XUmBtHixMlxXH+LilGdRiYnK86f+/ZVhw169\nlAW88+crz7VmzVJm6NnawlvvFPHqu4noLTPZtlfD7xs1NNaoKShQhut++EEZ4tq4URmWe/NNeP11\n+OtvQc+QKC46bOaccRcJB3pxV8eX2PqzI15eUKAr4Osj6yg2FBPQNIC27m1Z/WMWA+eso9XSbzE6\nxlLQyIt2i725u9fdLP1oDt985sKBo9kYh4Zgb2XNiu4nSHDczNK/l9Lr614AOFg70MSpCc1dm9PC\ntQVWFlZ8nf01DMyiqMAVq4fGMnTSAaANp9NOM2fHHAY1H8RjXR/jk4hPmL9zPl8f/Zqwh8Jo7tr8\nuj5nozAyK2wWnx3+rMLzPk4+fBL0CdPumobeqKffN/2YGDqRvx/7mxauLfjpv59Ye2wt4bHhFBuK\nsVHb4G7vjp2lHekF6ezR7uHYk8fQOFxZYLY3di8/n/yZiIQIjiQd4ZLuEgDWamsa2jWkhVsLWri2\n4HDiYY6nHKdnk574ufgx/qfxJOYl8vQ9T5dp48Wci8wIm8EvZ34hqFUQ/+v/P+7yvOphIXA2/Syv\n7H6F9SfW42HvwTuD3mHb2W3M2jaLzw9/zrCWw1ChwiAM7L+wn7/i/8IojLjYuhA1IwofZ59yddYm\ng8HAokWLWLVqFRqNhjFjxhAYGFjmEZJWq2XlypWsW7cOZ2dn0tPLZpD68MMP6d69e522szSVEPUz\nX37Pnj0sWbIEg8HA6NGjefLJJ1m2bBkdOnRgwIABCCF466232Lt3L2q1mhkzZhAcHExRUREjR44E\nwMHBgddff900hb4y/v7+nD59usyx1NRU3N3dzWprXt6/qNXO2Nn5Xdfveju7ls+xPhiNypTsV19V\nnoO4ucH338OgwUYsVBacOwf79inTv/38IKswCxdbl0rrO5V2iifDnkRn0NHdqzvdvbvTwaMDLVxb\nYG9tD0B2YTa/RUQzc+Epcu2O0z34OEUWmfwX4UZRtistmtpxJqYAtW0Bjm75ZBZmoHbIRGWfht46\n7apfwIKGdh7kZdhTlGeHd2Mb3NyLKdAXUKgvRAhBcTFk5BRisEkHoYKUDqA5RmN7T94a+Capl1J5\n75/3SMpLMlXb0K4hucW5FBuKIakzXOxOiy6JOHpf5N+kf3G1c+WVvq+wK2YX289tZ+fDO+nnpzzA\nyyrMYtvZbVzIucDFnIvE58RzPvM80ZnR5BXnMbLNSF7s8zLRxxrxzLEeONk6sW/KPoK/D0abpSXq\nySi8HJUZIbtjdjPmxzGoVWq2PLiFnk16AmAwGlCpVFioql7FJIRg9rbZfBLxCVO7TKW9R3sAbC1t\naefejvbu7XG3L/v/Y0JuAt2/6I7OoKNAX0BecR4tXFvwQJsHGNJiCAG+AdhaKgkUTqScoNsX3Qhs\nFsgvD/6CSqViw4kNPPTzQ1irrenq2ZUeXj3wdPSkQFdAgb6AlPwUojOjic6IxsnGiUX9FzG67WgK\n9YVMDJ3IxlMbmX7XdO7zuw8vRy9OpZ1i/s756Aw6JnWaxE///URmYSYh/iEMaj6IDh4daNigIR/8\n8wHf/vstNpY2PNfzOeb3no+jjSNCCEJPhvLCrheIy44z/Z5tGrUhxD+E7l7defDnB+nh3YOdj+ys\n9jOtSkXfnaUdOXKE5cuXm9IGfv755wA88cQTpjJLly6lWbNmjB07ttz1x48f56uvviIgIIDjx4+z\ncOHC626rueotyNWnmga5/Pz/UKmsaNCgVV0075Z2I4LcyZNw8aLSa9IZi8kuzMbW0pbCPDumTbVk\nyxZlQsK8eTBuupbTTV7Csv0WfP/9gujND5rqaTFxGedbzeFeq5m0j1tGbIwaKytlsoCjIxw1fM+h\nxtNBZ0eDwtYUuhxBryowXe9koaFYr6fQ4spfpmqVJW3d29CoQSNSczM5E5+BThTiYGOHpqEtTnb2\nWBS5EXfajcyEhgzv68Xwft642rnw19EUln97kWLrJNR2l+h+bwFuHkXYqG2wtbTF1tLW9IVlobLg\nHu97CG41HBcrDf+mHeCpX5/iUMIhAAY0G8DLAS/T1Lkp4bHh7I3bi7ONM5O7TGb/pi4kJ8PChcps\nwaNJR5n3+zx2nt8JwPJhy5nVY1a1/x2EEBTqC7GzsjMd+zv+b/p/2x9Ha0fSC9IJHRfKyLYjy1x3\nJv0Mwd8HcyHnAn19+3I+8zzaLC2O1o70adqHgKYBNHdtToG+gAJdAfbW9nTw6IB/Q39e2vUS7+9/\nn+fvfZ6lg5aavWFvREIEE0Mn0tunN1O6TKFP0z6VXvvxgY95evvTrAhaQaMGjXjo54e41+detj20\nDUcbR7PuV8JgNPDs9mdZfmh5meP3+d3HFyO+oKVbS7ILs/lw/4d8fPBj0guu/L9ko7bhyW5P8kKf\nF8r0Ks3xVeRXTNs6jaUDlzKv97xrurY0f39/2rdvb3o/fvx4U+YlgO3bt7N3715TZqRNmzYRFRVV\nJljNnDkTPz8/IiMjy0wiNBqNTJ48mXfeeYe///5bBrmaqGmQu3TpLEIUY2/fvvrCd5i6CHJ6o54/\nzu3jzY2bSdKdxr5RJrl6ZRf3gkw3EqLdEOix9Yqm2C4WI6X2tCtypIV9V0K6dUdv1PHZ4c8w6NQY\n0pqDxwlGWL/PwkHPMm/bq/wp/gepbcD9FNbnRtPx7FqEzpZUcZKUVu9Q1G4Vztl9GJDxA6nnvTl0\nWE+h4wlodArcosE1Gku1Ja3cWtLTvwUPDWlN3/atsVZfSQ2fl6ekcvLzq+D31JefFXjokDIk+eqr\n1z4TzyiMbDu7DQ97D3p497ima4UQ/Bb9G7HZsTx+1+M12u3926Pf8ujmR5nWdRpf3P9FhWXSLqUx\nbcs04nPiaenWkuYuzUm9lEp4bDhnM85WeI2FygKjMPJUj6dYNnRZne1IL4Qg6PsgdsfsRm/U08un\nF9smbsPB+vpT2uUU5XAx5yIJuQkYhZEBzQeU62EJIUjITeB4ynFismIY3no4TZyaXPfvMHrDaH45\n8wsHph2gq2fX66qnup6cOUHuiSeewNLSkg8//JCkpCQmTZrE1q1b2bJlCwUFBTz++OOEhobWW5CT\nKS0qYGFhjU5X+Zo2qWZ+PLyTQwmHyBEXSbp0gV1n95JnzAC9DaS0R3W2IU0a+pKbqyKrMBM373Qc\nHVQkneyJMWkidkYPCoqLaN2+gC59kogtPsyKQ8spNhTzaJdHWdR/EbbGRsz4bRI/n3yOpCPrOCQO\nMaXLVJ73/5yNFz/m/3gOy34XUKlUxF/Yj1qlZkGvBSwOXIzl5W2WdDpL/vuvMwUFnXF0VHp7np5l\np8dfzcFB+alIRdPeu3eH0NDr+xwtVBYMbz38uq5VqVQMaTnk+m58lcldJnO31920adSm0jKNGjRi\n04RNFZ5LyksiJT8FO0s7bC1tyS7K5kTKCY6nHMfd3p2nejxVZwEOlM9iVcgqun7eldYNWxP2UFiN\nAhyAk40TTu5OtHWv/NGKSqXC28kbbyfvGt2rpK4vRnxBp8868ea+N9kwdkON66yIRqMhKenKsHhy\ncjKaq5JlajQaOnfujJWVFT4+Pvj5+aHVajly5AiHDx9m3bp15Ofno9PpaNCgAc8//3ydtNWkXqa3\n1LOazK4UQojCwosiJ+eQMFYzBbqudenSpdLjSblJ4psj34ik3CSz6yvUFYpCXeF1t0dn0InvD30v\nxmwYI3w/8BUrI1ZeU2aYmPgC0X7+DNPsOxa4CtXsdoKRDwu/oJ/EL7/liv/+E+LZZ5WZhRqNEKGh\nV67PylLyBI4eLcS+qybEFemLRFp+WpljeoNezA6bLXgNMXfH3DJtXXdsnbD+n7Vov6K9ePevd6/p\nc5RuT5XNoLyVXMi+IGIyY677+upmV+p0OhEYGFgme9WZM2fKlNmzZ4+YP3++EEKI9PR00bdvX1Me\n3hI///zz7TW78lZTekG4SmVzg1tzhRCCtVFrOdfrHF7ve2EURu5tci97Ht2Dlbrqhev74vYxMXQi\nFioLQseFVjicoTfq+b8//o9tZ7cR2CyQB9o8QAePDuzR7mFD5G9sPb2VAstE1IXuWOX7Mj17OptO\n7GDt2C9wtXOt9N7nzsE7X0XzRfZYhOYIXfLm80SbhWSm2JNUAHc/oGSfKMkM8cEHSj5Atbpstghn\n5yuLha9mrbamYYOGZY6pLdR8NOwjFvRZgLejd5newIQOExjRegQNrBrUaS9BunWUTCy6ldVGr7Aq\nlpaWLFy4kGnTppkmEbZq1arMJMKAgAD++usvgoKCUKvVzJ8/H1fXyr8f6pp8JlcBvT6bgoKz2Nn5\nY2l5bQ+eK/Puu+/i6enJxIkTAWVj0wYNGjBhwgRmzpxJTk4Oer2eZ555hoEDBwLQtWtXjhw5AijP\nX2Zvm82nEZ9inWfN/GHzcbVzZe5vc2me2Jx2F9vx5JNPMmDwABbtXMTmbZtxzHDEKdcJv/F+fHnu\nS5wMThTqCym2LOYx98dYOftKouzU/FTG/zSe3drd9PDuwb9J/1JkKDKdVxU7oY4dSLOcB2mjvp+C\nfEt2XnoPAl/CzuhBj8YBdGnhhbeTJ442jhTl2/LvEUt2RB0hwTIcPI9gLZz4OHA10++7viE2SZLq\nVnXP5G5Fd2aQW71aSfddCSGMGIz5WFjYYaEys7M7dSo88kilp//77z+WLFnC2rVrAQgKCuKrr77C\n3d2dwsJCHBwcyMjIYPz48fz222+oVCpTkDMYDUzbOo1vjn7Dgt4L2D5vO0ePHGXHjh08/fvTnHE8\nw7rh6/jw5Q8pCiniaMpRLLAoM0FjeNPhOO114oOPP2D8T+P5U/snQ1sOxcfJh8xEV7ZfWEeROpUX\nO37GC8MmY1DnsePcDo4nnWH90r7E7OtB+G4r/PyufI7nzsHCzyL4Mf3/0DueB6eLYHWpzO+tMtjg\nZ9mTEZ0DmHvf4zR1bmre5ylJUr27HYOcHK6siGn4ylhlsWvh4O3AUfVRRqwZwcXsixiaGSiyK0II\nwZL3lrD79G4u2V4iS53FjhM7aOXVinzXfEJPhvL9se/5+eTPvNbvNRb2W8gOdgBw+PBhFvZYyNKM\npczaNYtLPS+hTlfz9l1v8+vyX2k3pB34QGD7QAZ6D2T0d6P5/P3PeSXgFXp692TT6U38ff4IOcWZ\nkNMUfvyLRYl3sdQWBg92YNSo0URtgVO/KYuTu3dXcgyWaNkSvn+3G18XbmfbNlj7nWD7H3m06ZRH\n/0EF9O1fzJAezbC1unmGfCVJurPcmT25agghyMs7gpWVO7a215dBIC47jnXH1nEo4RCHEg6ZFnG6\nql2xF/ZcMF4AoJFVI9J0aVVVhQoVbw18i/m95wNXhjGXLFlC69at6XhfR7p/0R27Ajve7fEuDw9/\nmOTkZPbs2cN3333HlClTaN/+AVavLubcuXjy87fi759Go0aLeeEFGDJU8NOPkJqq4uhRZf+tjRsh\nPl65/wcfwLPPKv++2RaDS5JUe2RP7g6hUqlQqayuO7VXbFYsvb7uRUJuAs1dm3Nvk3uZ03MO/pb+\nfPPuN2RlZhG6IpQ/kv4g9J9QOuk7Mf+h+VyKv8SzLz7L6++/Dg1g0QuLCP0mlKbOTXGzcyt3n27d\nurF+/XpGjhzJkclHmDppKkOfGsrFixdp3Lgx48aNY88eT2bPbsHFi0rKMrW6BXr9s4SF6TAalU0i\nv/lGhbW1MvW9WTMYOVLZTuXwYWUR9v331/QTlSRJujFkkKuEhYU1RqPumq9Lu5TGkLVDyC/O58gT\nR+jSuOyWOZ/kf4KHhwfdW3ane8vuPN7ucZ588kmWz1lOhw4daOfajoFNB9KkSROWJS8rd31pgwYN\n4siRI4SEhKBSqVjw/ALc3d3ZuHEjX331FfHxw4mKmkGbNsXMn5/Cv/++iFqdT1pae3x8ptCjRxOe\nf17Z2+tqKhV066b8SJIk3arkcGUlCgrOYzDk4eDQqfrCl+UX5zNg9QCOJh3lt4d/o6/vjduP7auv\nYNo0GDECfvoJrK2rv8YccrhSkm5fcrjyDqJSWSOEDiGEWeuohBA8vPFhDiUc4udxP9dpgCsqguPH\nlZ+771Y2qSyh08EXX8Ds2TB0qLLJZW0FOEmSpFuNDHKVsLCwAgRC6E3b71Tlm6PfsPHURpYOXMoD\nbR6o1bYUF8Nff8GOHbBzp7J9i67USGqPHjB5sjKlf+1aZQbkoEFKuigbObFRkqQ7mAxylSid9QSq\nDnLaLC3PbH+Gfr79mNtrbq3cPz0dwsJg82YluOXnK7kPe/WC555TenBt2ihB76uvlD3LrKyUSSJT\npsCQIRXnSpQkSbqT3DFfgxYWFhQWFmJra2tW+bKbpzaotJzBaGDypskAfPvAtzXay6nEoUPQty8U\nFoKXFzz8MAQFKXuiOV6VgKVjR2V6/3//gUYDjRrV+PaVKiwsxKKiWSqSJEk3qTsmyLm5uZGRkUFu\nbq5Z5Q2GfNLS9uLkVIydXWGWmTcAACAASURBVOX7yn0U+RHhseF8FPgRDXQNSC29Wvo6LVzohJ2d\nFZs2ZdOli940+7GwUPmpiIeHsmloLdy+UhYWFri5lV/KIEmSdLO6Y4KcSqWiYcOG1Re8zGh04dSp\nudjbv4K7e68Ky6w4uILF+xczpt0YZveZXSuJfk+fVoYn/+//YMiQG5fUVJIk6XYgx54qYWFhhZWV\nB8XFCRWef/fvd5n962xC/ENYO3JtrWWyf/99ZTbkrOo3apYkSZKqIYNcFWxsvCkqulju+NK/ljLv\n93mMaz+OH8f+iI1l7UxhTE6Gb79VZkpetQ+hJEmSdB1kkKtCRUEuvzif1/58jRD/EL4f9X21+7hd\nixUrlOUCc2tngqYkSdId7455Jnc9rK29yMn5p8yxHdE7KNAX8Mw9z6C2UFdypXlOnIB33gFvbyVn\n5IoVEBICrVvXqFpJkiTpMhnkqmBj441Ol4bRWISFhTIkGXoylIZ2DQnwDahR3YWFMG4cREeDXg8G\ng3J83ryatlqSJEkqUW/DleHh4QwZMoRBgwaxcuXKCsts27aNoKAggoODmVtqzG7p0qUEBwczbNgw\nFi9eTH2l27SxUbaSLypSJp8UG4r55cwv3O9/P5YWNfv74P/+T1nbtnmzEvBiYpSeXa+KJ3JKkiRJ\n16FeenIGg4FFixaxatUqNBoNY8aMITAwkJYtW5rKaLVaVq5cybp163B2diY9PR2AyMhIIiMj2bJl\nCwAPPfQQBw8e5J577qnzdltbewFQXJyAnV0zdsfsJrsom1FtR11TPQYD5OWBs7Pyfs8eZRbljBlK\nZhIAP79abLgkSZIE1FNPLioqCl9fX3x8fLC2tiY4OJhdu3aVKbNhwwYmTpyI8+VIULKmTaVSUVxc\njE6nM702qsu0HqVc6ckpk09CT4biYO3AwOYDr6mehQvB1VXppb39Njz6KDRvrjyPkyRJkupOvfTk\nkpOTady4sem9RqMhKiqqTBmtVgvAhAkTMBqNzJ49m759+9K1a1fuuece+vTpgxCCSZMm0aJFi3L3\nWL9+PevXr6/VdpcOcgajgU2nNxHUKghbS/NSg5X4+Wdo2VKZOfnCC8r+beHhyialkiRJUt25aZYQ\nGAwGYmNjWbNmDe+99x6vvPIKOTk5xMbGEh0dzZ49ewgPD2f//v1ERESUu378+PGEhoYSGhp6/Y0Q\nAp5/XpkNAlhauqJS2VBcnMA/F/4hJT+FUW2ubagyJkbJYjJrFkREQFycsuN2797X30xJkiTJPPUS\n5DQaDUlJSab3ycnJaK5a7azRaAgMDMTKygofHx/8/PzQarX8/vvvdO7cGXt7e+zt7QkICODIkSN1\n09BLl+C995Q9alCGSkvWyoWeDMVabU1Qq6BrqnL7duV16FDl1ccHulS+2bckSZJUi+olyHXs2BGt\nVkt8fDzFxcWEhYURGBhYpszAgQM5ePAgABkZGWi1Wnx8fPDy8uLQoUPo9Xp0Oh2HDh2qcLiyVjRo\nAHZ2kJJiOlQS5Dad2sSg5oNwtHGsooLytm9X1sDJtW+SJEn1r16CnKWlJQsXLmTatGkEBQUxbNgw\nWrVqxbJly0wTUAICAnBxcSEoKIjJkyczf/58XF1dGTJkCE2bNmXEiBGEhITQpk2bcgGy1qhU4O5e\nJpW/tbUXiblxxGTFENjs2u5bVAS7dim9uFpKbSlJknRDXe9ysJMnTzJ+/HiCg4MZMWIE27Ztq58G\ni9tQ69atr//ibt2EGDbM9Pbs2efE26HWgtcQf8b8eU1V7dolBAixZcv1N0eSJKm+VPfdqdfrxYAB\nA0RcXJwoKioSI0aMEGfPni1TJiYmRoSEhIisrCwhhBBpaWlCCCHOnz8vYmJihBBCJCUlid69e4vs\n7Oza/yWuctNMPLlpXNWTs7Hx5lROMQBdPbteU1W//qrs1t2/f622UJIk6YaoyXKwZs2a4Xd5QbBG\nozHt8VnXZFqvq3l4KKlHLrO29uJsLrRy9cPJxumaqtq+HQIC5FIBSZJuHaNGXZlBPn78eMaPH296\nX5PlYKVFRUWh0+lo2rRpHfwGZckgdzUPD2XiiRBweXblmTwI8PW7pmouXIDjx+WCb0mSbi01WoZF\n2eVgSUlJTJo0ia1bt+LkpHQSUlJSmDdvHm+//TYWFnU/mCiHK6/m7q4kk8zPByDHYEtKEbR387im\naq5eOiBJknSrq8lyMIC8vDyeeOIJ5syZQ5d6Wkslg9zVPC4Hs8vLCI6lK8mZ2zqbP+ao08Gnn0LT\nptC+fa23UJIk6YaoyXKw4uJiZs2aRUhICEPr8a9/OVx5NXd35TU1FZo352jScQBaXcNztf/9DyIj\nlXRecumAJEm3i9LLwQwGA6NHjzYtB+vQoQMDBgwgICCAv/76i6CgINRqtWk52ObNm4mIiCArK4uN\nGzcC8NZbb9G2bds6bbNKiHrat6Ye+fv7c/r06eu7OCICuneHLVtgxAhGrR9FRFwYYUOH0rHj5mov\n379fSdn18MPwzTfX1wRJkqQboUbfnTcpOVx5tauGKw8nHqadq5tpJ4Kq5OUpwc3HB5Ytq8tGSpIk\nSeaQQe5qpYYr0y6lEZcdR8dGPhQXJ1R76auvKrmdV6++snecJEmSdOPIIHc1OztlYVtKCocTDgPQ\nxcOf4uJkjEZ9lZdu3w7DhsFVS0IkSZKkG0QGuYpcznpyOFEJcl0bdwWMFBcnVXpJUZGypY7cYUCS\nJOnmIYNcRS4vCD+ceJgWri3wcGoFUOWQ5alTYDBAx4711UhJkiSpOjLIVaQkyCUc5m6vu8vsEF6Z\n48pKAzp0qI8GSpIkSeaQQa4i7u7kZSYTmx1LZ01nrK29gKqD3LFjSjJmf//6aqQkSZJUHRnkKuLh\ngVan7ETQ3LU51tYeqFSWFBdXHeTatFECnSRJknRzkEGuIu7uaB2UmZR+Ln6oVBbY2DShsDC+0kuO\nHZPP4yRJkm42MshVxMMDrYvyTz8XPwBsbf0oLIypsHhWFsTHyyAnSZJ0s5FBriIeHsQ6g62FNRp7\nJcO2EuS0FRYvmXQig5wkSdLNRQa5iri7o3UBXyt3VJczLNva+lFcnIDRWFSu+LFjyqsMcpIkSTcX\nGeQqcnm40o8rublsbZsBUFgYV674sWNKGi8fn3proSRJkmQGGeQq0qiR0pMrbmA6ZGvrB1Dhc7lj\nx5T1cXJbHUmSpJuLDHIVyFPpSLMHv/wr6wGuBDltmbJCyJmVkiRJN6t6C3Lh4eEMGTKEQYMGsXLl\nygrLbNu2jaCgIIKDg5k7dy4A+/fvJyQkxPTTsWNHdu7cWadtjc2KBcAv88oxa2svVCrLckHuwgXI\nzpZBTpIk6WZULzuDGwwGFi1axKpVq9BoNIwZM4bAwEBatmxpKqPValm5ciXr1q3D2dmZ9PR0AHr2\n7MnmzcpmpVlZWQwePJjevXvXaXu1WVoA/FJ1pmMWFpbY2PiUC3Jy0okkSdLNq156clFRUfj6+uLj\n44O1tTXBwcHs2rWrTJkNGzYwceJEnC9vxNawYcNy9ezYsYOAgADs7OzqtL2mIHcxr8xxW9tmlQY5\nmbNSkiTp5lMvPbnk5GQaN25seq/RaIiKiipTRqvVAjBhwgSMRiOzZ8+m71Ubs4WFhTFlypQK77F+\n/XrWr19fK+2NzY7FRqjRxGeWOW5r60dGxrYyx44dA29vcHWtlVtLkiRJtahegpw5DAYDsbGxrFmz\nhqSkJCZNmsTWrVtxcnICICUlhTNnztCnT58Krx8/fjzjx48HwL+GWZK1WVp8ccYiNQ2MRrBQOrzK\nWrkkDIYC1GqlN3n4MHTuXKPbSZIkSXWkXoYrNRoNSUlXNhxNTk5Go9GUKxMYGIiVlRU+Pj74+fmZ\nencAv/76K4MGDcKqHjIga7O0+Fm5KxvEZV7pzZXMsCwqUtbKJSQo+8jdd1+dN0mSJEm6DvUS5Dp2\n7IhWqyU+Pp7i4mLCwsIIDAwsU2bgwIEcPHgQgIyMDLRaLT6lVleHhYURHBxcH81Vgpy9soccqamm\n41cWhGsB+OMP5fiAAfXSLEmSJOka1ctwpaWlJQsXLmTatGkYDAZGjx5Nq1atWLZsGR06dGDAgAEE\nBATw119/ERQUhFqtZv78+bheftB14cIFEhMT6dGjR523Nb84n9RLqfj5+ikHUlKUPXS40pMrKFAW\nhO/aBW5u0KVLnTdLkiRJug719kyuX79+9OvXr8yxZ555xvRvlUrFiy++yIsvvlju2iZNmrB37946\nbyMok04A/NxbKwdK9eRsbDxRqawoLNQiBOzcCf37mx7ZSZIkSTcZ+fV8lZLlA77e7ZQDKSmmcyqV\nGhubphQWajl7VlkILocqJUm6k1xvYg+AjRs3MnjwYAYPHszGjRurvdesWbPYuXMnOp2u2rKVuWlm\nV94sTNlOfC9PmSwV5ADs7JS1cuHhynsZ5CRJulPUJLFHVlYWy5cv5+eff0alUjFq1CgCAwNNa6Mr\n0q1bN1asWMHLL7/M0KFDCQkJ4a677rqmNsue3FW0WVqs1dY0dmmiLH4rNVwJV/aV27VL2XWgVasb\n1FBJkqR6VpPEHvv27aN37964uLjg7OxM7969q30MNWXKFDZu3MjatWtxcnJi7ty5DB48mOXLlxMX\nV35HmIrIIHcVbbYWX2dfLFQW4O5eYZArKkph927BgAFy5wFJkm4vo0aNMv1cnWCjosQeycnJZcpo\ntVpiYmKYMGEC48aNI/zysJc511amVatWzJ07l3feeQdbW1tWrFjByJEjefTRRzl16lSV18rhyqto\ns7T4ufgpbyoJcufOdSEjQyWHKiVJuu2EhobW6PrKEntcr/Pnz7NlyxZ++eUXrKysTMn63dzc+P77\n75k5cyZ/lKznqoDsyV3FnCAXGalEt6uW+kmSJN3WapLYw5xrrzZq1CgefPBBsrOzee+99/j111+Z\nMWMGnp6e2NjYVJrmsTQZ5Eop1BeSkp9STZBrxuHDA2nVKgMvr/pvoyRJ0o1Sk8Qeffr0Yd++fWRn\nZ5Odnc2+ffsqTdNYYvr06ezdu5dXX32VzpXkT6yqFwdyuLIMG7UNr/R9hYkdJyoH3N0hrWz+Smvr\nxpw7Z8ngwWeBe25cYyVJkupZTRN7zJw5kzFjxgDK8gAXF5cq7+fg4MDFixdp1qyZ6dj58+dJTEw0\ne8s1lRBCXOfve9Py9/fn9OnTNa/oww9hzhxIT1dSmwC5ueDkBM8+u5oPPnik5veQJEm6SdTad2ct\nGTx4MGvXrsXDw8N0LDk5mUceeYQdO3aYVYccrqyKu7vyWmrI8vx55bVRowM3oEGSJEl3jvT09DIB\nDsDDw4PUqx4jVUUGuapUGeT2o9Nl3YBGSZIk3Rl8fHz4559/yhw7cOAATZo0MbsO+UyuKlUEOW/v\naPLzo3Bx6VvBhZIkSVJNzZ49m6eeeooxY8bg4+NDfHw8oaGhLFmyxOw6ZE+uKhUEuehocHU14uCQ\nTV7e0RvUMEmSpNvfwIED+frrr7l06RJ79uzh0qVLfPnllwwcONDsOmRPriqV9OSaN1dhZeUhg5wk\nSVId69SpE506dbru62WQq4qNDTg6lgtyXbuqcHDoIoOcJElSHTt58iQRERFkZmZSejFA6a3aqiKH\nK6tTakG4wQBaLTRvDg4OXcjPP4HRWHxj2ydJknSbWr9+PQ8++CD79+/niy++4MyZM6xatcrs5Mwg\ng1z1PDxMQe7CBdDprgQ5IYq5dOnkDW6gJEnS7enLL7/kyy+/ZMWKFabEzMuWLcPS0vxBSLOD3P79\n+4mPjwcgJSWFBQsW8OKLL17TeoVbUqmeXMnMSiXIdQWQQ5aSJEl1JD09nW7dugFgYWGB0WikX79+\n7N692+w6zA5yr7/+Omq1GoC3334bvV6PSqXilVdeucZm32IqCHItWkCDBq2wsLCTQU6SJKmONG7c\nmAsXLgDg5+fHrl27iIiIwMrKyuw6zO7zJScn4+XlhV6vZ9++ffzxxx9YWVkREBBw7S2/lZQEOSGI\njlZhaQlNmoBKpcbevpMMcpIkSXVk2rRpREdH06RJE2bOnMkzzzyDTqfj5ZdfNrsOs4Ocg4MDaWlp\nnD17lhYtWmBvb09xcTF6vf66Gn/LcHeH4mLIzeX8eSd8faFkONjBoQupqesRQqCSu6dKkiTVGiEE\n3bt3x9PTE4B+/fpx8OBBdDod9vb2Ztdj9nDlpEmTGDNmDM8//zwTJypZ+iMjI2nevPk1Nv0WU7JW\nLiXl8hq5K6ccHLqg12dRVGT+TB9JkiSpeiqVihEjRmBhcSVMWVtbX1OAg2sIctOnT2fVqlWsW7eO\n4OBgQNkcb/HixWZdHx4ezpAhQxg0aBArV66ssMy2bdsICgoiODiYuXPnmo4nJCQwdepUhg0bRlBQ\nkGmMtl6UWhB+/rzyPK6Eg0MXQE4+kSRJqgtt27YlJiamRnVc02Lw0nv67N+/HwsLC3r06FHtdQaD\ngUWLFrFq1So0Gg1jxowhMDCQli1bmspotVpWrlzJunXrcHZ2Jj093XRuwYIFzJgxg969e5Ofn18m\nste5y0EuW5tJevrVPbmOgIq8vKM0ahRSf22SJEm6A/To0YPHH3+ckSNH0rhx4zKPhUr2pauO2UFu\n0qRJzJkzh7vvvpuVK1fyzTffoFarmThxIjNmzKjy2qioKHx9ffHx8QEgODiYXbt2lQlyGzZsYOLE\niTg7OwPQsGFDAM6dO4derzdtkHetXdUauxzkzp9SFn2XDnJqtT12dq3JzT1Sv22SJEm6A0RGRuLt\n7W3aabyESqWq/SB39uxZunRRhud+/PFHVq9ejb29PQ8++GC1QS45OZnGjRub3ms0GqKiosqU0Wq1\nAEyYMAGj0cjs2bPp27cvWq0WJycnZs+ezYULF7j33nt5/vnnTcsZSqxfv57169eb++uYryTIRSvp\nZEoPVwI4OHQiN/dw7d9XkiTpDrdmzZoa12F2kDMajahUKuLi4hBCmHph2dnZNW4EKEOasbGxrFmz\nhqSkJCZNmsTWrVvR6/VERESwadMmPD09mTNnDqGhoYwdO7bM9ePHj2f8+PGAsrttrWnQABo04Hyc\n8lGVGrEFwN6+E6mpP6LX52Jp6Vh795UkSbrDGY3GSs+Z+9jK7CB39913s2jRIlJTUxk0aBAAcXFx\nuLq6VnutRqMhKSnJ9D45ORmNRlOuTOfOnbGyssLHxwc/Pz+0Wi2NGzembdu2pqHOAQMG8O+//5rb\n7Nrh7k50kj0NG8Ll0VQT5bkc5OefwNm5Z/22S5Ik6TbWrl27SpdnnTxpXkpFs4Pcm2++yapVq3Bz\nc+Oxxx4D4Pz58zzyyCPVXtuxY0e0Wi3x8fFoNBrCwsJ47733ypQZOHAgYWFhjB49moyMDLRaLT4+\nPjg5OZGTk0NGRgZubm4cOHCADh06mNvs2uHuzvkYFypaLWFvXxLkjskgJ0mSVIt27dpV5n1qaior\nV66kf//+ZtdhdpBzdXXlueeeK3PsvvvuM+8mlpYsXLiQadOmYTAYGD16NK1atWLZsmV06NCBAQMG\nEBAQwF9//UVQUBBqtZr58+ebeokLFixg8uTJALRv377cUGWdc3fn/DF3urcof8rW1g+12oH8/Kjy\nJyVJkqTr5u3tXe7922+/zZgxY8yOAypReoOeKuh0Oj799FM2b95MSkoKHh4ehISEMGPGDKytra+9\n9XXI39+f06dP11p94pHJ2K35nKfn2bJ0afnzkZH3olLZ0LXrn7V2T0mSpPpW29+ddSExMZH777+f\nQ4cOmVXe7J7cO++8Q1RUFK+//jpeXl4kJCTwySefkJeXx0svvXTdDb4V5Lk0oQhb07rwq9nbdyQ1\n9WeZ3kuSJKkWzZs3r8x3amFhIYcOHeL+++83uw6zg9z27dvZvHmzaQixefPmtGvXjpCQkNs+yKXZ\nKZNe3J2KAJty5+3tO5GY+AXFxYnY2HjVc+skSZLqT3h4OG+88QZGo5GxY8cyffr0MudDQ0NZunSp\naXLhpEmTTEOLS5cuZc+ePRiNRnr37s3LL79cZcfA19e3zHs7OzsmTJhAr169zG6v2UGuslFNM0c7\nb2mpVkrgamSZBWjKnS+ZYZmXFyWDnCRJty1zslcBBAUFsXDhwjLHIiMjiYyMZMuWLQA89NBDHDx4\nkHvuuafS+82ePbvGbTY7P9bQoUN58skn2bt3L9HR0YSHhzNr1iyGDh1a40bc7NLUSmBrpEqv8Hzp\nGZaSJEm3q9LZq6ytrU3Zq8yhUqkoLi5Gp9OZXhs1alTlNYsXLyYyMrLMscjISN544w2z22x2kJs3\nbx733nsvixYtYtSoUSxevJh77rmH+fPnm32zW1WqUUkx5i5SKjxvZeWGtbW3DHKSJN3WKspelZyc\nXK7cb7/9xogRI3j66adJTEwEoGvXrtxzzz306dOHPn36EBAQQIurU0hd5Zdffim3ZKxDhw788ssv\nZre5yuHKf/75p8z7Hj16lEvIfPjwYe69916zb3grStO7ANCoOKHSMg4OHcnLk8sIJEm6tY0aNcr0\n79KZpMzVv39/hg8fjrW1NT/88AMLFixg9erVxMbGEh0dzZ49ewCYOnUqERERdOvWrdK6VCpVuUdi\nBoOhykwoV6syyFW2+2rJg8KS2YTmdldvVamFjlhRjFNe5UHO3r4jmZl/YDTqsLAwf2t2SZKkm0lo\naGil58zJXlU6C9bYsWN55513APj999/p3LmzKcl+QEAAR44cqTLIdevWjQ8//JB58+ZhYWGB0Wjk\n448/rvKaq1UZ5P744w+zK7qdpeVY04hEVKkVD1eCEuSEKKag4Cz29u3qsXWSJEn1w5zsVSXrqEGJ\nISVDkl5eXmzYsAG9Xo8QgkOHDpmSfFTm5Zdf5oknnqBPnz54eXmRmJiIu7s7n332mdltvqb95O5U\naekq3K2z4eLFSss4OHQClBmWMshJknQ7Mid71Zo1a/jjjz9Qq9U4Ozvz5ptvAjBkyBD279/PiBEj\nUKlUBAQEEBgYWOX9GjduzMaNG4mKiiIxMRFPT086dep0TXuKmp3x5FZS26v2e/cG2/8i2dX+adi3\nr8IyRmMR4eH2NG26gObNzZ/5I0mSdLO42TKenDx5EhcXFzw9PU3HEhMTyc7Opk2bNmbVUY9bbN+6\n0tLA3UUHl/e8q4iFhQ0NGviTl1fPOyRIkiTdpubNm4dery9zTKfTMW/ePLPrkEHODKmp0MhdBQkJ\nUFxcaTlX1wFkZv5GQcH5emydJEnS7SkhIcG0zVqJpk2bcrGKR0dXk0GuGno9ZGaCu5cVCAEXLlRa\ntmnTF1CpLImJWVhpGUmSJMk8jRs35sSJE2WOnThxwjSxxRwyyFUj/XKSk0ZNGyj/qGLI0sbGiyZN\nniEl5Xs5bClJklRDjz76KDNnzmTNmjXs2bOHNWvWMHv2bKZMmWJ2HXJ2ZTXS0pRX95aXtwSPja2y\nvI/PfBISPuP8+Zfo1CmsjlsnSZJ0+xo3bhyOjo789NNPJCUl4enpyYIFC64pnaQMctVITVVeG/k3\nBJWq2iBnZeVK06YvcP78C2RlhePi0rceWilJknR76t69O9bW1mRmZgKQl5fHTz/9xJgxY8y6Xga5\naph6cl5W4OVVbZAD8PZ+igsXPiImZqHcSFWSJOk67dy5k3nz5uHr68u5c+do2bIlZ8+e5a677jI7\nyMlnctUw9eQaAb6+VT6TK6FWN8DT83Gys8PR6TLqtH2SJEm3qw8//JAlS5awadMm7Ozs2LRpE4sW\nLSqXtLkqMshVo6Qn16gR4OdnVk8OwM1tECDIytpdV02TJEm6rSUkJDBs2LAyx0aOHMmmTZvMrkMG\nuWqkpoKzM1hZofTk4uPBYKj2OkfHHqjVjmRm7qz7RkqSJN2GGjZsSNrlnoa3tzdHjhwhLi7umnYh\nkEGuGmlpl3txoAQ5vV5ZFF4NCwsrXFzuk0FOkiTpOo0dO5bDhw8DynKCRx55hJCQEB588EGz66i3\niSfh4eG88cYbGI1Gxo4dy/Tp08uV2bZtG8uXL0elUtGmTRtTduu2bdvSunVrADw9Pa8pA3VNpaaC\nu/vlN35+ymtsLFy1Cr8irq4DSU/fSkGBFjs7v7pqoiRJ0m2pdJx44IEH6NGjBwUFBdVutlpavQQ5\ng8HAokWLWLVqFRqNhjFjxhAYGEjLli1NZbRaLStXrmTdunU4OzuTXrIKG7C1tWXz5s310dRy0tKg\nSZPLb3x9ldfYWOjTp9prXV0HApCZuRM7u2l11EJJkqQ7g5eX1zVfUy/DlVFRUfj6+uLj44O1tTXB\nwcHlNlrdsGEDEydOxNlZWXTdsGHD+mhatdLSSvXkmjZVXs2YYQnQoEFbrK095ZClJEnSDVIvQS45\nOZnGjRub3ms0GpKTk8uU0Wq1xMTEMGHCBMaNG0d4eLjpXFFREaNGjWLcuHHs3Fl/AUOIy8mZS57J\nNWgAHh5mz7BUqVS4ug4kK2sXQpj/oFSSJEmqHTfNYnCDwUBsbCxr1qwhKSmJSZMmsXXrVpycnNi9\nezcajYb4+HgmT55M69ataVrSq7ps/fr1rF+/vlbblJ8PRUWlenKgDFmaGeRAGbJMTl5DXl4Ujo5d\narV9kiRJUtXqpSen0WhISkoyvU9OTkaj0ZQrExgYiJWVFT4+Pvj5+aG9PCxYUtbHx4cePXrw33//\nlbvH+PHjCQ0NJTQ0tNbaXWYheAkzF4SXKP1cTpIkSapf9RLkOnbsiFarJT4+nuLiYsLCwsptez5w\n4EAOHjwIQEZGBlqtFh8fH7Kzsym+vIdbRkYGkZGRZSas1CVTSq/SPTk/P4iLU8YyzWBj40WDBu1k\nkJMkSboB6mW40tLSkoULFzJt2jQMBgOjR4+mVatWLFu2jA4dOjBgwAACAgL466+/CAoKQq1WM3/+\nfFxdXYmMjOTVV19FpVIhhODxxx+vtyBXaU+usBBSUuCq3mhlGjYMIj7+A9LTf6Vhw2HVXyBJkiTV\nCpUQZnZJbiH+/v6cXF8UWQAAIABJREFUPn26xvWsXg2TJ8O5c2BalrF1K9x/Pxw4AD16mFWPXp/L\n0aP9uHTpNF267MHJqVuN2yZJklTbauu782YiM55UocKeXMmC8Gt4Lmdp6UjHjtuwtvbg2LFgCgqi\na6uJkiRJUhVkkKtCWpqSs9LJqdTB0gvCr4GNTWM6ddqOEHqiooIwGAprr6GSJElShWSQq0LJGjmV\nqtRBJydwcbmmnlyJBg38adfuBwoKzpCQ8GmttVOSJEmqmAxyVSiT7aQ0f384ceK66nRzG4Sr62Bi\nY99Ar8+uWQMlSZKkKskgV4Uy2U5K69YNDh+Ga9juobTmzZeg16cTH/9ezRooSZJUz8LDwxkyZAiD\nBg1i5cqV5c6HhobSs2dPQkJCCAkJ4ccffzSdS0hIYOrUqQwbNoygoCAuXLhQ5+29aTKe3IzS0qBr\n1wpOdOsGK1bAmTPQps011+voeDfu7uOIj38fb+9ZWFubtxRBkiTpRjIn2T5AUFAQCxcuLHf9ggUL\nmDFjBr179yY/Px8Li7rvZ8meXBUq7cl17668Hjp03XU3a/Y/jMZCoqOfJy7uHY4eHcA///hRUBBz\n3XVKkiTVJXOS7Vfm3Llz6PV6evfuDYC9vT12dnZ12VxABrlKGQyQmVlJkGvTBuztISLiuutv0KA1\nnp6PkZy8lvPn56PTpaLTJaPVvnbddUqSJNXUqFGjTD9X5wM2J9k+wG+//caIESN4+umnSUxMBJQk\n/E5OTsyePZsHHniAt99+G4PBULe/DHK4slK5ucrr5Z1/ylKr4a67atSTA2jR4l1cXQfg5NQLW9sm\nnDv3PBcufEDTpi9gb9+2RnVLkiRdj5rm/+3fvz/Dhw/H2tqaH374gQULFrB69Wr0ej0RERFs2rQJ\nT09P5syZQ2hoKGPHjq2llldM9uQqkX154mOFQQ6U53JHjoBef933sLR05P/bu/PwKIr0gePfnnty\nTe7LhCsQjnAfAgooN8uxIALqAiLuD3dRRBRXRdRVBBQFj0VRkUWBVUQFYSEsoGRXEBAEAsgtRyAJ\n5DD3NZmj5/dHJYFAAgFywFCf5+knyUxPd/X0ZN6u6qp6g4NHYTKJrKz16r2AVutBQsLlbdmSJEl1\nrSqT7fv5+WEwGAAYOXIkh0p6ooeGhtK8eXMiIyPR6XT07t27wsn2q5sMcpXIzRU/yw0Ev1jHjmIO\ny+scSlARgyGQiIinSU//lry8+GrbriRJUnWoymT7aWlpZb/HxcURVTInYqtWrcjNzSUzMxOAnTt3\n1so8xLK5shJXrcmVdj7ZvRvatKm2/UZGTiU5+QNOn36J1q1jq227kiRJN6oqk+0vW7aMuLg4tFot\nFouFN954AwCtVsvzzz/PuHHjAIiJianxpkqQEzRXav16GDQIfv4ZOneuYAVVBX9/ePBB+PjjG9rX\npc6ceZPTp6dRr950GjR4BY3GUK3blyRJqoicoPk2ctWanEYDHTrcUA/LykRGPk1o6COcPTuLvXu7\nUlBQ8+3WkiRJ7kgGuUpc9Z4ciCbLAweguLha963RGGnW7DNiYr6juDiR3bvbc+TIWDIzv8flqvku\nt5IkSe5CBrlKXLUmB6Lzid0uAl0NCAoaRqdOBwkLG09GxjoOHOjHjh31yc3dWSP7kyRJcjcyyFUi\nN1cMh/PwuMJKF3c+qSEGQzDR0R/Rtet5WrT4BkXRcvToI6hq9dYeJUmS3JEMcpXIyRFNleXS7Fyq\nXj0xJcoNDgqvCq3WRHDwCKKjP6Gw8Chnz75V4/uUJEm61ckgV4nc3KvcjwMRAe+5R3TFvIFB4dci\nIGAAwcEPcubMLAoLj1/2vMul4nQW4IadZiVJkq6ZDHKVyMm5yv24Un/6E6SmQhUnKa0OUVHvotGY\nOH58Inl5ezl1ahq7djVn61ZvfvxRx9atXvz226RaK48kSdLNSga5SlSpJgcwcKDIFP7FFzVeplJG\nYyhRUXPIzo5jz54OnD37NkZjJGFhE6hf/2X8/Qdx7twnFBa613gXSZKkayVnPKlETg6EhVVhRZMJ\nRo6EL7+Ejz4S2QlqQVjYBGy2dAyGUAIDh2EwXEiXYLOl8fPPjTh9+mViYr6ulfJIkiTdjGqtJne1\nbLIA69evZ+DAgQwaNIipU6eWey4/P58ePXowY8aM2ihu1WtyAGPGQEEBrFlTo2W6mKJoaNDgJcLD\n/69cgAPRIzMy8hnS078hL29vrZVJkiTpZlMrQa40m+yiRYuIjY1l3bp1nDhxotw6CQkJLFy4kOXL\nlxMbG8uLL75Y7vn33nuPTqVd9mtBle/JAXTrJnpa/utfNVqmaxEZORWdLoBTp168+sqSJEluqlaa\nKy/OJguUZZO9eAbqr7/+mtGjR2MpiSwBAQFlzx08eJCMjAy6d+/OwYMHa6PI11aT02hEB5S334a0\nNAgOrtGyVYVOZ6F+/WmcPPksCQkzcTgyyc/fh92eiUZjQqMx4uvbkwYN/o5yxXESkiRJt65aqclV\nJZtsQkICp0+f5sEHH2TUqFFs2bIFAFVVmTNnDs8//3xtFBUQs3QVF19DTQ5Ek6XTCZdk0q1L4eGP\nYzRGkpDwMufOfYTTWYDJVB+dzhdVLeLMmddITJTj7SRJcl83TccTp9PJmTNnWLZsGSkpKYwZM4a1\na9fy73//mx49epQLkhVZsWLFZanar1eV5q28VEwMtG0L//gHjBt3jS+uGVqtmfbtf8bhyMFsboJG\nc+F0u1wujhz5E6dOTcNsbkpQ0LByr1XVYjIyYrFaE4iIeApF0dZ28SVJkm5YrQS5qmSTDQkJoU2b\nNuj1eiIjI2nQoAEJCQnEx8ezZ88eli9fTkFBAXa7HQ8PD5599tlyr3/ggQd44IEHAJEu4kZUad7K\nirz7LvTpI2p1q1eLZsw6ZjSGYzSGX/a4oig0bbqYoqJTHDkyGp3u3yiKHqv1NDk520hP/waHIxsQ\nvTWjot6s7aJLkiTdsFoJchdnkw0JCSE2NpZ58+aVW6dPnz7ExsZy//33k5mZSUJCApGRkeXWW7Vq\nFQcPHrwswFW366rJAdx7L7z/PkyaBK+8AjNnVnfRqpVWa6Zly9Xs3Xsn+/f3KXtco/EgKGg4ISFj\nSU9fRWLiHDw9YwgNHVuHpZUkSbp2tRLkqpJNtnv37mzbto2BAwei1Wp57rnn8PPzq43iXaa0Jndd\nLY6PPw779sGsWdC6NYwaVa1lq25GYxjt2m0jK+t7jMZITKYGmEz10WiMAPj69qSo6BjHjk3AbG6C\nxdKljkssSZJUdTIzeAXWrIFhw2DPHmjf/jo2UFwMvXrB3r0QFwddu153WW4GdnsGe/bcicORQ3j4\nBAIDh+Ht3QlFqfvmWEmSqo87Zga/aTqe3ExuqCYHYDTCd9/B3XfDkCGwfTtER1db+WqbXh9A69br\n+e23SSQmzuXs2TfR6fxQFD2qagVcBAQMIjT0Ufz8esvgJ0nSTUMGuQqU3pO75o4nFwsOhg0bRC1u\nwAAR6K7SQ/Rm5uHRlDZtvsduzyIjI5acnC2ABo3GhNOZz++/ryIt7SuMxvo0aPB3QkPHlQU7qzWJ\npKT3CAgYhJ9fz7o9EEmSbisyyFXghmtypaKiIDZWdEjp1Qvmz4fevW+0eHVKr/cjNHQMoaFjyj3u\ndH5ARsYakpLe49ixRzl37hMaNXqTrKxNJCW9i6paSU5+n+jojwkL+3MdlV6SpNuNDHIVyM0VLY5G\nYzVsrFMn+Pe/4dFHxfCCvn3hzTev82bfzUskdX2AoKBRpKb+i5Mn/8b+/aLWFhIyhsjI5zh58lmO\nHfs/iopO0rDhzHLNmjZbOkePjkdRdERHL6hw2IMkXcrlcpGZmYmqqnVdlFuKRqPB39//tpjtSHY8\nqcBf/wqrVokZuqqN1QoLFohel5mZMGIEzJgBzZtX405uHg5HDqmpX+Lj0xlvbxHQVdXOb789wfnz\nn2KxdKNBg1fx9e1FXt4eDh0ajs2WhqJo0GjMREd/RHDwzd0zVap7GRkZeHp6YjKZ6rootxSr1UpB\nQUG56RPBPTueyB4CFcjNvcH7cRUxmeCZZ+DUKTGGbsMGaNkSxo4VQw7cjE5n4Y47JpYFOACNRk90\n9CdER39CUdEp9u/vw969nYmP7wYotG+/jY4d4zGbG3P48APs39+XxMR3yc/fj8slr9Sly6mqKgPc\ndTCZTLdN7VcGuQrk5NTgrFwWC7z2Gpw+DVOniipju3bivt3KlSJljxtTFIXw8Mfo3PkkTZp8iM2W\niq9vdzp02I23dwc8PJrSrt02GjacjdV6lpMnn2H37rb89JM/8fE9OH58EufPf0ZxcXJdH4ok3Zau\nljZt1apVdOnShaFDhzJ06FC++eabcs/Xdto02VxZge7dQa8XQ9xqXFYW/POfolPK2bNix127iqAX\nGAhms4i4ffqAv38tFKh2lX78Krs3YLUmkp0dR27uTvLzD1BQcACnMw8AT89WBAXdT71609BoDBW+\nPj9/P+fPLyIsbAJeXq2vuXwZGRswmSLx9Iy55tdKNS89PZ2goKC6LsYtqaL37mrfnU6nk/79+/PZ\nZ58REhLCiBEjeOedd8pllCmdmeqVV16pcBszZ84kKysLi8VS6TrVSdbkKlCjNblL+fnBs8/CyZOw\naRM8/TTk58Prr8PkyTBhAjzwAISHi3Q+P/wA2dngJtcmiqJc8ea3yRRJaOg4oqMX0L79T3TrlkPH\njvtp1Ogt9PpAEhJeZf/+ftjtGeVe53DkceLEM+ze3YHk5A/Yvbs9J05MxeHIq3LZsrI28+uvA4mP\n70Z+/q/XfYyS+8rNzeWLL764rtdOmDCB3NLxSreIi9OmGQyGsrRpVVWaNu3uu++uwVKWJ3tXVqBG\n7sldjU4nel727Sv+tlpF02VhIZw/LxKyLlsGy5eL5z094Y47xE+DQSxNmogB6HffLX6/CSaIrm6K\nouDl1Rovr9bUq/c3UlO/4OjRP7NnT2datPgSmy2V7Oz/kZa2ApstmbCwv1Cv3t84e/YtkpLeJS1t\nOf7+A/HyaouXV2v0+gA0Gg+0Wk/0+qCygFtcnMLhw6Mxm6NxOvM4cKA/7dptw2xuWMfvgHQzyc3N\nZfny5YwePfqy5xwOBzpd5V+xn376aU0W7boNHz687PeLJ76HitOmHThw4LJtbNq0iV9++YWGDRsy\nbdo0wsLCytKmvf3222zfvr1mD+IiMshVoFZrcpUxmcQSEACRkXDnnTBnDmzcKGp9SUlw7pwIgjab\nCIrffQeLF4vXm81inF7jxmIbWq0IpFaraCLNyhJBMDJSLE2bQs+eInDeQkJCRmMyNeLgwWHs3dsZ\nAEUxYrF0IybmGywWMaVa06afEBb2KAkJr5GRsYaUlH+iLQKnkbL2DIulO1FRc/H27sCRI3/C6cyl\nbVtxlRof350DB/rRqNFb5OXtITf3Z8zmhiU1yrqZY1Uqb+nSCx//6vLoo/Dww5U/P2/ePM6ePcvQ\noUO56667uPfee3n//ffx8fHh9OnTbNy4kccff5yUlBSKi4t5+OGHy4JGr169+PbbbyksLGTChAl0\n6NCB+Ph4QkJCWLBgwWUdauLi4vjoo4+w2+34+voyd+5cAgMDKSgoYObMmWUJpSdNmkT//v3ZsmUL\n7777Lk6nEz8/P5YsWVKlY161atX1vVklevbsyeDBgzEYDHz11Vc8//zzLF26lC+//LJKadOqmwxy\nl3C56qgmVxVms5hUszKqCseOidlVDh+GEyfg+HHRvOl0isVgEE2kfn7i782bRbAs7WnVvLkYw+dw\niIBot4vgqNeL5zMy4PffoahI3Dvs21cEx6AgMbCwDsbdWCxd6dDhF37/fTVeXm3w9u6MVnvRF0RJ\nmncfn860br0eV04O6gtPoflkKY7WDcl7eSR5HS0kJb3H3r2d8fJqR35+PE2bLi67F9eq1Xr27+/N\noUPDAS2enjHk5PxIZuZGmjdfhq/vPbV+3IC4yDEaxUWMO3G54J134JdfxNCbm/R+9NSpU/ntt99Y\ns2YNADt37uTw4cOsXbuWyMhIAGbPno2vry9Wq5URI0bQr1+/yyafP3PmDO+88w4zZ87kqaeeYuPG\njQwdOrTcOh06dODrr79GURS++eYbFi1axAsvvMCCBQvw8vJi7dq1AOTk5JCZmcnLL7/Mv/71LyIj\nI8nOzq6W461K2rSLj23kyJG8/fbbAFVOm1bdZJC7REGB+L6v85rc9dBoRJC61rF3DgccPCgC3g8/\nwE8/iS9Ok0kEOKdTBDuXS9QKGzcW+9qwQTSjXsxoFFcIvr5i8fISi6enCIxZWSLoenmJWmN4uHhO\nVcUSFgZ33SUS0CoK/PqrCNqnTl1I2W40QosWYghGdDT4+mIyRhIRMflCOVJTRdPusmViouzoaBg8\nGJo1Q5kxA21yMowejf7HH/EfMQf/gQOJ+OPLpHnsIFG7krCYMYSGPlK2OYulC506/UpxcSLe3h3R\naj3Jzf2FI0dGs29fTwIChqDRmAAVkymKBg1eRqv1BETnmqSk98nIWEdMzLfo9b5l23W5VPLyfsHb\nu+O1JaY9dgzefltUX5o0ET87dLjw3EsviXM3ezY0vMWaWB0Okc2jtDkvPh7WrxctE1fw8MNXrnXV\nqKws8eXhcNCqVauyAAewbNkyvv/+ewDOnz/PmTNnLgtyERERNC/5v42JiSE5+fLewykpKTz99NOk\np6djs9mIiIgAYMeOHbzzzjtl61ksFuLi4ujYsWNZOXx9fS/b3vWoStq0tLQ0goODAVH7jCo5b3WR\nNg1kkLtMtcxbeavR6URQadtWDGuoKlWF/fth2zbIyxM1v6Ii8SZmZ4slP18Mfs/PFzVRX1+oV0+s\nv2+fmPasqEgETUURwRTEulqteB2Ah4d4zGgUj116w16vF+s4HGIbNpt4vH17mD4ddu+GDz4Qj7ds\nCd98A126iH3Pnw9vvIF2/XrCgDAA/gV+sSLoBgaCjw9miwWzRgPFC8Bmw8fppJMSTWGhA6smDruf\nHoefjiLTtyR4f05446cx+DfmTN4C0p1xqHo4d+L/qG94GNLTwWYjM+XfZP2+gWzPegRHjMXk01gc\nh6en+Ol0isButYom6tOn4cgR+P578V6MHSuasDt3hmnTxHrvvXfhtatXwwsviDGanp7ifXa5RA0w\nL098OZ87J5asLFHTL60ZFhaKL267XQTKpk2hUSPxXquqeK+zs0XtPitL/F0qJESsW3q1WLpPp1O8\nXqcT+1AUsTgcYl85OWI2hv/8B158Ucz7et994viWLxddny8dF+d0Xvjs2Wzic+LldaH1oaLPbemx\n5+SI7YWEiH/6q7VElHb4Kl3P4RD7PHlS/H36NB4ajSiTVsvOnTvZvn07K776CrNez9hx4yguKLiw\nHZcLiosxaLXi+LVatEBx6UXlReWZOXMmj4wbR+8uXdj50098sHixmLHC4RDHkZUl1vf0vPIx3ICq\npE1btmwZcXFxaLVaLBYLb7zxRo2Vp0plrtO934Sqbd7K24FGI8b4tWtXfdtMSoIdO0TtzeEQtbq7\n7hKBsfQf3uWC5GRR+zx1Spy03FzxJarXi8XHB4YOFTW+Uvn5ombYoYP4MgfxhfjccyIIJCVBQoJY\nkpNFh59z50SQTk4WTcCqemHON40GjcuFl+qFV4EiAldeVsnOUoEXAGhUsggrSxYhoGSBs8Csq749\nqlmPI9IPpv4Zw99miYnAs7JgyhSRpFdR4M9/FjPr2Gyi5+6rr4oFLgS52uqd6+8v9pmdXT4IXolW\nC598Ao89Jv7esQMGDYJ+/cTf4eHiuHNyxHnrUkmOQ6NRBFNFEWVwOC5cBLlcYj8+PiK4nDghPhNm\nszjHTmf596g0qDudYnslnwHPlBQKCgpEq4SfHyQmioC7bx9oteQdPIhFUTAfOcLJ5GT2HTggAqLR\nKC5eDhwQ69ts4uIFxOfOahW5vhRFlFOjIS8tjZCsLDh6lNUrV4rXnD3LXdHRfLF4MdPHiqTGOXo9\nbdu25bXXXiMxMbGsubK6anP33HMP99xTvnn+qaeeKvt96tSpTL3KxfLw4cPLdXCpSTLIXeK2rMnd\nTCIiYORIsVRGUcR6Jc01VeblVXluP50OGjQQy42wWiE7G3vuec4ceYHC1D009J2Kty0SZ34Wx3Ne\nRQ31J/rulRw49kdcOpW2HX7CZbdz9sRM0hM/R1MMWqsGsyucYmcKqt6BqgNbINgtdlDSgM+IyPWh\ngf+r6Pz8YMkSGDdOfNG2a0dh4XHy8w/gfHcA2pGheO7LwUPXAMVuF++ftzcuTw9cFk80kY1E4PD3\nFwHAahVf5qU1So1GXEwcOyYuAFwu8ZhWK2rm/v5iv6U1J1UVX9SnTomap0Yj/qEsFvE+lwaa0kDi\nconHS2uvHTqIOV9LNWkCu3bB2rVie6dPiwuKli1FE+Ydd4jamNksAlVhobigKSi4sI/SGqTZLH56\ne4ulNOhnZ4tt2u3iMZ2ufO/k0se02rLaF8XF+IWE0L5jRwZPmED37t259957xecsJAScTnrcfTdf\nbd7MH154gYb16tG2ZUvxXFiY2FZoqHg/DAZxG8DpFO9pYaE4J6XN+KrKpLFjeerDD7H4+NC5a1eS\nioqgTRsmRkUxY+ZMBr/yChqNhkmTJtGvTRtmzJjBk08+iaqqBAQE8Nlnn93YZ/sWJQeDX+L778UF\n49at0K1bNRdMuu24XK5y4wDT07/j0KHhmEwNsVrP0Lbt//D17V72fHFxCnl5u8jN3UVBwUE8PKKx\nWLphsdyNTucHuLDbszh9+iXOn1+IwRDGHXc8iZ9fb7y925Off4AzZ2bx+++rgPL/2hZLD6Ki5uHh\n0YyUlMUkJb1HcXEiAQFDCAv7M35+/dFobq3rXjkY/Ppdz2DwW9Gt9YmuBbK5UqpOlw50DwwcRkDA\nEDIy1lKv3ovlAhyA0RiK0fhHAgP/WOk2DYZAmjb9mLCwRzlxYgqnT0/j9GnQar1xOvPQan2oV+9F\ngoNHotVa0Go9SE9fSULC39m7t1PZej4+dxEQMJi0tK/4/ffvMJka0KLFV/j4dK6R90KS6oIMcpeQ\nzZVSTVIUhaZNF5GW9jXh4Y/d0LZ8fO6kffvt2GypZGX9l5ycHzEa6xEePrFcD06AO+6YSEjIaBIT\n38ZqTSQ8/C9lYwijouaSkbGOkyenEh/fgyZN5hMWNuGKM9E4HHnk5u5AVYsBBY3GiK9vDzSa6shP\nJUnVRwa5S8ianFTTDIZgIiImVeP2QggJeZCQkAevuJ5O50PDhq9f9rhGYyAoaDi+vvdw+PBojh//\nC5mZGzAa6+FyFeNyOdBqfdDp/FAUhaysOHJytuJy2cttx9u7IzExqzCZIi/bhyTVFRnkLlFak5NB\nTrrd6PUBtG4dS0LCayQlvQto0GiMKIoWhyMXVS0EwMMjhoiIKfj59UOv98PlclFYeITffnuCPXs6\nEBPzTd0NjpekS8ggd4mcHNHJy90mkJCkqlAULQ0bzqBhw8vToKhqMapqRae7vC3fx6cj3t6dOHhw\nGPv29SY8fAJhYRPw9m6P01lIWtpyzp9fjFbrSVDQCAIDh2EwBKOqduz23ykuTsZqTcBqTUBVrRgM\nIRgMoTidBeTk/Eh29o+4XE5atPgSb+8O1Xa8qmpHUXS3RYbs21WtBbktW7Ywa9YsVFVl5MiRPPbY\n5fcj1q9fzwcffICiKDRr1ox58+aRnJzMpEmTUFUVh8PBmDFjeOihh2qsnDftlF6SVMc0GuMV77l5\nejajQ4ddnDw5lZSUzzl37mM8PVtjtZ7B6czBwyMGuz2d48f/wvHjE9HpfHA4rj7dlFbrhcXSjYKC\nw8TH96BFi+VlHXNU1YbDkQMoKIoGRdGVzDxzdXZ7NlbrSbRaT0ymxrdcz1KpamrlrDqdTmbMmFEu\nB1GvXr3K5SBKSEhg4cKFLF++HIvFQkaGSJ0SFBTEihUrMBgMFBQUMGTIEHr16nXZfGnV5aaYnFmS\nblE6nQ9Nm35Ko0ZvkZr6BWlpXxIQ8AfCw5/AYhHpVQoKfiU9fSV2ewYGQzB6fRAGQxhmc0OMxvpo\ntWZstjRsthQURYOnZxs0Gh3FxSkcPPhHDh4cRnj4RAoLD6PTDcXDo/xYH70+EKMx8orTpNntGVit\np9FoTDidBRQVHcNsblJpXsKKOJ1FuFxOdDqva3qP2rVrR3x8/DW9Rrp+tRLkLs5BBJTlILo4yH39\n9deMHj0aS0k1KiBAzANhMFz40NlsthpP2S5rcpJ04/R6PyIiJlXYwaY0VdKVmEyRl3VgMRpDadv2\nfxw5MoZz5xbg6dkSiyUGszkaUAAVhyMXuz0Vp7MAk6kRWq253DZcLmdJ82giWq03ZnPjkiB3gsLC\no5jN0eUm93a5XBQV/YaqFqPXB6HXBwIqxcXncDh+B0Cn88NojLymACnVnloJclXJQZSQkADAgw8+\niKqqTJo0iR49egBiUtPHHnuMs2fP8txzz1VYi1uxYgUrVqy44bLKmpwk3by0Wg9iYlbicOSg1/uS\nnp6OTif+YZfuX8ri+MW4XA5U1YoYDK9FUTSIIOjE5XIClDRrXgiALpcTVS0CFLRaj5L14eFWIxgZ\n3QWNxozNloTNdq70Fej1Ibz//j8JDDQyatQADIZgFiz4HLPZwKhRg5ky5XXy8gpxOJw89dRT9OnT\n54rHVllKnopS5lSWXke63E3TCO10Ojlz5gzLli0jJSWFMWPGsHbtWnx8fAgLC2Pt2rWkpqbyxBNP\n0L9/fwIDA8u9/uLkfk2bNr3ucuTmivRqkiTdnBRFuWwcYPnndWi1nqiqDVBLApuKCHgGFEWLougu\neY0WjcaMqhaiqsVlGSXsjky0Wh/M5iaoahF2ezrgwmAIQ6MxMmTIKGbNmslDDw3HZjvPxo2b+eCD\n6ShKAXPm/BUvL09yc52MG/c8PXq0Q6s143K5sNuzAWfJfU4PFEVTYUoel8tVYcqcitLrSBWrlSBX\nlRxEISEhtGnfBUx5AAAME0lEQVTTBr1eT2RkJA0aNCAhIYHWrVuXW6dJkybs3r2bAQMG1EhZZU1O\nkm5ND7d5mIfbVJxr59Lp1SpTXHwOm+0cRmN9HI5snM48TKb6KIqo4Wm19cut36JFCzIzs8jL8yUz\n04GvbzCNG/fGZrPx/vuvs3v3LyiKi7S0dJKT4wkM9AVUrNYT5baj0Zj5/PN1xMVtAy6k5MnMzKww\nZU5peh2Xy4XDkYmn501TX7npaK6+yo27OAeRzWYjNjaWXr16lVunT58+7Nq1C4DMzEwSEhKIjIwk\nJSUFq9UKiKuVvXv30rAG82PJe3KS5H6qOkTAYAhDq/WhuFj0CDUa77jqLC4DBgxg48aN/Oc/Gxk4\ncCAA69atIzs7j+++W8vatRsJCAhCq62PydQI0ODh0RwPjxhMpigMhjB27z7E9u0/8dlnr/Pdd1/R\nvHlzrNYiXC4HoJYM3yjm4qmGXS61ZNjFaYqKfsNmS73et8et1Ur4r0oOou7du7Nt2zYGDhyIVqvl\nueeew8/Pj23btvHmm2+iKAoul4tHH330hpojr8TpFJOXy5qcJN2eFEXBZGpIYeFhFMWIXh981dcM\nHDiQl19+maysLJYtWwZAXl4eAQEB6PV6fv75Z86dO4dGY0av9y+pFYqcb6JjjB92uz++vkEYjS4O\nH/6B/fv3UVT0G02aRPDLLz9z/Phm7rgjmJwcKwEBd9ClSzuWLPmQqVMfwmAIIzs7HUhEVW0YjRFy\n3N9FZBaCi+TkiCwX8+aJNFWSJN3caioLgahBaUo6rVzdkCFD8PX1LQtymZmZTJw4kcLCQlq2bMm+\nffv49NNPiYiIqHAIgc1m4/HHHyc5OZn69cPIy8tn4sTx3HlnR7Zu/Zl//OMTVFXFz8+HDz98nsLC\nAubMWcKxY0lotXqeeOIJ7rmnOXZ7GgZDKEbj1dNQ3S5ZCGSQu4jLBa+/Do88InJ0SpJ0c7sdU+24\nXCpOZ0FJp5Xywxbs9t8BDXq9/1W3c7sEOXm38iKKAq+8UtelkCRJqpyiaNDpvCt8Tozjky5WKx1P\nJEmSJKkuyCAnSZIkuS0Z5CRJumVpNJqyIUZS1VmtVjSa2+PrX96TkyTpluXv709mZiZ5eXl1XZRb\nikajwd//6p1T3IEMcpIk3bIURSmbzF2SKnJ71FclSZKk25IMcpIkSZLbkkFOkiRJcltue0+upua3\nlCRJkm4dbjmtlyRJkiSBbK6UJEmS3JgMcpIkSZLbkkFOkiRJclsyyEmSJEluSwY5SZIkyW3JICdJ\nkiS5LRnkLrJlyxb69+9P3759WbhwYV0Xp8acP3+esWPHMnDgQAYNGsSSJUsAyM7OZvz48fTr14/x\n48eTk5NTxyWtfk6nk2HDhvGXv/wFgMTEREaOHEnfvn2ZMmUKNputjktY/XJzc5k8eTIDBgzgD3/4\nA/Hx8W5/rj///HMGDRrE4MGDeeaZZyguLnbLcz1t2jS6du3K4MGDyx6r7Ny6XC5mzpxJ3759GTJk\nCIcOHaqrYtcqGeRKOJ1OZsyYwaJFi4iNjWXdunWcOHGirotVI7RaLS+88ALr169nxYoVfPnll5w4\ncYKFCxfStWtXNm3aRNeuXd0y0C9dupSoqKiyv+fOncsjjzzC999/j4+PD99++20dlq5mzJo1i+7d\nu7NhwwbWrFlDVFSUW5/r1NRUli5dysqVK1m3bh1Op5PY2Fi3PNfDhw9n0aJF5R6r7Nxu2bKFhIQE\nNm3axOuvv86rr75aByWufTLIlThw4AD169cnMjISg8HAoEGD2Lx5c10Xq0YEBwcTExMDgJeXF40a\nNSI1NZXNmzczbNgwAIYNG8YPP/xQl8WsdikpKfzvf/9jxIgRgLiy/fnnn+nfvz8A9913n9ud87y8\nPH755ZeyYzYYDPj4+Lj9uXY6nVitVhwOB1arlaCgILc81506dcJisZR7rLJzW/q4oii0bduW3Nxc\n0tLSar3MtU0GuRKpqamEhoaW/R0SEkJqamodlqh2JCUlceTIEdq0aUNGRgbBwcEABAUFkZGRUcel\nq16zZ8/mb3/7W1myyKysLHx8fNDpxOx2oaGhbnfOk5KS8Pf3Z9q0aQwbNozp06dTWFjo1uc6JCSE\nRx99lJ49e9KtWze8vLyIiYlx+3NdqrJze+l3nDu/BxeTQe42VlBQwOTJk3nxxRfx8vIq95yiKCiK\nUkclq37//e9/8ff3p2XLlnVdlFrlcDg4fPgwDz30EKtXr8ZsNl/WNOlu5zonJ4fNmzezefNmtm7d\nSlFREVu3bq3rYtUJdzu318NtJ2i+ViEhIaSkpJT9nZqaSkhISB2WqGbZ7XYmT57MkCFD6NevHwAB\nAQGkpaURHBxMWlqaW2UO3rt3L3FxcWzZsoXi4mLy8/OZNWsWubm5OBwOdDodKSkpbnfOQ0NDCQ0N\npU2bNgAMGDCAhQsXuvW53r59OxEREWXH1K9fP/bu3ev257pUZef20u84d34PLiZrciVatWpFQkIC\niYmJ2Gw2YmNj6dWrV10Xq0a4XC6mT59Oo0aNGD9+fNnjvXr1YvXq1QCsXr2a3r1711URq93UqVPZ\nsmULcXFxvPPOO3Tp0oV58+bRuXNnNm7cCMB3333nduc8KCiI0NBQTp06BcCOHTuIiopy63MdHh7O\n/v37KSoqwuVysWPHDho3buz257pUZee29HGXy8W+ffvw9vYua9Z0ZzILwUV+/PFHZs+ejdPp5P77\n72fixIl1XaQasXv3bkaPHk10dHTZ/alnnnmG1q1bM2XKFM6fP094eDjvvfcevr6+dVza6rdz504W\nL17MJ598QmJiIk8//TQ5OTk0b96cuXPnYjAY6rqI1erIkSNMnz4du91OZGQkb7zxBqqquvW5/sc/\n/sH69evR6XQ0b96cWbNmkZqa6nbn+plnnmHXrl1kZWUREBDAk08+SZ8+fSo8ty6XixkzZrB161bM\nZjOzZ8+mVatWdX0INU4GOUmSJMltyeZKSZIkyW3JICdJkiS5LRnkJEmSJLclg5wkSZLktmSQkyRJ\nktyWDHKSdJNKSkqiadOmOByOui6KJN2yZJCTJEmS3JYMcpIkSZLbkkFOkq5BamoqTz75JF26dKFX\nr14sXboUgPnz5zN58mSmTJlCu3btuO+++zh69GjZ606ePMnYsWPp2LHjZWmcrFYrb775Jj179qRD\nhw489NBDWK3WsufXrl3LvffeS+fOnfnoo49q72AlyQ3IICdJVaSqKhMnTqRp06Zs2bKFJUuWsGTJ\nkrIZ7jdv3syAAQPYtWsXgwcP5vHHH8dut2O32/nrX//K3Xffzfbt23nppZd49tlny+aTnDNnDocO\nHeKrr75i165d5dIBAezZs4cNGzawZMkSPvzwQ06ePFknxy9JtyIZ5CSpin799VcyMzOZNGkSBoOB\nyMhIRo0axfr16wGIiYlhwIAB6PV6xo8fj81mY//+/ezfv5/CwkIee+wxDAYDXbt2pWfPnsTGxqKq\nKitXrmT69OmEhISg1Wpp3759uTkVJ02ahMlkolmzZjRr1qxcDVGSpCuTqXYkqYqSk5NJS0ujY8eO\nZY85nU46duxIeHh4uYSUGo2GkJCQsszLoaGh5Wpn4eHhpKamkpWVRXFxMZGRkZXuNzAwsOx3s9lM\nYWFhdR6WJLk1GeQkqYrCwsKIiIhg06ZNlz03f/78crm6VFUlNTW1LJVJSkoKqqqWBbrz58/ToEED\n/Pz8MBqNJCYm0qxZs9o5EEm6jcjmSkmqotatW+Pp6cnChQuxWq04nU6OHz/OgQMHADh06BCbNm3C\n4XCwZMkSDAYDbdq0oXXr1phMJhYtWoTdbmfnzp3ExcUxcOBANBoN999/P2+88Qapqak4nU7i4+Ox\n2Wx1fLSS5B5kkJOkKtJqtXz88cccPXqU3r1706VLF1566SXy8/MB6N27N+vXr6dTp06sWbOG+fPn\no9frMRgMfPzxx2zZsoUuXbrw2muv8dZbbxEVFQXA888/T3R0NCNGjODOO+9k7ty5qKpal4cqSW5D\n5pOTpGowf/58zpw5w9y5c+u6KJIkXUTW5CRJkiS3JYOcJEmS5LZkc6UkSZLktmRNTpIkSXJbMshJ\nkiRJbksGOUmSJMltySAnSZIkuS0Z5CRJkiS39f/3Q6MhmPfJxwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjVnDKnJ1zNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# activation = \"tanh\"\n",
        "# optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "# input_dim = train_x.shape[1]\n",
        "# batch_size = 128\n",
        "# epochs = 100\n",
        "activation = \"relu\"\n",
        "optimizer = keras.optimizers.Adam()\n",
        "input_dim = train_x.shape[1]\n",
        "batch_size = 128\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPPmLKAGB1FL",
        "colab_type": "code",
        "outputId": "afe01d84-f9a6-4527-de70-512fe55a8c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "K.clear_session() # Tensorboard Callback error 방지를 위해 tensorboard의 session을 clear 해줌\n",
        "with tf.Session(graph=tf.Graph()) as sess:\n",
        "  model = Sequential()\n",
        "  # 첫 번째 Layer (Input layer)\n",
        "  model.add(Dense(input_dim=input_dim, init='glorot_uniform', output_dim=256))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.3)) # 30% 정도를 Drop \n",
        "\n",
        "\n",
        "  # # 두 번째 Layer (Hidden layer 1)\n",
        "  model.add(Dense(output_dim=256))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.3)) # 30% 정도를 Drop \n",
        "\n",
        "  # # 세 번째 Layer (Hidden layer 2)\n",
        "  model.add(Dense(output_dim=256))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.3)) # 30% 정도를 Drop \n",
        "  \n",
        "  # Dense Layer (Output layer)\n",
        "  model.add(Dense(output_dim=1))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "  # Cost function 및 Optimizer 설정 # binary class 분류이므로 binary_crossentropy 사용 # Adam optimizer 사용\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=True),  metrics=['accuracy', precision, recall, f1score])\n",
        "\n",
        "  callbacks = [TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "                          write_graph=True,\n",
        "                          write_grads=True,\n",
        "                          batch_size=batch_size,\n",
        "                          write_images=True),\n",
        "               keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                           patience=5),\n",
        "              keras.callbacks.ModelCheckpoint(filepath='best_model.h5',\n",
        "                                             monitor='val_loss',\n",
        "                                             save_best_only=True)]\n",
        "  \n",
        "  hist = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(val_x, val_y), callbacks=callbacks)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 61797 samples, validate on 15450 samples\n",
            "Epoch 1/100\n",
            "61797/61797 [==============================] - 7s 113us/step - loss: 0.7134 - acc: 0.5511 - precision: 0.5495 - recall: 0.5497 - f1score: 0.5481 - val_loss: 0.6592 - val_acc: 0.6113 - val_precision: 0.6279 - val_recall: 0.5389 - val_f1score: 0.5780\n",
            "Epoch 2/100\n",
            "61797/61797 [==============================] - 6s 101us/step - loss: 0.6715 - acc: 0.5872 - precision: 0.5866 - recall: 0.5786 - f1score: 0.5812 - val_loss: 0.6542 - val_acc: 0.6249 - val_precision: 0.6369 - val_recall: 0.5748 - val_f1score: 0.6024\n",
            "Epoch 3/100\n",
            "61797/61797 [==============================] - 6s 99us/step - loss: 0.6627 - acc: 0.6030 - precision: 0.6031 - recall: 0.5940 - f1score: 0.5968 - val_loss: 0.6495 - val_acc: 0.6274 - val_precision: 0.6411 - val_recall: 0.5728 - val_f1score: 0.6032\n",
            "Epoch 4/100\n",
            "61797/61797 [==============================] - 6s 102us/step - loss: 0.6592 - acc: 0.6062 - precision: 0.6075 - recall: 0.5928 - f1score: 0.5985 - val_loss: 0.6464 - val_acc: 0.6294 - val_precision: 0.6412 - val_recall: 0.5811 - val_f1score: 0.6079\n",
            "Epoch 5/100\n",
            "61797/61797 [==============================] - 6s 102us/step - loss: 0.6544 - acc: 0.6138 - precision: 0.6146 - recall: 0.6036 - f1score: 0.6075 - val_loss: 0.6434 - val_acc: 0.6311 - val_precision: 0.6439 - val_recall: 0.5803 - val_f1score: 0.6086\n",
            "Epoch 6/100\n",
            "61797/61797 [==============================] - 6s 102us/step - loss: 0.6515 - acc: 0.6187 - precision: 0.6202 - recall: 0.6054 - f1score: 0.6111 - val_loss: 0.6414 - val_acc: 0.6334 - val_precision: 0.6402 - val_recall: 0.6022 - val_f1score: 0.6187\n",
            "Epoch 7/100\n",
            "61797/61797 [==============================] - 6s 103us/step - loss: 0.6500 - acc: 0.6197 - precision: 0.6212 - recall: 0.6065 - f1score: 0.6121 - val_loss: 0.6401 - val_acc: 0.6342 - val_precision: 0.6451 - val_recall: 0.5891 - val_f1score: 0.6140\n",
            "Epoch 8/100\n",
            "61797/61797 [==============================] - 6s 100us/step - loss: 0.6479 - acc: 0.6260 - precision: 0.6288 - recall: 0.6086 - f1score: 0.6171 - val_loss: 0.6392 - val_acc: 0.6344 - val_precision: 0.6449 - val_recall: 0.5914 - val_f1score: 0.6150\n",
            "Epoch 9/100\n",
            "61797/61797 [==============================] - 6s 101us/step - loss: 0.6461 - acc: 0.6240 - precision: 0.6260 - recall: 0.6105 - f1score: 0.6164 - val_loss: 0.6384 - val_acc: 0.6340 - val_precision: 0.6463 - val_recall: 0.5851 - val_f1score: 0.6123\n",
            "Epoch 10/100\n",
            "61797/61797 [==============================] - 6s 100us/step - loss: 0.6460 - acc: 0.6261 - precision: 0.6297 - recall: 0.6067 - f1score: 0.6162 - val_loss: 0.6382 - val_acc: 0.6344 - val_precision: 0.6524 - val_recall: 0.5686 - val_f1score: 0.6056\n",
            "Epoch 11/100\n",
            "61797/61797 [==============================] - 6s 100us/step - loss: 0.6440 - acc: 0.6297 - precision: 0.6326 - recall: 0.6122 - f1score: 0.6206 - val_loss: 0.6376 - val_acc: 0.6349 - val_precision: 0.6508 - val_recall: 0.5755 - val_f1score: 0.6089\n",
            "Epoch 12/100\n",
            "61797/61797 [==============================] - 6s 100us/step - loss: 0.6435 - acc: 0.6310 - precision: 0.6337 - recall: 0.6132 - f1score: 0.6218 - val_loss: 0.6371 - val_acc: 0.6346 - val_precision: 0.6482 - val_recall: 0.5826 - val_f1score: 0.6116\n",
            "Epoch 13/100\n",
            "61797/61797 [==============================] - 6s 101us/step - loss: 0.6427 - acc: 0.6315 - precision: 0.6345 - recall: 0.6140 - f1score: 0.6224 - val_loss: 0.6367 - val_acc: 0.6351 - val_precision: 0.6490 - val_recall: 0.5824 - val_f1score: 0.6118\n",
            "Epoch 14/100\n",
            "61797/61797 [==============================] - 6s 101us/step - loss: 0.6426 - acc: 0.6307 - precision: 0.6350 - recall: 0.6091 - f1score: 0.6200 - val_loss: 0.6366 - val_acc: 0.6362 - val_precision: 0.6488 - val_recall: 0.5870 - val_f1score: 0.6143\n",
            "Epoch 15/100\n",
            "61797/61797 [==============================] - 6s 100us/step - loss: 0.6421 - acc: 0.6312 - precision: 0.6356 - recall: 0.6099 - f1score: 0.6208 - val_loss: 0.6364 - val_acc: 0.6366 - val_precision: 0.6498 - val_recall: 0.5863 - val_f1score: 0.6143\n",
            "Epoch 16/100\n",
            "61797/61797 [==============================] - 6s 102us/step - loss: 0.6422 - acc: 0.6308 - precision: 0.6358 - recall: 0.6070 - f1score: 0.6193 - val_loss: 0.6363 - val_acc: 0.6366 - val_precision: 0.6509 - val_recall: 0.5831 - val_f1score: 0.6130\n",
            "Epoch 17/100\n",
            "61797/61797 [==============================] - 6s 101us/step - loss: 0.6407 - acc: 0.6329 - precision: 0.6375 - recall: 0.6117 - f1score: 0.6225 - val_loss: 0.6360 - val_acc: 0.6383 - val_precision: 0.6516 - val_recall: 0.5882 - val_f1score: 0.6162\n",
            "Epoch 18/100\n",
            "61797/61797 [==============================] - 6s 101us/step - loss: 0.6406 - acc: 0.6330 - precision: 0.6366 - recall: 0.6132 - f1score: 0.6228 - val_loss: 0.6357 - val_acc: 0.6382 - val_precision: 0.6481 - val_recall: 0.5970 - val_f1score: 0.6195\n",
            "Epoch 19/100\n",
            "61797/61797 [==============================] - 6s 103us/step - loss: 0.6406 - acc: 0.6319 - precision: 0.6356 - recall: 0.6125 - f1score: 0.6223 - val_loss: 0.6358 - val_acc: 0.6377 - val_precision: 0.6526 - val_recall: 0.5832 - val_f1score: 0.6138\n",
            "Epoch 20/100\n",
            "61797/61797 [==============================] - 6s 100us/step - loss: 0.6395 - acc: 0.6338 - precision: 0.6382 - recall: 0.6138 - f1score: 0.6239 - val_loss: 0.6354 - val_acc: 0.6391 - val_precision: 0.6495 - val_recall: 0.5969 - val_f1score: 0.6200\n",
            "Epoch 21/100\n",
            "61797/61797 [==============================] - 6s 99us/step - loss: 0.6392 - acc: 0.6354 - precision: 0.6408 - recall: 0.6118 - f1score: 0.6241 - val_loss: 0.6354 - val_acc: 0.6384 - val_precision: 0.6499 - val_recall: 0.5930 - val_f1score: 0.6181\n",
            "Epoch 22/100\n",
            "61797/61797 [==============================] - 6s 99us/step - loss: 0.6392 - acc: 0.6337 - precision: 0.6377 - recall: 0.6143 - f1score: 0.6241 - val_loss: 0.6353 - val_acc: 0.6377 - val_precision: 0.6501 - val_recall: 0.5897 - val_f1score: 0.6164\n",
            "Epoch 23/100\n",
            "61797/61797 [==============================] - 6s 98us/step - loss: 0.6390 - acc: 0.6348 - precision: 0.6397 - recall: 0.6111 - f1score: 0.6234 - val_loss: 0.6352 - val_acc: 0.6377 - val_precision: 0.6496 - val_recall: 0.5909 - val_f1score: 0.6167\n",
            "Epoch 24/100\n",
            "61797/61797 [==============================] - 6s 96us/step - loss: 0.6389 - acc: 0.6344 - precision: 0.6400 - recall: 0.6107 - f1score: 0.6232 - val_loss: 0.6351 - val_acc: 0.6392 - val_precision: 0.6489 - val_recall: 0.5996 - val_f1score: 0.6213\n",
            "Epoch 25/100\n",
            "61797/61797 [==============================] - 6s 92us/step - loss: 0.6387 - acc: 0.6342 - precision: 0.6388 - recall: 0.6130 - f1score: 0.6238 - val_loss: 0.6351 - val_acc: 0.6391 - val_precision: 0.6501 - val_recall: 0.5960 - val_f1score: 0.6198\n",
            "Epoch 26/100\n",
            "61797/61797 [==============================] - 6s 94us/step - loss: 0.6381 - acc: 0.6366 - precision: 0.6412 - recall: 0.6146 - f1score: 0.6259 - val_loss: 0.6350 - val_acc: 0.6399 - val_precision: 0.6516 - val_recall: 0.5946 - val_f1score: 0.6198\n",
            "Epoch 27/100\n",
            "61797/61797 [==============================] - 6s 93us/step - loss: 0.6379 - acc: 0.6345 - precision: 0.6387 - recall: 0.6147 - f1score: 0.6248 - val_loss: 0.6350 - val_acc: 0.6390 - val_precision: 0.6523 - val_recall: 0.5886 - val_f1score: 0.6167\n",
            "Epoch 28/100\n",
            "61797/61797 [==============================] - 6s 99us/step - loss: 0.6381 - acc: 0.6370 - precision: 0.6419 - recall: 0.6145 - f1score: 0.6262 - val_loss: 0.6349 - val_acc: 0.6396 - val_precision: 0.6512 - val_recall: 0.5942 - val_f1score: 0.6194\n",
            "Epoch 29/100\n",
            "61797/61797 [==============================] - 6s 104us/step - loss: 0.6376 - acc: 0.6349 - precision: 0.6397 - recall: 0.6136 - f1score: 0.6245 - val_loss: 0.6349 - val_acc: 0.6399 - val_precision: 0.6521 - val_recall: 0.5926 - val_f1score: 0.6190\n",
            "Epoch 30/100\n",
            "61797/61797 [==============================] - 7s 106us/step - loss: 0.6374 - acc: 0.6362 - precision: 0.6403 - recall: 0.6151 - f1score: 0.6258 - val_loss: 0.6349 - val_acc: 0.6394 - val_precision: 0.6537 - val_recall: 0.5864 - val_f1score: 0.6161\n",
            "Epoch 31/100\n",
            "61797/61797 [==============================] - 7s 111us/step - loss: 0.6367 - acc: 0.6363 - precision: 0.6408 - recall: 0.6150 - f1score: 0.6259 - val_loss: 0.6346 - val_acc: 0.6410 - val_precision: 0.6519 - val_recall: 0.5986 - val_f1score: 0.6221\n",
            "Epoch 32/100\n",
            "61797/61797 [==============================] - 7s 112us/step - loss: 0.6366 - acc: 0.6375 - precision: 0.6416 - recall: 0.6183 - f1score: 0.6279 - val_loss: 0.6346 - val_acc: 0.6401 - val_precision: 0.6518 - val_recall: 0.5948 - val_f1score: 0.6200\n",
            "Epoch 33/100\n",
            "61797/61797 [==============================] - 7s 107us/step - loss: 0.6372 - acc: 0.6378 - precision: 0.6422 - recall: 0.6171 - f1score: 0.6278 - val_loss: 0.6345 - val_acc: 0.6406 - val_precision: 0.6507 - val_recall: 0.6005 - val_f1score: 0.6225\n",
            "Epoch 34/100\n",
            "61797/61797 [==============================] - 6s 104us/step - loss: 0.6369 - acc: 0.6381 - precision: 0.6422 - recall: 0.6176 - f1score: 0.6281 - val_loss: 0.6346 - val_acc: 0.6397 - val_precision: 0.6507 - val_recall: 0.5968 - val_f1score: 0.6206\n",
            "Epoch 35/100\n",
            "61797/61797 [==============================] - 6s 104us/step - loss: 0.6363 - acc: 0.6369 - precision: 0.6415 - recall: 0.6146 - f1score: 0.6260 - val_loss: 0.6344 - val_acc: 0.6409 - val_precision: 0.6510 - val_recall: 0.6008 - val_f1score: 0.6229\n",
            "Epoch 36/100\n",
            "61797/61797 [==============================] - 6s 104us/step - loss: 0.6359 - acc: 0.6372 - precision: 0.6411 - recall: 0.6169 - f1score: 0.6275 - val_loss: 0.6342 - val_acc: 0.6405 - val_precision: 0.6498 - val_recall: 0.6029 - val_f1score: 0.6235\n",
            "Epoch 37/100\n",
            "61797/61797 [==============================] - 6s 102us/step - loss: 0.6363 - acc: 0.6370 - precision: 0.6416 - recall: 0.6142 - f1score: 0.6261 - val_loss: 0.6344 - val_acc: 0.6399 - val_precision: 0.6523 - val_recall: 0.5928 - val_f1score: 0.6190\n",
            "Epoch 38/100\n",
            "61797/61797 [==============================] - 6s 102us/step - loss: 0.6362 - acc: 0.6383 - precision: 0.6422 - recall: 0.6191 - f1score: 0.6289 - val_loss: 0.6342 - val_acc: 0.6403 - val_precision: 0.6500 - val_recall: 0.6013 - val_f1score: 0.6226\n",
            "Epoch 39/100\n",
            "61797/61797 [==============================] - 6s 103us/step - loss: 0.6358 - acc: 0.6387 - precision: 0.6426 - recall: 0.6198 - f1score: 0.6292 - val_loss: 0.6341 - val_acc: 0.6410 - val_precision: 0.6505 - val_recall: 0.6028 - val_f1score: 0.6237\n",
            "Epoch 40/100\n",
            "61797/61797 [==============================] - 6s 101us/step - loss: 0.6348 - acc: 0.6390 - precision: 0.6437 - recall: 0.6165 - f1score: 0.6283 - val_loss: 0.6342 - val_acc: 0.6412 - val_precision: 0.6530 - val_recall: 0.5961 - val_f1score: 0.6211\n",
            "Epoch 41/100\n",
            "61797/61797 [==============================] - 6s 105us/step - loss: 0.6349 - acc: 0.6379 - precision: 0.6419 - recall: 0.6184 - f1score: 0.6284 - val_loss: 0.6341 - val_acc: 0.6424 - val_precision: 0.6519 - val_recall: 0.6044 - val_f1score: 0.6252\n",
            "Epoch 42/100\n",
            "61797/61797 [==============================] - 6s 101us/step - loss: 0.6355 - acc: 0.6398 - precision: 0.6438 - recall: 0.6201 - f1score: 0.6302 - val_loss: 0.6341 - val_acc: 0.6415 - val_precision: 0.6535 - val_recall: 0.5961 - val_f1score: 0.6213\n",
            "Epoch 43/100\n",
            "61797/61797 [==============================] - 6s 102us/step - loss: 0.6352 - acc: 0.6392 - precision: 0.6441 - recall: 0.6177 - f1score: 0.6289 - val_loss: 0.6341 - val_acc: 0.6410 - val_precision: 0.6507 - val_recall: 0.6023 - val_f1score: 0.6235\n",
            "Epoch 44/100\n",
            "61797/61797 [==============================] - 6s 103us/step - loss: 0.6350 - acc: 0.6401 - precision: 0.6445 - recall: 0.6197 - f1score: 0.6302 - val_loss: 0.6342 - val_acc: 0.6403 - val_precision: 0.6541 - val_recall: 0.5891 - val_f1score: 0.6177\n",
            "Epoch 45/100\n",
            "61797/61797 [==============================] - 6s 101us/step - loss: 0.6352 - acc: 0.6394 - precision: 0.6443 - recall: 0.6167 - f1score: 0.6285 - val_loss: 0.6340 - val_acc: 0.6413 - val_precision: 0.6509 - val_recall: 0.6032 - val_f1score: 0.6240\n",
            "Epoch 46/100\n",
            "61797/61797 [==============================] - 6s 102us/step - loss: 0.6347 - acc: 0.6400 - precision: 0.6444 - recall: 0.6194 - f1score: 0.6297 - val_loss: 0.6342 - val_acc: 0.6418 - val_precision: 0.6546 - val_recall: 0.5944 - val_f1score: 0.6210\n",
            "Epoch 47/100\n",
            "61797/61797 [==============================] - 6s 104us/step - loss: 0.6344 - acc: 0.6393 - precision: 0.6431 - recall: 0.6203 - f1score: 0.6300 - val_loss: 0.6340 - val_acc: 0.6415 - val_precision: 0.6524 - val_recall: 0.5992 - val_f1score: 0.6226\n",
            "Epoch 48/100\n",
            "61797/61797 [==============================] - 6s 103us/step - loss: 0.6344 - acc: 0.6399 - precision: 0.6446 - recall: 0.6180 - f1score: 0.6294 - val_loss: 0.6339 - val_acc: 0.6409 - val_precision: 0.6494 - val_recall: 0.6063 - val_f1score: 0.6249\n",
            "Epoch 49/100\n",
            "61797/61797 [==============================] - 6s 103us/step - loss: 0.6346 - acc: 0.6396 - precision: 0.6436 - recall: 0.6196 - f1score: 0.6299 - val_loss: 0.6339 - val_acc: 0.6413 - val_precision: 0.6515 - val_recall: 0.6012 - val_f1score: 0.6232\n",
            "Epoch 50/100\n",
            "61797/61797 [==============================] - 6s 102us/step - loss: 0.6339 - acc: 0.6416 - precision: 0.6466 - recall: 0.6193 - f1score: 0.6310 - val_loss: 0.6340 - val_acc: 0.6403 - val_precision: 0.6526 - val_recall: 0.5938 - val_f1score: 0.6197\n",
            "Epoch 51/100\n",
            "61797/61797 [==============================] - 6s 100us/step - loss: 0.6343 - acc: 0.6405 - precision: 0.6459 - recall: 0.6180 - f1score: 0.6299 - val_loss: 0.6338 - val_acc: 0.6416 - val_precision: 0.6505 - val_recall: 0.6062 - val_f1score: 0.6255\n",
            "Epoch 52/100\n",
            "61797/61797 [==============================] - 6s 98us/step - loss: 0.6341 - acc: 0.6412 - precision: 0.6456 - recall: 0.6219 - f1score: 0.6318 - val_loss: 0.6339 - val_acc: 0.6400 - val_precision: 0.6505 - val_recall: 0.5988 - val_f1score: 0.6215\n",
            "Epoch 53/100\n",
            "61797/61797 [==============================] - 6s 97us/step - loss: 0.6342 - acc: 0.6404 - precision: 0.6446 - recall: 0.6213 - f1score: 0.6309 - val_loss: 0.6339 - val_acc: 0.6405 - val_precision: 0.6507 - val_recall: 0.6003 - val_f1score: 0.6223\n",
            "Epoch 54/100\n",
            "61797/61797 [==============================] - 6s 99us/step - loss: 0.6342 - acc: 0.6412 - precision: 0.6466 - recall: 0.6175 - f1score: 0.6303 - val_loss: 0.6339 - val_acc: 0.6399 - val_precision: 0.6514 - val_recall: 0.5960 - val_f1score: 0.6203\n",
            "Epoch 55/100\n",
            "61797/61797 [==============================] - 6s 98us/step - loss: 0.6337 - acc: 0.6409 - precision: 0.6459 - recall: 0.6189 - f1score: 0.6305 - val_loss: 0.6339 - val_acc: 0.6403 - val_precision: 0.6503 - val_recall: 0.6003 - val_f1score: 0.6222\n",
            "Epoch 56/100\n",
            "61797/61797 [==============================] - 6s 99us/step - loss: 0.6331 - acc: 0.6404 - precision: 0.6447 - recall: 0.6205 - f1score: 0.6307 - val_loss: 0.6339 - val_acc: 0.6419 - val_precision: 0.6540 - val_recall: 0.5964 - val_f1score: 0.6217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSHDoGQ7mUz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "81af50ed-3255-48c4-8d93-50662690ba2c"
      },
      "source": [
        "model_best = keras.models.load_model('best_model.h5', custom_objects={'precision': precision, 'recall':recall, 'f1score':f1score})\n",
        "\n",
        "score = model_best.evaluate(test_x, test_y, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "Test loss: 0.635629473882168\n",
            "Test accuracy: 0.6410521955260977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4mkzKfqTFE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d373e040-1799-458f-d2fc-42ff9dab762f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "loss_ax.legend(loc='upper left')\n",
        "\n",
        "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "acc_ax.legend(loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAAEJCAYAAAAJnlldAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDqc08606kkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}